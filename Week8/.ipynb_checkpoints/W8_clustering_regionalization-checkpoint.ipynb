{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c295634e",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Implement geodemographic and regionalization clustering\n",
    "\n",
    "This week's lesson is a simplied version of:  \n",
    "- The [Chapter 10 in Geographic Data Science textbook](https://geographicdata.science/book/notebooks/10_clustering_and_regionalization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esda.moran import Moran\n",
    "from libpysal.weights import Queen, KNN\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-drove",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We return to the San Diego tracts dataset we have used earlier in the book. In this case, we will not only rely on its polygon geometries, but also on its attribute information. The data comes from the American Community Survey\n",
    "(ACS) from 2017. Let us begin by reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "db = gpd.read_file(\"https://www.dropbox.com/s/g8ete3zligcozzq/sandiego_tracts.gpkg?dl=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-yesterday",
   "metadata": {},
   "source": [
    "To make things easier later on, let us collect the variables we will use to\n",
    "characterize Census tracts. These variables capture different aspects of the \n",
    "socioeconomic reality of each area and, taken together, provide a comprehensive\n",
    "characterization of San Diego as a whole. We thus create a list with the names of the columns we will use later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_variables = [\n",
    "    \"median_house_value\",  # Median house value\n",
    "    \"pct_white\",  # % tract population that is white\n",
    "    \"pct_rented\",  # % households that are rented\n",
    "    \"pct_hh_female\",  # % female-led households\n",
    "    \"pct_bachelor\",  # % tract population with a Bachelors degree\n",
    "    \"median_no_rooms\",  # Median n. of rooms in the tract's households\n",
    "    \"income_gini\",  # Gini index measuring tract wealth inequality\n",
    "    \"median_age\",  # Median age of tract population\n",
    "    \"tt_work\",  # Travel time to work\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-newsletter",
   "metadata": {},
   "source": [
    "Let's start building up our understanding of this\n",
    "dataset through both visual and statistical summaries.\n",
    "The first stop is considering the spatial distribution of each variable alone.\n",
    "This will help us draw a picture of the multi-faceted view of the tracts we\n",
    "want to capture with our clustering. Let's use (quantile) choropleth maps for\n",
    "each attribute and compare them side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-blake",
   "metadata": {
    "caption": "The complex, multi-dimensional human geography of San Diego."
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n",
    "# Make the axes accessible with single indexing\n",
    "axs = axs.flatten()\n",
    "# Start a loop over all the variables of interest\n",
    "for i, col in enumerate(cluster_variables):\n",
    "    # select the axis where the map will go\n",
    "    ax = axs[i]\n",
    "    # Plot the map\n",
    "    db.plot(\n",
    "        column=col,\n",
    "        ax=ax,\n",
    "        scheme=\"Quantiles\",\n",
    "        linewidth=0,\n",
    "        cmap=\"RdPu\",\n",
    "    )\n",
    "    # Remove axis clutter\n",
    "    ax.set_axis_off()\n",
    "    # Set the axis title to the name of variable being plotted\n",
    "    ax.set_title(col)\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-lodging",
   "metadata": {},
   "source": [
    "Several visual patterns jump out from the maps, revealing both commonalities as\n",
    "well as differences across the spatial distributions of the individual variables.\n",
    "Several variables tend to increase in value from the east to the west\n",
    "(`pct_rented`, `median_house_value`, `median_no_rooms`, and `tt_work`) while others\n",
    "have a spatial trend in the opposite direction (`pct_white`, `pct_hh_female`,\n",
    "`pct_bachelor`, `median_age`). This will help show the strengths of clustering;\n",
    "when variables have\n",
    "different spatial distributions, each variable contributes distinct \n",
    "information to the profiles of each cluster. However, if all variables display very similar \n",
    "spatial patterns, the amount of useful information across the maps is \n",
    "actually smaller than it appears, so cluster profiles may be much less useful as well.\n",
    "It is also important to consider whether the variables display any\n",
    "spatial autocorrelation, as this will affect the spatial structure of the\n",
    "resulting clusters. \n",
    "\n",
    "Recall from [Chapter 6](06_spatial_autocorrelation) that Moran's I is a commonly used\n",
    "measure for global spatial autocorrelation. We can use it to formalise some of the\n",
    "intuitions built from the maps. Recall from earlier in the book that we will need\n",
    "to represent the spatial configuration of the data points through a spatial weights\n",
    "matrix. We will start with queen contiguity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Queen.from_dataframe(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-poultry",
   "metadata": {},
   "source": [
    "Now let's calculate Moran's I for the variables being used. This will measure\n",
    "the extent to which each variable contains spatial structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "# Calculate Moran's I for each variable\n",
    "mi_results = [\n",
    "    Moran(db[variable], w) for variable in cluster_variables\n",
    "]\n",
    "# Structure results as a list of tuples\n",
    "mi_results = [\n",
    "    (variable, res.I, res.p_sim)\n",
    "    for variable, res in zip(cluster_variables, mi_results)\n",
    "]\n",
    "# Display on table\n",
    "table = pd.DataFrame(\n",
    "    mi_results, columns=[\"Variable\", \"Moran's I\", \"P-value\"]\n",
    ").set_index(\"Variable\")\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-measurement",
   "metadata": {},
   "source": [
    "Each of the variables displays significant positive spatial autocorrelation,\n",
    "suggesting clear spatial structure in the socioeconomic geography of San\n",
    "Diego. This means it is likely the clusters we find will have\n",
    "a non random spatial distribution.\n",
    "\n",
    "Spatial autocorrelation only describes relationships between observations for a\n",
    "single attribute at a time.\n",
    "So, the fact that all of the clustering variables are positively autocorrelated does not\n",
    "say much about how attributes co-vary over space. To explore cross-attribute relationships,\n",
    "we need to consider the spatial correlation between variables. We will take our first dip\n",
    "in this direction exploring the bivariate correlation in the maps of covariates themselves.\n",
    "This would mean that we would be comparing each pair of choropleths to look for associations\n",
    "and differences. Given there are nine attributes, there are 36 pairs of maps that must be\n",
    "compared. \n",
    "\n",
    "This would be too many maps to process visually. Instead, we focus directly\n",
    "on the bivariate relationships between each pair of attributes, devoid for now of geography, and use a scatterplot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-transformation",
   "metadata": {
    "caption": "A scatter matrix demonstrating the various pair-wise dependencies between each of the variables considered in this section. Each 'facet', or little scatterplot, shows the relationship between the vairable in that column (as its horizontal axis) and that row (as its vertical axis). Since the diagonal represents the situation where the row and column have the same variable, it instead shows the univariate distribution of that variable."
   },
   "outputs": [],
   "source": [
    "_ = sns.pairplot(\n",
    "    db[cluster_variables], kind=\"reg\", diag_kind=\"kde\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-plate",
   "metadata": {},
   "source": [
    "Two different types of plots are contained in the scatterplot matrix. On the\n",
    "diagonal are the density functions for the nine attributes. These allow for an\n",
    "inspection of the univariate distribution of the values for each attribute.\n",
    "Examining these we see that our selection of variables includes some that are\n",
    "negatively skewed (`pct_white` and `pct_hh_female`) as well as positively skewed\n",
    "(`median_house_value`, `pct_bachelor`, and `tt_work`).\n",
    "\n",
    "The second type of visualization lies in the off-diagonal cells of the matrix; \n",
    "these are bi-variate scatterplots. Each cell shows the association between one\n",
    "pair of variables. **Several of these cells indicate positive linear\n",
    "associations (`median_age` Vs. `median_house_value`, `median_house_value` Vs. `median_no_rooms`)\n",
    "while other cells display negative correlation (`median_house_value` Vs. `pct_rented`,\n",
    "`median_no_rooms` Vs. `pct_rented`, and `median_age` Vs. `pct_rented`)**. The one variable\n",
    "that tends to have consistently weak association with the other variables is\n",
    "`tt_work`, and in part this appears to reflect its rather concentrated \n",
    "distribution as seen on the lower right diagonal corner cell.\n",
    "\n",
    "Indeed, this kind of concentration in values is something you need to be very aware of in clustering contexts. *Distances between datapoints* are of paramount importance in clustering applications. In fact, (dis)similarity between observations is calculated as the statistical distance between themselves. **Because distances are sensitive to the units of measurement, cluster solutions can change when you re-scale your data.**\n",
    "\n",
    "For example, say we locate an observation based on only two variables: house price and gini coefficient. In this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[[\"income_gini\", \"median_house_value\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-milwaukee",
   "metadata": {},
   "source": [
    "The distance between observations in terms of these variates can be computed easily using `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.pairwise_distances(\n",
    "    db[[\"income_gini\", \"median_house_value\"]].head()\n",
    ").round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-closer",
   "metadata": {},
   "source": [
    "In this case, we know that the housing values are in the hundreds of thousands, but the Gini coefficient (which we discussed in the previous chapter) is constrained to fall between zero and one. So, for example, the distance between the first two observations is nearly totally driven by the difference in median house value (which is 259100 dollars) and ignores the difference in the Gini coefficient (which is about .11). Indeed, a change of a single dollar in median house value will correspond to *the maximum possible* difference in Gini coefficients. So, a clustering algorithm that uses this distance to determine classifications will pay a lot of attention to median house value, but very little to the Gini coefficient! \n",
    "\n",
    "Therefore, *as a rule*, we standardize our data when clustering. There are many different methods of standardization offered in the `sklearn.preprocessing` module, and these map onto the main methods common in applied work. We review a small subset of them here. The `scale()` method subtracts the mean and divides by the standard deviation:\n",
    "\n",
    "$$ z = \\frac{x_i - \\bar{x}}{\\sigma_x}$$\n",
    "\n",
    "This \"normalizes\" the variate, ensuring the re-scaled variable has a mean of zero and a variance of one. However, the variable can still be quite skewed, bimodal, etc, and insofar as the mean and vairance may be affected by outliers in a given variate, the scaling can be too dramatic. One alternative intended to handle outliers better is `robust_scale()`, which uses the median and the interquartile range in the same fashion:\n",
    "\n",
    "$$ z = \\frac{x_i - \\tilde{x}}{\\lceil x \\rceil_{75} - \\lceil x \\rceil_{25}}$$\n",
    "\n",
    "where $\\lceil x \\rceil_p$ represents the value of the $p$th percentile of $x$. Alternatively, sometimes it is useful to ensure that the maximum of a variate is $1$ and the minimum is zero. In this instance, the `minmax_scale()` is appropriate: \n",
    "\n",
    "$$ z = \\frac{x - min(x)}{max(x-min(x))} $$\n",
    "\n",
    "In most clustering problems, the `robust_scale()` or `scale()` methods are useful. Further, transformations of the variate (such as log-transforming or Box-Cox transforms) can be used to nonlinearly rescale the variates, but these generally should be done before the above kinds of scaling. Here, we will analyze robust-scaled variables. To detach the scaling from the analysis, we will perform the former now, creating a scaled view of our data which we can use later for clustering. For this, we import the scaling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import robust_scale\n",
    "\n",
    "## robust_scale is a function that scales the data to have a median of 0 and a\n",
    "## standard deviation of 1. This is useful for clustering algorithms that are\n",
    "## sensitive to the scale of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-crowd",
   "metadata": {},
   "source": [
    "And create the `db_scaled` object which contains only the variables we are interested in, scaled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_scaled = robust_scale(db[cluster_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12000cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_scaled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-remove",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In conclusion, exploring the univariate and bivariate relationships is a good first step into building\n",
    "a fully multivariate understanding of a dataset. To take it to the next level, we would\n",
    "want to know to what extent these pair-wise relationships hold across different attributes,\n",
    "and whether there are patterns in the \"location\" of observations within the scatter plots.\n",
    "For example, do nearby dots in each scatterplot of the matrix represent the _same_ observations?\n",
    "This type of questions are exactly what clustering helps us explore.\n",
    "\n",
    "## Geodemographic Clusters in San Diego Census Tracts\n",
    "\n",
    "Geodemographic analysis is a form of multivariate\n",
    "clustering where the observations represent geographical areas {cite}`webber2018predictive`. The output\n",
    "of these clusterings is nearly always mapped. Altogether, these methods use\n",
    "multivariate clustering algorithms to construct a known number of\n",
    "clusters ($k$), where the number of clusters is typically much smaller than the \n",
    "number of observations to be clustered. Each cluster is given a unique label,\n",
    "and these labels are mapped. Using the clusters' profile and label, the map of \n",
    "labels can be interpreted to get a sense of the spatial distribution of \n",
    "socio-demographic traits. The power of (geodemographic) clustering comes\n",
    "from taking statistical variation across several dimensions and compressing it\n",
    "into a single categorical one that we can visualize through a map. To\n",
    "demonstrate the variety of approaches in clustering, we will show two\n",
    "distinct but very popular clustering algorithms: k-means and Ward's hierarchical method.\n",
    "\n",
    "### K-means\n",
    "\n",
    "K-means is probably the most widely used approach to\n",
    "cluster a dataset. The algorithm groups observations into a\n",
    "pre-specified number of clusters so that that each observation is\n",
    "closer to the mean of its own cluster than it is to the mean of any other cluster.\n",
    "The k-means problem is solved by iterating between an assignment step and an update step. \n",
    "First, all observations are randomly assigned one of the $k$ labels. Next, the \n",
    "multivariate mean over all covariates is calculated for each of the clusters.\n",
    "Then, each observation is reassigned to the cluster with the closest mean. \n",
    "If the observation is already assigned to the cluster whose mean it is closest to,\n",
    "the observation remains in that cluster. This assignment-update process continues\n",
    "until no further reassignments are necessary.\n",
    "\n",
    "The nature of this algorithm requires us to select the number of clusters we \n",
    "want to create. The right number of clusters is unknown in practice. For\n",
    "illustration, we will use $k=5$ in the `KMeans` implementation from\n",
    "`scikit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-replica",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Initialise KMeans instance\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-football",
   "metadata": {},
   "source": [
    "This illustration will also be useful as virtually every algorithm in `scikit-learn`,\n",
    "the (Python) standard library for machine learning, can be run in a similar fashion.\n",
    "To proceed, we first create a `KMeans` clusterer object that contains the description of\n",
    "all the parameters the algorithm needs (in this case, only the number of clusters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise KMeans instance\n",
    "kmeans = KMeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-domestic",
   "metadata": {},
   "source": [
    "Next, we set the seed for reproducibility and call the `fit` method to compute the algorithm specified in `kmeans` to our scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "# Run K-Means algorithm\n",
    "k5cls = kmeans.fit(db_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-nurse",
   "metadata": {},
   "source": [
    "Now that the clusters have been assigned, we can examine the label vector, which \n",
    "records the cluster to which each observation is assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first five labels\n",
    "k5cls.labels_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-hopkins",
   "metadata": {},
   "source": [
    "In this case, the first observation is assigned to cluster 2, the second and fourth ones are assigned to cluster 1, the third to number 3 and the fifth receives the label 4. It is important\n",
    "to note that the integer labels should be viewed as denoting membership only &mdash;\n",
    "the numerical differences between the values for the labels are meaningless.\n",
    "The profiles of the various clusters must be further explored by looking\n",
    "at the values of each dimension. But, before we do that, let's make a map.\n",
    "\n",
    "### Spatial Distribution of Clusters\n",
    "\n",
    "Having obtained the cluster labels, we can display the spatial\n",
    "distribution of the clusters by using the labels as the categories in a\n",
    "choropleth map. This allows us to quickly grasp any sort of spatial pattern the \n",
    "clusters might have. Since clusters represent areas with similar\n",
    "characteristics, mapping their labels allows to see to what extent similar areas tend\n",
    "to have similar locations.\n",
    "Thus, this gives us one map that incorporates the information from all nine covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-friday",
   "metadata": {
    "caption": "Clusters in the sociodemographic data, found using K-means with k=5. Note that the large eastern part of San Diego actually contains few observations, since those tracts are larger.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign labels into a column\n",
    "db[\"k5cls\"] = k5cls.labels_\n",
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot unique values choropleth including\n",
    "# a legend and with no boundary lines\n",
    "db.plot(\n",
    "    column=\"k5cls\", categorical=True, legend=True, linewidth=0, ax=ax\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-federal",
   "metadata": {},
   "source": [
    "The map provides a useful view of the clustering results; it allows for\n",
    "a visual inspection of the extent to which Tobler's first law of geography is\n",
    "reflected in the multivariate clusters. Recall that the law implies that nearby\n",
    "tracts should be more similar to one another than tracts that are geographically\n",
    "more distant from each other. We can see evidence of this in\n",
    "our cluster map, since clumps of tracts with the same color emerge. However, this\n",
    "visual inspection is obscured by the complexity of the underlying spatial\n",
    "units. Our eyes are drawn to the larger polygons in the eastern part of the\n",
    "county, giving the impression that more observations fall into that cluster. While this\n",
    "seems to be true in terms of land area (and we will verify this below), there is\n",
    "more to the cluster pattern than this. Because the tract polygons are all \n",
    "different sizes and shapes, we cannot solely rely on our eyes to interpret \n",
    "the spatial distribution of clusters.\n",
    "\n",
    "### Statistical Analysis of the Cluster Map\n",
    "\n",
    "To complement the geovisualization of the clusters, we can explore the\n",
    "statistical properties of the cluster map. This process allows us to delve\n",
    "into what observations are part of each cluster and what their\n",
    "characteristics are.\n",
    "This gives us the profile of each cluster so we can interpret the meaning of the\n",
    "labels we've obtained. We can start, for example, by\n",
    "considering cardinality, or the count of observations in each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data table by cluster label and count observations\n",
    "k5sizes = db.groupby(\"k5cls\").size()\n",
    "k5sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-reynolds",
   "metadata": {},
   "source": [
    "There are substantial differences in the sizes of the five clusters, with two very\n",
    "large clusters (0,1), one medium sized cluster (2), and two small clusters (3,\n",
    "4). Cluster 0 is the largest when measured by the number of assigned tracts, but cluster 1 is not far behind. This confirms our discussion from the map above, where we got the visual impression that tracts in cluster 1 seemed to have the largest area by far, but we missed exactly how large cluster 0 would be.\n",
    "\n",
    "Let's see if this is  the case. One way to do so involves using the `dissolve` operation in `geopandas`, which \n",
    "combines all tracts belonging to each cluster into a single\n",
    "polygon object. After we have dissolved all the members of the clusters,\n",
    "we report the total land area of the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolve areas by Cluster, aggregate by summing,\n",
    "# and keep column for area\n",
    "areas = db.dissolve(by=\"k5cls\", aggfunc=\"sum\")[\"area_sqm\"]\n",
    "areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-aging",
   "metadata": {},
   "source": [
    "We can then use cluster shares to show visually a comparison of the two membership representations (based on land and tracts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-wayne",
   "metadata": {
    "caption": "Measuring cluster size by the number of tracts per cluster and land area per cluster.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bind cluster figures in a single table\n",
    "area_tracts = pd.DataFrame({\"No. Tracts\": k5sizes, \"Area\": areas})\n",
    "# Convert raw values into percentages\n",
    "area_tracts = area_tracts * 100 / area_tracts.sum()\n",
    "# Bar plot\n",
    "ax = area_tracts.plot.bar()\n",
    "# Rename axes\n",
    "ax.set_xlabel(\"Cluster labels\")\n",
    "ax.set_ylabel(\"Percentage by cluster\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-communist",
   "metadata": {},
   "source": [
    "Our visual impression from the map is confirmed: cluster 1 contains tracts that\n",
    "together comprise 8622 square miles (about 22,330 square kilometers)\n",
    "which accounts for well over half of the total land area in the county:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas[1] / areas.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-superintendent",
   "metadata": {},
   "source": [
    "Let's move on to build the profiles for each cluster. Again, the profiles is what\n",
    "provides the conceptual shorthand, moving from the arbitrary label to a meaningful\n",
    "collection of observations with similar attributes. To build a basic profile, we can compute the (unscaled) means of each of the attributes in every cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group table by cluster label, keep the variables used\n",
    "# for clustering, and obtain their mean\n",
    "k5means = db.groupby(\"k5cls\")[cluster_variables].mean()\n",
    "# Transpose the table and print it rounding each value\n",
    "# to three decimals\n",
    "k5means.T.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-basis",
   "metadata": {},
   "source": [
    "Note in this case we do not use scaled measures. This is to create profiles that are easier to interpret and relate to. We see that cluster 3, for example, is composed of tracts that have\n",
    "the highest average `median_house_value`, and also the highest level of inequality\n",
    "(`income_gini`); and cluster 0 contains a younger population (`median_age`)\n",
    "who tend to live in housing units with fewer rooms (`median_no_rooms`).\n",
    "For interpretability, it is useful to consider the raw features, rather than scaled versions that the clusterer sees. However, you can also give profiles in terms of re-scaled features. \n",
    "\n",
    "Average values, however, can hide a great deal of detail and, in some cases,\n",
    "give wrong impressions about the type of data distribution they represent. To\n",
    "obtain more detailed profiles, we could use the `describe` command in `pandas`, \n",
    "after grouping our observations by their clusters:\n",
    "```python\n",
    "#-----------------------------------------------------------#\n",
    "# Illustrative code only, not executed\n",
    "#-----------------------------------------------------------#\n",
    "# Group table by cluster label, keep the variables used \n",
    "# for clustering, and obtain their descriptive summary\n",
    "k5desc = db.groupby('k5cls')[cluster_variables].describe()\n",
    "# Loop over each cluster and print a table with descriptives\n",
    "for cluster in k5desc.T:\n",
    "    print('\\n\\t---------\\n\\tCluster %i'%cluster)\n",
    "    print(k5desc.T[cluster].unstack())\n",
    "#-----------------------------------------------------------#\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-regression",
   "metadata": {},
   "source": [
    "However, this approach quickly gets out of hand: more detailed profiles can simply\n",
    "return to an unwieldy mess of numbers. A better way of constructing\n",
    "cluster profiles is to draw the distributions of cluster members' data.\n",
    "To do this we need to \"tidy up\" the dataset. A tidy dataset {cite}`wickham2014tidy`\n",
    "is one where every row is an observation, and every column is a variable. This is akin to the long-format refered to in [Chapter 9](09_spatial_inequality), and contrasts with the wide-format we used when looking at inequality over time. A few steps are required  to tidy up our labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index db on cluster ID\n",
    "tidy_db = db.set_index(\"k5cls\")\n",
    "# Keep only variables used for clustering\n",
    "tidy_db = tidy_db[cluster_variables]\n",
    "# Stack column names into a column, obtaining\n",
    "# a \"long\" version of the dataset\n",
    "tidy_db = tidy_db.stack()\n",
    "# Take indices into proper columns\n",
    "tidy_db = tidy_db.reset_index()\n",
    "# Rename column names\n",
    "tidy_db = tidy_db.rename(\n",
    "    columns={\"level_1\": \"Attribute\", 0: \"Values\"}\n",
    ")\n",
    "# Check out result\n",
    "tidy_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-morris",
   "metadata": {},
   "source": [
    "Now we are ready to plot. Below, we'll show the distribution of each cluster's values\n",
    "for each variable. This gives us the full distributional profile of each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-journalism",
   "metadata": {
    "caption": "Distributions of each variable for the different cluters."
   },
   "outputs": [],
   "source": [
    "# Scale fonts to make them more readable\n",
    "sns.set(font_scale=1.5)\n",
    "# Setup the facets\n",
    "facets = sns.FacetGrid(\n",
    "    data=tidy_db,\n",
    "    col=\"Attribute\",\n",
    "    hue=\"k5cls\",\n",
    "    sharey=False,\n",
    "    sharex=False,\n",
    "    aspect=2,\n",
    "    col_wrap=3,\n",
    ")\n",
    "# Build the plot from `sns.kdeplot`\n",
    "_ = facets.map(sns.kdeplot, \"Values\", shade=True).add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-story",
   "metadata": {},
   "source": [
    "Note that we create the figure using the facetting functionality in `seaborn`, which\n",
    "streamlines notably the process to create multi-plot figures whose dimensions and\n",
    "content are data-driven. This happens in two steps: first, we set up the frame (`facets`),\n",
    "and then we \"map\" a function (`seaborn.kdeplot`) to the data, within such frame.\n",
    "\n",
    "The figure allows us to see that, while some attributes such as the percentage of\n",
    "female households (`pct_hh_female`) display largely the same distribution for\n",
    "each cluster, others paint a much more divided picture (e.g. `median_house_value`).\n",
    "Taken altogether, these graphs allow us to start delving into the multidimensional \n",
    "complexity of each cluster and the types of areas behind them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-economics",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "As mentioned above, k-means is only one clustering algorithm. There are\n",
    "plenty more. In this section, we will take a similar look at the San Diego\n",
    "dataset using another staple of the clustering toolkit: agglomerative\n",
    "hierarchical clustering (AHC). Agglomerative clustering works by building a hierarchy of\n",
    "clustering solutions that starts with all singletons (each observation is a single\n",
    "cluster in itself) and ends with all observations assigned to the same cluster.\n",
    "These extremes are not very useful in themselves. But, in between, the hierarchy\n",
    "contains many distinct clustering solutions with varying levels of detail. \n",
    "The intuition behind the algorithm is also rather straightforward: \n",
    "\n",
    "1) begin with everyone as part of its own cluster; \n",
    "2) find the two closest observations based on a distance metric (e.g. euclidean); \n",
    "3) join them into a new cluster; \n",
    "4) repeat steps 2) and 3) until reaching the degree of aggregation desired. \n",
    "\n",
    "The algorithm is thus called \"agglomerative\"\n",
    "because it starts with individual clusters and \"agglomerates\" them into fewer\n",
    "and fewer clusters containing more and more observations each. Also, like with \n",
    "k-means, AHC does require the user to specify a number of clusters in advance.\n",
    "This is because, following from the mechanism the method has to build clusters, \n",
    "AHC can provide a solution with as many clusters as observations ($k=n$),\n",
    "or with a only one ($k=1$).\n",
    "\n",
    "Enough of theory, let's get coding! In Python, AHC can be run\n",
    "with `scikit-learn` in very much the same way we did for k-means in the previous\n",
    "section. First we need to import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-access",
   "metadata": {},
   "source": [
    "In this case, we use the `AgglomerativeClustering` class and again \n",
    "use the `fit` method to actually apply the clustering algorithm to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "# Iniciate the algorithm\n",
    "model = AgglomerativeClustering(linkage=\"ward\", n_clusters=5)\n",
    "# Run clustering\n",
    "model.fit(db_scaled)\n",
    "# Assign labels to main data table\n",
    "db[\"ward5\"] = model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-timber",
   "metadata": {},
   "source": [
    "As above, we can check the number of observations that fall within each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ward5sizes = db.groupby(\"ward5\").size()\n",
    "ward5sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-lightweight",
   "metadata": {},
   "source": [
    "Further, we can check the simple average profiles of our clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "ward5means = db.groupby(\"ward5\")[cluster_variables].mean()\n",
    "ward5means.T.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-intention",
   "metadata": {},
   "source": [
    "And again, we can tidy our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index db on cluster ID\n",
    "tidy_db = db.set_index(\"ward5\")\n",
    "# Keep only variables used for clustering\n",
    "tidy_db = tidy_db[cluster_variables]\n",
    "# Stack column names into a column, obtaining\n",
    "# a \"long\" version of the dataset\n",
    "tidy_db = tidy_db.stack()\n",
    "# Take indices into proper columns\n",
    "tidy_db = tidy_db.reset_index()\n",
    "# Rename column names\n",
    "tidy_db = tidy_db.rename(\n",
    "    columns={\"level_1\": \"Attribute\", 0: \"Values\"}\n",
    ")\n",
    "# Check out result\n",
    "tidy_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-queue",
   "metadata": {},
   "source": [
    "And create a plot of the profiles' distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-brave",
   "metadata": {
    "caption": "Distributions of each variable in clusters obtained from Ward's hierarchical clutering.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the facets\n",
    "facets = sns.FacetGrid(\n",
    "    data=tidy_db,\n",
    "    col=\"Attribute\",\n",
    "    hue=\"ward5\",\n",
    "    sharey=False,\n",
    "    sharex=False,\n",
    "    aspect=2,\n",
    "    col_wrap=3,\n",
    ")\n",
    "# Build the plot as a `sns.kdeplot`\n",
    "facets.map(sns.kdeplot, \"Values\", shade=True).add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-globe",
   "metadata": {},
   "source": [
    "For the sake of brevity, we will not spend much time on the plots above.\n",
    "However, the interpretation is analogous to that of the k-means example.\n",
    "\n",
    "On the spatial side, we can explore the geographical dimension of the\n",
    "clustering solution by making a map of the clusters. To make the comparison\n",
    "with k-means simpler, we will display both side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-wheel",
   "metadata": {
    "caption": "Two clutering solutions, one for the K-means solution, and the other for Ward's hierarchical clutering. Note that colorings cannot be directly compared between the two maps.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "db[\"ward5\"] = model.labels_\n",
    "# Setup figure and ax\n",
    "f, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "### K-Means ###\n",
    "ax = axs[0]\n",
    "# Plot unique values choropleth including\n",
    "# a legend and with no boundary lines\n",
    "db.plot(\n",
    "    column=\"ward5\",\n",
    "    categorical=True,\n",
    "    cmap=\"Set2\",\n",
    "    legend=True,\n",
    "    linewidth=0,\n",
    "    ax=ax,\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Add title\n",
    "ax.set_title(\"K-Means solution ($k=5$)\")\n",
    "\n",
    "### AHC ###\n",
    "ax = axs[1]\n",
    "# Plot unique values choropleth including\n",
    "# a legend and with no boundary lines\n",
    "db.plot(\n",
    "    column=\"k5cls\",\n",
    "    categorical=True,\n",
    "    cmap=\"Set3\",\n",
    "    legend=True,\n",
    "    linewidth=0,\n",
    "    ax=ax,\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Add title\n",
    "ax.set_title(\"AHC solution ($k=5$)\")\n",
    "\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-sussex",
   "metadata": {},
   "source": [
    "While we must remember our earlier caveat about how irregular polygons can \n",
    "baffle our visual intuition, a closer visual inspection of the cluster geography\n",
    "suggests a clear pattern: although they are not identical, both clustering solutions capture\n",
    "very similar overall spatial structure. Furthermore, both solutions slightly violate \n",
    "Tobler's law in the sense all of the clusters have disconnected components. The five\n",
    "multivariate clusters in each case are actually composed of many disparate \n",
    "geographical areas, strewn around the map according only to the structure of the\n",
    "data and not its geography. That is, in order to travel to\n",
    "every tract belonging to a cluster, we would have to journey through\n",
    "other clusters as well.\n",
    "\n",
    "## Regionalisation: Spatially Constrained Hierarchical Clustering\n",
    "\n",
    "### Contiguity constraint\n",
    "\n",
    "Fragmented clusters are not intrinsically invalid, particularly if we are\n",
    "interested in exploring the overall structure and geography of multivariate\n",
    "data. However, in some cases, the application we are interested in might\n",
    "require that all the observations in a class be spatially connected. For\n",
    "example, when detecting communities or neighborhoods (as is sometimes needed when\n",
    "drawing electoral or census boundaries), they are nearly always distinct \n",
    "self-connected areas, unlike our clusters shown above. To ensure that clusters are\n",
    "not spatially fragmented, we turn to regionalization.\n",
    "\n",
    "Regionalization methods are clustering techniques that impose a spatial constraint\n",
    "on clusters. In other words, the result of a regionalization algorithm contains clusters with\n",
    "areas that are geographically coherent, in addition to having coherent data profiles. \n",
    "Effectively, this means that regionalization methods construct clusters that are \n",
    "all internally-connected; these are the *regions*. Thus, a regions' members must\n",
    "be geographically *nested* within the region's boundaries.\n",
    "\n",
    "This type of nesting relationship is easy to identify\n",
    "in the real world. Census geographies provide good examples: counties nest within states\n",
    "in the US; or local super output areas (LSOAs) nest within middle super output areas \n",
    "(MSOAs) in the UK. \n",
    "The difference between these real-world nestings and the output of a regionalization\n",
    "algorithm is that the real-world nestings are aggregated according to administrative\n",
    "principles, while regions' members are aggregated according to statistical similarity. In the same manner as the\n",
    "clustering techniques explored above, these regionalization methods aggregate \n",
    "observations that are similar in their attributes; the profiles of regions are useful\n",
    "in a similar manner as the profiles of clusters. But, in regionalization, the \n",
    "clustering is also spatially constrained, so the region profiles and members will\n",
    "likely be different from the unconstrained solutions.\n",
    "\n",
    "As in the non-spatial case, there are many different regionalization methods.\n",
    "Each has a different way to measure (dis)similarity, how the similarity is used\n",
    "to assign labels, how these labels are iteratively adjusted, and so on. However,\n",
    "as with clustering algorithms, regionalization methods all share a few common traits.\n",
    "In particular, they all take a set of input attributes and a representation of \n",
    "spatial connectivity in the form of a binary spatial weights matrix. Depending \n",
    "on the algorithm, they also require the desired number of output regions. For\n",
    "illustration, we will take the AHC algorithm we have just used above, and apply \n",
    "an additional spatial constraint. In `scikit-learn`, this is done using\n",
    "our spatial weights matrix as a `connectivity` option.\n",
    "This parameter will force the agglomerative algorithm to only allow observations to be grouped\n",
    "in a cluster if they are also spatially connected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "# Specify cluster model with spatial constraint\n",
    "model = AgglomerativeClustering(\n",
    "    linkage=\"ward\", connectivity=w.sparse, n_clusters=5\n",
    ")\n",
    "# Fit algorithm to the data\n",
    "model.fit(db_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-tutorial",
   "metadata": {},
   "source": [
    "Let's inspect the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-location",
   "metadata": {
    "caption": "Spatially-constrained clusters, or 'regions', of San Diego using Ward's hierarchical clustering.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "db[\"ward5wq\"] = model.labels_\n",
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot unique values choropleth including a legend and with no boundary lines\n",
    "db.plot(\n",
    "    column=\"ward5wq\",\n",
    "    categorical=True,\n",
    "    legend=True,\n",
    "    linewidth=0,\n",
    "    ax=ax,\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-tiger",
   "metadata": {},
   "source": [
    "Introducing the spatial constraint results in fully-connected clusters with much\n",
    "more concentrated spatial distributions. From an initial visual impression, it might\n",
    "appear that our spatial constraint has been violated: there are tracts for both cluster 0 and\n",
    "cluster 1 that appear to be disconnected from the rest of their clusters.\n",
    "However, closer inspection reveals that each of these tracts is indeed connected\n",
    "to another tract in its own cluster by very narrow shared boundaries.\n",
    "\n",
    "### Changing the spatial constraint\n",
    "\n",
    "The spatial constraint in regionalization algorithms is structured by the\n",
    "spatial weights matrix we use. An interesting\n",
    "question is thus how the choice of weights influences the final region structure.\n",
    "Fortunately, we can directly explore the impact that a change in the spatial weights matrix has on\n",
    "regionalization. To do so, we use the same attribute data\n",
    "but replace the Queen contiguity matrix with a spatial k-nearest neighbor matrix,\n",
    "where each observation is connected to its four nearest observations, instead\n",
    "of those it touches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = KNN.from_dataframe(db, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-brazil",
   "metadata": {},
   "source": [
    "With this matrix connecting each tract to the four closest tracts, we can run \n",
    "another AHC regionalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "# Specify cluster model with spatial constraint\n",
    "model = AgglomerativeClustering(\n",
    "    linkage=\"ward\", connectivity=w.sparse, n_clusters=5\n",
    ")\n",
    "# Fit algorithm to the data\n",
    "model.fit(db_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-occasion",
   "metadata": {},
   "source": [
    "And plot the final regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-argentina",
   "metadata": {
    "caption": "Regions from a spatially-constrained sociodemographic clutering, using a different connectivity constraint. Code generated for this figure is available on the web version of the book.",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "db[\"ward5wknn\"] = model.labels_\n",
    "# Setup figure and ax\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot unique values choropleth\n",
    "# including a legend and with no boundary lines\n",
    "db.plot(\n",
    "    column=\"ward5wknn\",\n",
    "    categorical=True,\n",
    "    legend=True,\n",
    "    linewidth=0,\n",
    "    ax=ax,\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-incidence",
   "metadata": {},
   "source": [
    "Even though we have specified a spatial constraint, the constraint applies to the\n",
    "connectivity graph modeled by our weights matrix. Therefore, using k-nearest neighbors\n",
    "to constrain the agglomerative clustering may not result in regions that are connected\n",
    "according to a different connectivity rule, such as the queen contiguity rule used\n",
    "in the previous section. However, the regionalization here is fortuitous; even though\n",
    "we used the 4-nearest tracts to constrain connectivity, all of our clusters are also connected according to the Queen contiguity rule. \n",
    "\n",
    "So, which one is a \"better\" regionalization? Well, regionalizations are often compared based on measures of *geographical coherence*, as well as measures of *cluster coherence*. The former involves measures of cluster *shape* that can answer to questions like \"are clusters evenly-sized, or are they very differently sized? are clusters very strangely-shaped, or are they compact?\";\n",
    "while the latter generally focuses on whether cluster observations are more similar to their current clusters than to other clusters. This *goodness of fit* is usually better for unconstrained clustering algorithms than for the corresponding regionalizations. We'll show this next. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-seating",
   "metadata": {},
   "source": [
    "### Geographical coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-tooth",
   "metadata": {},
   "source": [
    "One very simple measure of geographical coherence involves the \"compactness\" of a given shape. The most common of these measures is the isoperimetric quotient {cite}`Horn1993`. This compares the area of the region to the area of a circle with the same perimeter as the region. To obtain the statistic, we can recognize that the circumference of the circle $c$ is the same as the perimeter of the region $i$, so $P_i = 2\\pi r_c$. Then, the area of the isoperimetric circle is $A_c = \\pi r_c^2 = \\pi \\left(\\frac{P_i}{2 \\pi}\\right)^2$. Simplifying, we get:\n",
    "\n",
    "$$ IPQ_i = \\frac{A_i}{A_c} = \\frac{4 \\pi A_i}{P_i^2}$$\n",
    "\n",
    "For this measure, more compact shapes have an IPQ closer to 1, whereas very elongated or spindly shapes will have IPQs closer to zero. For the clustering solutions, we would expect the IPQ to be very small indeed, since the perimeter of a cluster/region gets smaller the more boundaries that members share. \n",
    "\n",
    "Computing this, then, can be done directly from the area and perimeter of a region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for cluster_type in (\"k5cls\", \"ward5\", \"ward5wq\", \"ward5wknn\"):\n",
    "    # compute the region polygons using a dissolve\n",
    "    regions = db[[cluster_type, \"geometry\"]].dissolve(by=cluster_type)\n",
    "    # compute the actual isoperimetric quotient for these regions\n",
    "    ipqs = (\n",
    "        regions.area * 4 * np.pi / (regions.boundary.length ** 2)\n",
    "    )\n",
    "    # cast to a dataframe\n",
    "    result = ipqs.to_frame(cluster_type)\n",
    "    results.append(result)\n",
    "# stack the series together along columns\n",
    "pd.concat(results, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-elite",
   "metadata": {},
   "source": [
    "From this, we can see that the *shape* measures for the clusters are much better under the regionalizations than under the clustering solutions. As we'll show in the next section, this comes at the cost of goodness of fit. Alternatively, the two spatial solutions have different compactness values; the knn-based regions are much more compact than the queen weights-based solutions. The most compact region in the Queen regionalization is about at the median of the knn solutions. \n",
    "\n",
    "Many other measures of shape regularity exist. Most of the well-used ones are implemented in the `esda.shapestats` module, which also documents the sensitivity of the different measures of shape. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-trouble",
   "metadata": {},
   "source": [
    "### Feature coherence (goodness of fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-container",
   "metadata": {},
   "source": [
    "Many measures of the feature coherence, or *goodness of fit*, are implemented in scikit-learn's `metrics` module, which we used earlier to compute distances. This metrics module also contains a few goodness of fit statistics that measure, for example:\n",
    "\n",
    "- `metrics.calinski_harabasz_score()` (CH): the within-cluster variance divided by the between-cluster variance\n",
    "- `metrics.silhouette_score()`: the average standardized distance from each observation to its \"next best fit\" cluster—the most similar cluster to which the observation is *not* currently assigned.\n",
    "\n",
    "To compute these, each scoring function requires both the original data and the labels which have been fit. We'll compute the CH score for all the different clusterings below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_scores = []\n",
    "for cluster_type in (\"k5cls\", \"ward5\", \"ward5wq\", \"ward5wknn\"):\n",
    "    # compute the CH score\n",
    "    ch_score = metrics.calinski_harabasz_score(\n",
    "        # using scaled variables\n",
    "        robust_scale(db[cluster_variables]),\n",
    "        # using these labels\n",
    "        db[cluster_type],\n",
    "    )\n",
    "    # and append the cluster type with the CH score\n",
    "    ch_scores.append((cluster_type, ch_score))\n",
    "\n",
    "# re-arrange the scores into a dataframe for display\n",
    "pd.DataFrame(\n",
    "    ch_scores, columns=[\"cluster type\", \"CH score\"]\n",
    ").set_index(\"cluster type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-serbia",
   "metadata": {},
   "source": [
    "For all functions in `metrics` that end in \"score\", higher numbers indicate greater fit, whereas functions that end in `loss` work in the other direction. Thus, the K-means solution has the highest Calinski-Harabasz score, while the ward clustering comes second. The regionalizations both come *well* below the clusterings, too. As we said before, the improved geographical coherence comes at a pretty hefty cost in terms of feature goodness of fit. This is because regionalization is *constrained*, and mathematically *can not* achieve the same score as the unconstrained K-means solution, unless we get lucky and the k-means solution *is* a valid regionalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-zambia",
   "metadata": {},
   "source": [
    "### Solution Similarity\n",
    "\n",
    "The `metrics` module also contains useful tools to compare whether the labellings generated from different clustering algorithms are similar, such as the Adjusted Rand Score or the Mutual Information Score. To show that, we can see how similar clusterings are to one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_scores = []\n",
    "# for each cluster solution\n",
    "for i_cluster_type in (\"k5cls\", \"ward5\", \"ward5wq\", \"ward5wknn\"):\n",
    "    # for every other clustering\n",
    "    for j_cluster_type in (\"k5cls\", \"ward5\", \"ward5wq\", \"ward5wknn\"):\n",
    "        # compute the adjusted mutual info between the two\n",
    "        ami_score = metrics.adjusted_mutual_info_score(\n",
    "            db[i_cluster_type], db[j_cluster_type]\n",
    "        )\n",
    "        # and save the pair of cluster types with the score\n",
    "        ami_scores.append((i_cluster_type, j_cluster_type, ami_score))\n",
    "# arrange the results into a dataframe\n",
    "results = pd.DataFrame(\n",
    "    ami_scores, columns=[\"source\", \"target\", \"similarity\"]\n",
    ")\n",
    "# and spread the dataframe out into a square\n",
    "results.pivot(\"source\", \"target\", \"similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-compression",
   "metadata": {},
   "source": [
    "From this, we can see that the K-means and Ward clusterings are the most self-similar, and the two regionalizations are slightly less similar to one another than the clusterings. The regionalizations are generally *not* very similar to the clusterings, as would be expected from our discussions above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-render",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Overall, clustering and regionalization are two complementary tools to reduce\n",
    "complexity in multivariate data and build better understandings of their spatial structure.\n",
    "Often, there is simply too much data to examine every variable's map and its\n",
    "relation to all other variable maps. \n",
    "Thus, clustering reduces this complexity into a single conceptual shorthand by which \n",
    "people can easily describe complex and multifaceted data. \n",
    "Clustering constructs groups of observations (called *clusters*)\n",
    "with coherent *profiles*, or distinct and internally-consistent \n",
    "distributional/descriptive characteristics. \n",
    "These profiles are the conceptual shorthand, since members of each cluster should\n",
    "be more similar to the cluster at large than they are to any other cluster. \n",
    "Many different clustering methods exist; they differ on how the cluster\n",
    "is defined, and how \"similar\" members must be to clusters, or how these clusters\n",
    "are obtained.\n",
    "Regionalization is a special kind of clustering that imposes an additional geographic requirement. \n",
    "Observations should be grouped so that each spatial cluster,\n",
    "or *region*, is spatially-coherent as well as data-coherent. \n",
    "Thus, regionalization is often concerned with connectivity in a contiguity \n",
    "graph for data collected in areas; this ensures that the regions that are identified\n",
    "are fully internally-connected. \n",
    "However, since many regionalization methods are defined for an arbitrary connectivity structure,\n",
    "these graphs can be constructed according to different rules as well, such as the k-nearest neighbor graph. \n",
    "Finally, while regionalizations are usually more geographically-coherent, they are also usually worse-fit to the features at hand. This reflects an intrinsic tradeoff that, in general, cannot be removed. \n",
    "\n",
    "In this chapter, we discussed the conceptual basis for clustering and regionalization, \n",
    "as well as showing why clustering is done. \n",
    "Further, we have demonstrated how to build clusters using a combination of (geographic) data \n",
    "science packages, and how to interrogate the meaning of these clusters as well.\n",
    "More generally, clusters are often used in predictive and explanatory settings, \n",
    "in addition to being used for exploratory analysis in their own right.\n",
    "Clustering and regionalization are intimately related to the analysis of spatial autocorrelation as well,\n",
    "since the spatial structure and covariation in multivariate spatial data is what\n",
    "determines the spatial structure and data profile of discovered clusters or regions.\n",
    "Thus, clustering and regionalization are essential tools for the geographic data scientist.\n",
    "\n",
    "## Q.1 \n",
    "Describe how you might use geodemographics and regionalization in your own research or projects. Describe the research or project topic and datasets or kind of datasets.  Why might you chose one over another? (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d65966-171d-4d57-a598-0d37655d940d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
