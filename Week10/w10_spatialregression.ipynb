{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-gambling",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99a7a244",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Understand the differences between SLX, spatial lag and spatial error models\n",
    "- Identify when to use different kinds of models\n",
    "- Implement all three types of models in pysal\n",
    "- Check our model outcomes \n",
    "\n",
    "This week's lesson is a simplied version of:  \n",
    "- The [Week 11 on Spatial Regression the Geographic Data Science book](https://geographicdata.science/book/notebooks/11_regression.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-introduction",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pysal.lib import weights\n",
    "from pysal.explore import esda\n",
    "import numpy\n",
    "import pandas\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import contextily"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "weekly-power",
   "metadata": {},
   "source": [
    "## Data: San Diego Airbnb\n",
    "\n",
    "To learn a little more about how regression works, we'll examine information about Airbnb properties in San Diego, CA. \n",
    "This dataset contains house intrinsic characteristics, both continuous (number of beds as in `beds`) and categorical (type of renting or, in Airbnb jargon, property group as in the series of `pg_X` binary variables), but also variables that explicitly refer to the location and spatial configuration of the dataset (e.g., distance to Balboa Park, `d2balboa` or neighborhood id, `neighborhood_cleansed`).\n",
    "\n",
    "Also, note that there is a great notebook in the Geographic Data Science book on [how this dataset was cleaned](https://geographicdata.science/book/data/airbnb/regression_cleaning.html). It has some example code for: \n",
    "- How to calculate the driving distance to a certain location (Balboa Park in this example) using a free API called Nomatim, since we only learned about the Google Maps API. \n",
    "- How to get the elevation of a location\n",
    "- How to categorize neighborhoods (here, by size and whether they are coastal).\n",
    "- Creating dummy variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = geopandas.read_file(\"https://www.dropbox.com/s/zkucu7jf1xug869/regression_db.geojson?dl=1\")\n",
    "# db['pool'] = db['pool'].astype(int)\n",
    "# db['coastal'] = db['coastal'].astype(int)\n",
    "# db['pg_Apartment'] = db['pg_Apartment'].astype(int)\n",
    "# db['pg_Condominium'] = db['pg_Condominium'].astype(int)\n",
    "# db['pg_House'] = db['pg_House'].astype(int)\n",
    "# db['pg_Other'] = db['pg_Other'].astype(int)\n",
    "# db['pg_Townhouse'] = db['pg_Townhouse'].astype(int)\n",
    "# db['rt_Entire_home/apt'] = db['rt_Entire_home/apt'].astype(int)\n",
    "# db['rt_Private_room'] = db['rt_Private_room'].astype(int)\n",
    "# db['rt_Shared_room'] = db['rt_Shared_room'].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6174fca9",
   "metadata": {},
   "source": [
    "Notice here that we have: \n",
    "- **Discrete variables** (number of bedrooms, beds, baths)\n",
    "- **Dummy variables** (whether there is a pool, whether near the coast, room type)\n",
    "\n",
    "**Remember that for dummy variables we always run the regression leaving out one category as our baseline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab78024",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-collins",
   "metadata": {},
   "source": [
    "These are the explanatory variables we will use throughout the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = [\n",
    "    \"accommodates\",  # Number of people it accommodates\n",
    "    \"bathrooms\",  # Number of bathrooms\n",
    "    \"bedrooms\",  # Number of bedrooms\n",
    "    \"beds\",  # Number of beds\n",
    "    # Below are binary variables, 1 True, 0 False\n",
    "    \"rt_Private_room\",  # Room type: private room\n",
    "    \"rt_Shared_room\",  # Room type: shared room\n",
    "    \"pg_Condominium\",  # Property group: condo\n",
    "    \"pg_House\",  # Property group: house\n",
    "    \"pg_Other\",  # Property group: other\n",
    "    \"pg_Townhouse\",  # Property group: townhouse\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "legitimate-telling",
   "metadata": {},
   "source": [
    "## Non-spatial regression, a (very) quick refresh\n",
    "For example, in our case, we may want to express the price of a house as a function of the number of bedrooms it has and whether it is a condominium or not. At the individual level, we can express this as:\n",
    "\n",
    "$$\n",
    "log(P_i) = \\alpha + \\sum_k \\mathbf{X}_{ik}\\beta_k  + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $P_i$ is the Airbnb price of house $i$, and $X$ is a set of covariates that we use to explain such price (e.g., No. of bedrooms and condominium binary variable). $\\beta$ is a vector of parameters that give us information about in which way and to what extent each variable is related to the price, and $\\alpha$, the constant term, is the average house price when all the other variables are zero. The term $\\epsilon_i$ is usually referred to as \"error\" and captures elements that influence the price of a house but are not included in $X$.\n",
    "\n",
    "We also take the log of prices often since a log scale allows us a better understanding of the percentage change instead of the absolute dollar change.  \n",
    "\n",
    "We can also express this relation in matrix form, excluding sub-indices for $i$, which yields:\n",
    "\n",
    "$$\n",
    "log(P) = \\alpha + \\mathbf{X}\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "Practically speaking, linear regressions in Python are rather streamlined and easy to work with. There are also several packages which will run them (e.g., `statsmodels`, `scikit-learn`, `pysal`). We will import the `spreg` module in Pysal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-block",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pysal.model import spreg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-porter",
   "metadata": {},
   "source": [
    "In the context of this chapter, it makes sense to start with `spreg`, as that is the only library that will allow us to move into explicitly spatial econometric models. To fit the model specified in the equation above with $X$ as the list defined, using ordinary least squares (OLS), we only need the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS model\n",
    "m1 = spreg.OLS(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    db[variable_names].values,\n",
    "    # Dependent variable name\n",
    "    name_y=\"log_price\",\n",
    "    # Independent variable name\n",
    "    name_x=variable_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-procedure",
   "metadata": {},
   "source": [
    "We use the command `OLS`, part of the `spreg` sub-package, and specify the dependent variable (the log of the price, so we can interpret results in terms of percentage change) and the explanatory ones. Note that both objects need to be arrays, so we extract them from the `pandas.DataFrame` object using `.values`.\n",
    "\n",
    "In order to inspect the results of the model, we can print the `summary` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m1.summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "multiple-scratch",
   "metadata": {},
   "source": [
    "Results are largely as expected: houses tend to be significantly more expensive if they accommodate more people (`accommodates`), if they have more bathrooms and bedrooms, and if they are a condominium or part of the \"other\" category of house type. Conversely, given a number of rooms, houses with more beds (i.e., listings that are more \"crowded\") tend to go for cheaper, as it is the case for properties where one does not rent the entire house but only a room (`rt_Private_room`) or even shares it (`rt_Shared_room`). Of course, you might conceptually doubt the assumption that it is possible to *arbitrarily* change the number of beds within an Airbnb without eventually changing the number of people it accommodates, but methods to address these concerns using *interaction effects* won't be discussed here. \n",
    "\n",
    "### Hidden structures\n",
    "\n",
    "In general, our model performs well, being able to predict slightly about two-thirds ($R^2=0.67$) of the variation in the mean nightly price using the covariates we've discussed above.\n",
    "But, our model might display some clustering in the errors, which may be a problem as that violates the i.i.d. assumption linear models usually come built-in with. \n",
    "To interrogate this, we can do a few things. \n",
    "One simple concept might be to look at the correlation between the error in predicting an Airbnb and the error in predicting its nearest neighbor. \n",
    "To examine this, we first might want to split our data up by regions and see if we've got some spatial structure in our residuals. \n",
    "One reasonable theory might be that our model does not include any information about *beaches*, a critical aspect of why people live and vacation in San Diego. \n",
    "Therefore, we might want to see whether or not our errors are higher or lower depending on whether or not an Airbnb is in a \"beach\" neighborhood, a neighborhood near the ocean. We use the code below to generate Figure XXX1XXX, which looks at prices between the two groups of houses, \"beach\" and \"no beach\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb874ea9",
   "metadata": {},
   "source": [
    "Note here that `m1.u` are the residuals from the model `m1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-above",
   "metadata": {
    "caption": "Distributions of prediction errors (residuals) for the basic linear model. Residuals for coastal Airbnbs are generally positive, meaning that the model under-predicts their prices.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boolean (True/False) with whether a\n",
    "# property is coastal or not\n",
    "is_coastal = db.coastal.astype(bool)\n",
    "# Split residuals (m1.u) between coastal and not\n",
    "coastal = m1.u[is_coastal]\n",
    "not_coastal = m1.u[~is_coastal]\n",
    "# Create histogram of the distribution of coastal residuals\n",
    "plt.hist(coastal, density=True, label=\"Coastal\")\n",
    "# Create histogram of the distribution of non-coastal residuals\n",
    "plt.hist(\n",
    "    not_coastal,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    linewidth=4,\n",
    "    label=\"Not Coastal\",\n",
    ")\n",
    "# Add Line on 0\n",
    "plt.vlines(0, 0, 1, linestyle=\":\", color=\"k\", linewidth=4)\n",
    "# Add legend\n",
    "plt.legend()\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-romantic",
   "metadata": {},
   "source": [
    "While it appears that the neighborhoods on the coast have only slightly higher average errors (and have lower variance in their prediction errors), the two distributions are significantly distinct from one another when compared using a classic $t$-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(coastal, not_coastal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-species",
   "metadata": {},
   "source": [
    "There are more sophisticated (and harder to fool) tests that may be applicable for this data, however. We cover them in the [Challenge](#Challenge) section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-ranking",
   "metadata": {},
   "source": [
    "Additionally, it might be the case that some neighborhoods are more desirable than other neighborhoods due to unmodeled latent preferences or marketing. \n",
    "For instance, despite its presence close to the sea, living near Camp Pendleton -a Marine base in the North of the city- may incur some significant penalties on area desirability due to noise and pollution. These are questions that domain knowledge provides and data analysis can help us answer.\n",
    "For us to determine whether this is the case, we might be interested in the full distribution of model residuals within each neighborhood. \n",
    "\n",
    "To make this more clear, we'll first sort the data by the median residual in that neighborhood, and then make a boxplot (Fig. XXX2XXX), which shows the distribution of residuals in each neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-symposium",
   "metadata": {
    "caption": "Boxplot of prediction errors by neighborhood in San Diego, showing that the basic model systematically over- (or under-) predicts the nightly price of some neighborhoods' Airbnbs.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create column with residual values from m1\n",
    "db[\"residual\"] = m1.u\n",
    "# Obtain the median value of residuals in each neighborhood\n",
    "medians = (\n",
    "    db.groupby(\"neighborhood\")\n",
    "    .residual.median()\n",
    "    .to_frame(\"hood_residual\")\n",
    ")\n",
    "\n",
    "# Increase fontsize\n",
    "seaborn.set(font_scale=1.25)\n",
    "# Set up figure\n",
    "f = plt.figure(figsize=(15, 3))\n",
    "# Grab figure's axis\n",
    "ax = plt.gca()\n",
    "# Generate bloxplot of values by neighborhood\n",
    "# Note the data includes the median values merged on-the-fly\n",
    "seaborn.boxplot(\n",
    "    data=db.merge(\n",
    "        medians, how=\"left\", left_on=\"neighborhood\", right_index=True\n",
    "    ).sort_values(\"hood_residual\"),\n",
    "    x=\"neighborhood\",\n",
    "    y=\"residual\",\n",
    "    ax=ax,\n",
    "    palette=\"bwr\",\n",
    ")\n",
    "# Auto-format of the X labels\n",
    "f.autofmt_xdate()\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "growing-shock",
   "metadata": {},
   "source": [
    "No neighborhood is disjoint from one another, but some do appear to be higher than others, such as the well-known downtown tourist neighborhoods areas of the Gaslamp Quarter, Little Italy, or The Core. \n",
    "Thus, there may be a distinctive effect of intangible neighborhood fashionableness that matters in this model. \n",
    "\n",
    "Noting that many of the most over- and under-predicted neighborhoods are near one another in the city, it may also be the case that there is some sort of *contagion* or spatial spillovers in the nightly rent price. \n",
    "This often is apparent when individuals seek to price their Airbnb listings to compete with similar nearby listings. \n",
    "Since our model is not aware of this behavior, its errors may tend to cluster. \n",
    "**One exceptionally simple way we can look into this structure is by examining the relationship between an observation's residuals and its surrounding residuals.**\n",
    "\n",
    "To do this, we will use *spatial weights* to represent the geographic relationships between observations. \n",
    "We cover spatial weights in detail in [Chapter 4](04_spatial_weights), so we will not repeat ourselves here.\n",
    "For this example, we'll start off with a $KNN$ matrix where $k=1$, meaning we're focusing only on the linkages of each Airbnb to their closest other listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = weights.KNN.from_dataframe(db, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-billion",
   "metadata": {},
   "source": [
    "This means that, when we compute the *spatial lag* of that $KNN$ weight and the residual, we get the residual of the Airbnb listing closest to each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-termination",
   "metadata": {
    "caption": "The relationship between prediction error for an Airbnb and the nearest Airbnb's prediction error. This suggests that if an Airbnb's nightly price is over-predicted, its nearby Airbnbs will also be over-predicted.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here lag_spatial is a function that returns the spatial lag of a variable\n",
    "## given a certain set of weights\n",
    "\n",
    "lag_residual = weights.spatial_lag.lag_spatial(knn, m1.u)\n",
    "ax = seaborn.regplot(\n",
    "    x=m1.u.flatten(),\n",
    "    y=lag_residual.flatten(),\n",
    "    line_kws=dict(color=\"orangered\"),\n",
    "    scatter_kws={'alpha': 0.3,'s':10},\n",
    "    ci=None,\n",
    ")\n",
    "ax.set_xlabel(\"Model Residuals - $u$\")\n",
    "ax.set_ylabel(\"Spatial Lag of Model Residuals - $W u$\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "positive-salad",
   "metadata": {},
   "source": [
    "In Figure XXX3XXX, we see that our prediction errors tend to cluster!\n",
    "Above, we show the relationship between our prediction error at each site and the prediction error at the site nearest to it. \n",
    "Here, we're using this nearest site to stand in for the *surroundings* of that Airbnb. \n",
    "This means that, when the model tends to over-predict a given Airbnb's nightly log price, sites around that Airbnb are more likely to *also be over-predicted*. \n",
    "\n",
    "Let's look at the stable $k=20$ number of neighbors.\n",
    "Examining the relationship between this stable *surrounding* average and the focal Airbnb, we can even find clusters in our model error. \n",
    "Recalling the *local Moran* statistics in [Chapter 7](07_local_autocorrelation), Figure XXX4XXX is generated from the code below to identify certain areas where our predictions of the nightly (log) Airbnb price tend to be significantly off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-annotation",
   "metadata": {
    "caption": "Map of clusters in regression errors, according to the Local Moran's $I_i$.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Re-weight W to 20 nearest neighbors\n",
    "knn = weights.KNN.from_dataframe(db, k=20)# Row standardize weights\n",
    "\n",
    "knn.transform = \"R\"\n",
    "# Run LISA on residuals\n",
    "outliers = esda.moran.Moran_Local(m1.u, knn, permutations=9999)\n",
    "# Select only LISA cluster cores\n",
    "error_clusters = outliers.q % 2 == 1\n",
    "# Filter out non-significant clusters\n",
    "error_clusters &= outliers.p_sim <= 0.001\n",
    "# Add `error_clusters` and `local_I` columns\n",
    "ax = (\n",
    "    db.assign(\n",
    "        error_clusters=error_clusters,\n",
    "        local_I=outliers.Is\n",
    "        # Retain error clusters only\n",
    "    )\n",
    "    .query(\n",
    "        \"error_clusters\"\n",
    "        # Sort by I value to largest plot on top\n",
    "    )\n",
    "    .sort_values(\n",
    "        \"local_I\"\n",
    "        # Plot I values\n",
    "    )\n",
    "    .plot(\"local_I\", cmap=\"bwr\", marker=\".\")\n",
    ")\n",
    "# Add basemap\n",
    "contextily.add_basemap(ax, crs=db.crs)\n",
    "# Remove axes\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-examination",
   "metadata": {},
   "source": [
    "Thus, these areas tend to be locations where our model significantly under-predicts the nightly Airbnb price both for that specific observation and observations in its immediate surroundings. \n",
    "This is critical since, if we can identify how these areas are structured &mdash; if they have a *consistent geography* that we can model &mdash; then we might make our predictions even better, or at least not systematically mis-predict prices in some areas while correctly predicting prices in other areas. Since significant under- and over-predictions do appear to cluster in a highly structured way, we might be able to use a better model to fix the geography of our model errors. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "neural-understanding",
   "metadata": {},
   "source": [
    "### Spatial feature engineering: proximity variables\n",
    "One relevant proximity-driven variable that could influence our San Diego model is based on the listings proximity to Balboa Park. A common tourist destination, Balboa Park is a central recreation hub for the city of San Diego, containing many museums and the San Diego Zoo. Thus, it could be the case that people searching for Airbnbs in San Diego are willing to pay a premium to live closer to the park. If this were true *and* we omitted this from our model, we may indeed see a significant spatial pattern caused by this distance decay effect. \n",
    "\n",
    "Therefore, this is sometimes called a *spatially patterned omitted covariate*: geographic information our model needs to make good predictions which we have left out of our model. Therefore, let's build a new model containing this distance to Balboa Park covariate. First, though, it helps to visualize (Fig. XXX5XXX) the structure of this distance covariate itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-sapphire",
   "metadata": {
    "caption": "A map showing the 'Distance to Balboa Park' variable.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = db.plot(\"d2balboa\", marker=\".\", s=5)\n",
    "contextily.add_basemap(ax, crs=db.crs)\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-insulin",
   "metadata": {},
   "source": [
    "To run a linear model that includes the additional variable of distance to the park, we add the name to the list of variables we included originally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "balboa_names = variable_names + [\"d2balboa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-practitioner",
   "metadata": {},
   "source": [
    "And then fit the model using the OLS class in Pysal's `spreg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = spreg.OLS(\n",
    "    db[[\"log_price\"]].values,\n",
    "    db[balboa_names].values,\n",
    "    name_y=\"log_price\",\n",
    "    name_x=balboa_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m2.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-bahrain",
   "metadata": {},
   "source": [
    "When you inspect the regression diagnostics and output, you see that this covariate is not quite as helpful as we might anticipate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(\n",
    "    [[m1.r2, m1.ar2], [m2.r2, m2.ar2]],\n",
    "    index=[\"M1\", \"M2\"],\n",
    "    columns=[\"R2\", \"Adj. R2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-advertising",
   "metadata": {},
   "source": [
    "And, there still appears to be spatial structure in our model's errors, as we can see in Figure XXX6XXX, generated by the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-treat",
   "metadata": {
    "caption": "The relationship between prediction error and the nearest Airbnb's prediction error for the model including the 'Distance to Balboa Park' variable. Note the much stronger relationship here than before.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lag_residual = weights.spatial_lag.lag_spatial(knn, m2.u)\n",
    "ax = seaborn.regplot(\n",
    "    x=m2.u.flatten(),\n",
    "    y=lag_residual.flatten(),\n",
    "    line_kws=dict(color=\"orangered\"),\n",
    "    ci=None,\n",
    ")\n",
    "ax.set_xlabel(\"Residuals ($u$)\")\n",
    "ax.set_ylabel(\"Spatial lag of residuals ($Wu$)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-tuition",
   "metadata": {},
   "source": [
    "Finally, the distance to Balboa Park variable does not fit our theory about how distance to amenity should affect the price of an Airbnb; the coefficient estimate is *positive*, meaning that people are paying a premium to be *further* from Balboa Park. We will revisit this result later on, when we consider spatial heterogeneity and will be able to shed some light on this. Further, the next chapter is an extensive treatment of spatial fixed effects, presenting many more spatial feature engineering methods. Here, we have only showed how to include these engineered features in a standard linear modeling framework. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "functioning-enlargement",
   "metadata": {},
   "source": [
    "### Spatial heterogeneity\n",
    "\n",
    "#### Spatial fixed effects\n",
    "Mathematically, we are now fitting the following equation:\n",
    "\n",
    "$$\n",
    "\\log{P_i} = \\alpha_r + \\sum_k \\mathbf{X}_{ik}\\beta_k  + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where the main difference is that we are now allowing the constant term, $\\alpha$, to vary by neighborhood $r$, $\\alpha_r$.\n",
    "\n",
    "Programmatically, we will show two different ways we can estimate this: one,\n",
    "using `statsmodels`; and two, with `spreg`. First, we will use `statsmodels`, the econometrician's toolbox in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-wages",
   "metadata": {},
   "source": [
    "This package provides a formula-like API, which allows us to express the *equation* we wish to estimate directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (\n",
    "    \"log_price ~ \"\n",
    "    + \" + \".join(variable_names)\n",
    "    + \" + neighborhood - 1\"\n",
    ")\n",
    "print(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "outside-limit",
   "metadata": {},
   "source": [
    "The *tilde* operator in this statement is usually read as \"log price is a function of ...\", to account for the fact that many different model specifications can be fit according to that functional relationship between `log_price` and our covariate list. **Critically, note that the trailing `-1` term means that we are fitting this model without an intercept term. This is necessary, since including an intercept term alongside unique means for every neighborhood would make the underlying system of equations underspecified.**\n",
    "\n",
    "Using this expression, we can estimate the unique effects of each neighborhood, fitting the model in `statsmodels` (note how the specification of the model, formula and data is separated from the fitting step): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-bahrain",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m3 = sm.ols(f, data=db).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-rental",
   "metadata": {},
   "source": [
    "We could rely on the `summary2()` method to print a similar summary report from the regression but, given it is a lengthy one in this case, we will illustrate how you can extract the spatial fixed effects into a table for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store variable names for all the spatial fixed effects\n",
    "sfe_names = [i for i in m3.params.index if \"neighborhood[\" in i]\n",
    "# Create table\n",
    "pandas.DataFrame(\n",
    "    {\n",
    "        \"Coef.\": m3.params[sfe_names],\n",
    "        \"Std. Error\": m3.bse[sfe_names],\n",
    "        \"P-Value\": m3.pvalues[sfe_names],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-drove",
   "metadata": {},
   "source": [
    "The approach above shows how spatial FE are a particular case of a linear regression with a categorical  variable. Neighborhood membership is modeled using binary dummy variables. Thanks to the formula grammar used in `statsmodels`, we can express the model abstractly, and Python parses it, appropriately creating binary variables as required.\n",
    "\n",
    "The second approach leverages `spreg` Regimes functionality. We will see regimes below but, for now, think of them as a generalization of spatial fixed effects where not only $\\alpha$ can vary. This framework allows the user to specify which variables are to be estimated separately for each group. In this case, instead of describing the model in a formula, we need to pass each element of the model as separate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spreg spatial fixed effect implementation\n",
    "## Note that we are using the new function OLS_Regimes\n",
    "\n",
    "m4 = spreg.OLS_Regimes(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    db[variable_names].values,\n",
    "    # Variable specifying neighborhood membership\n",
    "    db[\"neighborhood\"].tolist(),\n",
    "    # Allow the constant term to vary by group/regime\n",
    "    constant_regi=\"many\",\n",
    "    # Variables to be allowed to vary (True) or kept\n",
    "    # constant (False). Here we set all to False\n",
    "    cols2regi=[False] * len(variable_names),\n",
    "    # If True, a separate regression is run for each regime.\n",
    "    regime_err_sep=False,\n",
    "    # Dependent variable name\n",
    "    name_y=\"log_price\",\n",
    "    # Independent variables names\n",
    "    name_x=variable_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3512c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m4.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-province",
   "metadata": {},
   "source": [
    "Similarly as above, we could rely on the `summary` attribute to print a report with all the results computed. For simplicity here, we will only confirm that, to the 12th decimal, the parameters estimated are indeed the same as those we get from `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "numpy.round(m4.betas.flatten() - m3.params.values, decimals=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "centered-creation",
   "metadata": {},
   "source": [
    "To make a map of neighborhood fixed effects, we need to process the results from our model slightly.\n",
    "\n",
    "First, we extract only the effects pertaining to the neighborhoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_effects = m3.params.filter(like=\"neighborhood\")\n",
    "neighborhood_effects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-desert",
   "metadata": {},
   "source": [
    "Then, we need to extract just the neighborhood name from the index of this Series. A simple way to do this is to strip all the characters that come before and after our neighborhood names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence with the variable names without\n",
    "# `neighborhood[` and `]`\n",
    "stripped = neighborhood_effects.index.str.strip(\n",
    "    \"neighborhood[\"\n",
    ").str.strip(\"]\")\n",
    "# Reindex the neighborhood_effects Series on clean names\n",
    "neighborhood_effects.index = stripped\n",
    "# Convert Series to DataFrame\n",
    "neighborhood_effects = neighborhood_effects.to_frame(\"fixed_effect\")\n",
    "# Print top of table\n",
    "neighborhood_effects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-sector",
   "metadata": {},
   "source": [
    "Good, we're back to our raw neighborhood names. These allow us to join it to an auxillary file with neighborhood boundaries that is indexed on the same names. Let's read the boundaries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_path = 'http://data.insideairbnb.com/united-states/ca/san-diego/2022-12-24/visualisations/neighbourhoods.geojson'\n",
    "neighborhoods = geopandas.read_file(sd_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-reputation",
   "metadata": {},
   "source": [
    "And we can then merge the spatial FE and plot them on a map (Fig. XXX7XXX):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-lease",
   "metadata": {
    "caption": "Neighborhood effects on Airbnb nightly prices. Neighborhoods shown in grey are 'not statistically significant' in their effect on Airbnb prices.",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot base layer with all neighborhoods in grey\n",
    "ax = neighborhoods.plot(\n",
    "    color=\"k\", linewidth=0, alpha=0.5, figsize=(12, 6)\n",
    ")\n",
    "# Merge SFE estimates (note not every polygon\n",
    "# receives an estimate since not every polygon\n",
    "# contains Airbnb properties)\n",
    "neighborhoods.merge(\n",
    "    neighborhood_effects,\n",
    "    how=\"left\",\n",
    "    left_on=\"neighbourhood\",\n",
    "    right_index=True\n",
    "    # Drop polygons without a SFE estimate\n",
    ").dropna(\n",
    "    subset=[\"fixed_effect\"]\n",
    "    # Plot quantile choropleth\n",
    ").plot(\n",
    "    \"fixed_effect\",  # Variable to display\n",
    "    scheme=\"quantiles\",  # Choropleth scheme\n",
    "    k=7,  # No. of classes in the choropleth\n",
    "    linewidth=0.1,  # Polygon border width\n",
    "    cmap=\"viridis\",  # Color scheme\n",
    "    ax=ax,  # Axis to draw on\n",
    ")\n",
    "# Add basemap\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=neighborhoods.crs,\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels,\n",
    ")\n",
    "# Remove axis\n",
    "ax.set_axis_off()\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-edmonton",
   "metadata": {},
   "source": [
    "We can see a clear spatial structure in the SFE estimates. The most expensive neighborhoods tend to be located near the coast, while the cheapest ones are more inland."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "another-generic",
   "metadata": {},
   "source": [
    "#### Spatial regimes\n",
    "The equation we will be estimating is:\n",
    "\n",
    "$$\n",
    "\\log{P_i} = \\alpha_r + \\sum_k \\mathbf{X}_{ki}\\beta_{k-r} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where we are not only allowing the constant term to vary by region ($\\alpha_r$), but also every other parameter ($\\beta_{k-r}$).\n",
    "\n",
    "To illustrate this approach, we will use the \"spatial differentiator\" of whether a house is in a coastal neighborhood or not (`coastal_neig`) to define the regimes. The rationale behind this choice is that renting a house close to the ocean might be a strong enough pull that people might be willing to pay at different *rates* for each of the house's characteristics.\n",
    "\n",
    "To implement this in Python, we use the `OLS_Regimes` class in `spreg`, which does most of the heavy lifting for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pysal spatial regimes implementation\n",
    "m5 = spreg.OLS_Regimes(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    db[variable_names].values,\n",
    "    # Variable specifying neighborhood membership\n",
    "    db[\"coastal\"].tolist(),\n",
    "    # Allow the constant term to vary by group/regime\n",
    "    constant_regi=\"many\",\n",
    "    # Allow separate sigma coefficients to be estimated\n",
    "    # by regime (False so a single sigma)\n",
    "    regime_err_sep=False,\n",
    "    # Dependent variable name\n",
    "    name_y=\"log_price\",\n",
    "    # Independent variables names\n",
    "    name_x=variable_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-germany",
   "metadata": {},
   "source": [
    "The result can be explored and interpreted similarly to the previous ones. If you inspect the `summary` attribute, you will find the parameters for each variable mostly conform to what you would expect, across both regimes. To compare them, we can plot them side-by-side on a bespoke table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "res = pandas.DataFrame(\n",
    "    {\n",
    "        # Pull out regression coefficients and\n",
    "        # flatten as they are returned as Nx1 array\n",
    "        \"Coeff.\": m5.betas.flatten(),\n",
    "        # Pull out and flatten standard errors\n",
    "        \"Std. Error\": m5.std_err.flatten(),\n",
    "        # Pull out P-values from t-stat object\n",
    "        \"P-Value\": [i[1] for i in m5.t_stat],\n",
    "    },\n",
    "    index=m5.name_x,\n",
    ")\n",
    "# Coastal regime\n",
    "## Extract variables for the coastal regime\n",
    "coastal = [i for i in res.index if \"1_\" in i]\n",
    "## Subset results to coastal and remove the 1_ underscore\n",
    "coastal = res.loc[coastal, :].rename(lambda i: i.replace(\"1_\", \"\"))\n",
    "## Build multi-index column names\n",
    "coastal.columns = pandas.MultiIndex.from_product(\n",
    "    [[\"Coastal\"], coastal.columns]\n",
    ")\n",
    "# Non-coastal model\n",
    "## Extract variables for the non-coastal regime\n",
    "ncoastal = [i for i in res.index if \"0_\" in i]\n",
    "## Subset results to non-coastal and remove the 0_ underscore\n",
    "ncoastal = res.loc[ncoastal, :].rename(lambda i: i.replace(\"0_\", \"\"))\n",
    "## Build multi-index column names\n",
    "ncoastal.columns = pandas.MultiIndex.from_product(\n",
    "    [[\"Non-coastal\"], ncoastal.columns]\n",
    ")\n",
    "# Concat both models\n",
    "pandas.concat([coastal, ncoastal], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "waiting-rubber",
   "metadata": {},
   "source": [
    "An interesting question arises around the relevance of the regimes. *Are estimates for each variable across regimes statistically different?* For this, the model object also calculates for us what is called a **Chow test. This is a statistic that tests the null hypothesis that estimates from different regimes are undistinguishable. If we reject the null, we have evidence suggesting the regimes actually make a difference.**\n",
    "\n",
    "Results from the Chow test are available on the `summary` attribute, or we can extract them directly from the model object, which we will do here. There are two types of Chow test. First is a global one that jointly tests for differences between the two regimes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "m5.chow.joint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-apparatus",
   "metadata": {},
   "source": [
    "The first value represents the statistic, while the second one captures the p-value. In this case, the two regimes are statistically different from each other. The next step then is to check whether each of the coefficients in our model differs across regimes. For this, we can pull them out into a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(\n",
    "    # Chow results by variable\n",
    "    m5.chow.regi,\n",
    "    # Name of variables\n",
    "    index=m5.name_x_r,\n",
    "    # Column names\n",
    "    columns=[\"Statistic\", \"P-value\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-receipt",
   "metadata": {},
   "source": [
    "As we can see in the table, most variables do indeed differ across regimes, statistically speaking. This points to systematic differences in the data generating processes across spatial regimes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "immediate-latter",
   "metadata": {},
   "source": [
    "### Spatial dependence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "indonesian-latino",
   "metadata": {},
   "source": [
    "#### Exogenous effects: The SLX model\n",
    "Now we estimate the following model:\n",
    "\n",
    "$$\n",
    "\\log(P_i) = \\alpha + \\sum^{p}_{k=1}X_{ij}\\beta_j + \\sum^{p}_{k=1}\\left(\\sum^{N}_{j=1}w_{ij}x_{jk}\\right)\\gamma_k + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $\\sum_{j=1}^N w_{ij}x_{jk}$ represents the spatial lag of the $k$ th explanatory variable.\n",
    "This can be stated in *matrix* form using the spatial weights matrix, $\\mathbf{W}$, as:\n",
    "\n",
    "$$\n",
    "\\log(P_i) = \\alpha + \\mathbf{X}\\beta + \\mathbf{WX}\\gamma + \\epsilon\n",
    "$$\n",
    "\n",
    "This splits the model to focus on two main effects: $\\beta$ and $\\gamma$. The\n",
    "$\\beta$ effect describes the change in $y_i$ when $X_{ik}$ changes by one. The subscript for site $i$ is important here: since we're dealing \n",
    "with a $\\mathbf{W}$ matrix, it's useful to be clear about where the change occurs. The $\\gamma$ effect represents the \n",
    "*indirect* association of a change in $X_i$ with the house price. \n",
    "\n",
    "This can be conceptualized in two ways. \n",
    "- First, one could think of $\\gamma$ as simply *the association between the price in a given house and a unit change in its average surroundings.*\n",
    "This is useful and simple. But this interpretation blurs *where* this change\n",
    "might occur. In truth, a change in a variable at site $i$ will result in a *spillover* to its surroundings:\n",
    "when $x_i$ changes, so too does the *spatial lag* of any site near $i$. \n",
    "The precise size of the change in the lag will depend on the structure of $\\mathbf{W}$, and it can be \n",
    "different for every site it is connected with. For example, think of a very highly connected \"focal\" site in a \n",
    "row-standardized weight matrix. This focal site will not be strongly affected \n",
    "if a neighbor changes by a single unit, since each site only contributes a \n",
    "small amount to the lag at the focal site. \n",
    "- Alternatively, consider a site with only \n",
    "one neighbor: its lag will change by *exactly* the amount its sole neighbor changes.\n",
    "Thus, to discover the exact indirect effect of a change $y$ caused by the change\n",
    "at a specific site $x_i$ you would need to compute the *change in the spatial lag*,\n",
    "and then use that as your *change* in $X$. We will discuss this in the following section. \n",
    "\n",
    "In Python, we can calculate the spatial lag of each variable whose name starts by `pg_`\n",
    "by first creating a list of all of those names, and then applying `pysal`'s\n",
    "`lag_spatial` to each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only columns in `db` containing the keyword `pg_`\n",
    "lag_variables_used = db.filter(\n",
    "        like=\"pg_\"\n",
    "        # Compute the spatial lag of each of those variables\n",
    "    )\n",
    "wx = lag_variables_used.apply(lambda y: weights.spatial_lag.lag_spatial(knn, y)\n",
    "        # Rename the spatial lag, adding w_ to the original name\n",
    "    )\n",
    "wx = wx.rename(columns=lambda c: \"w_\"+ c\n",
    "        # Remove the lag of the binary variable for apartments\n",
    "    )\n",
    "wx = wx.drop(\"w_pg_Apartment\", axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-irrigation",
   "metadata": {},
   "source": [
    "Once computed, we can run the model using OLS estimation because, in this\n",
    "context, the spatial  lags included do not violate any of the assumptions OLS\n",
    "relies on (they are essentially additional exogenous variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge original variables with the spatial lags in `wx`\n",
    "slx_exog = db[variable_names].join(wx)\n",
    "# Fit linear model with `spreg`\n",
    "m6 = spreg.OLS(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    slx_exog.values,\n",
    "    # Dependent variable name\n",
    "    name_y=\"l_price\",\n",
    "    # Independent variables names\n",
    "    name_x=slx_exog.columns.tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-analyst",
   "metadata": {},
   "source": [
    "As in the previous cases, printing the `summary` attribute of the model object would show a full report table. The variables we included in the original regression\n",
    "display similar behavior, albeit with small changes in size, and can be\n",
    "interpreted also in a similar way. To focus on the aspects that differ from the previous models here, we will only pull out results for the variables for which we also included their spatial lags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect names of variables of interest\n",
    "vars_of_interest = (\n",
    "    db[variable_names].filter(like=\"pg_\").join(wx).columns\n",
    ")\n",
    "# Build full table of regression coefficients\n",
    "pandas.DataFrame(\n",
    "    {\n",
    "        # Pull out regression coefficients and\n",
    "        # flatten as they are returned as Nx1 array\n",
    "        \"Coeff.\": m6.betas.flatten(),\n",
    "        # Pull out and flatten standard errors\n",
    "        \"Std. Error\": m6.std_err.flatten(),\n",
    "        # Pull out P-values from t-stat object\n",
    "        \"P-Value\": [i[1] for i in m6.t_stat],\n",
    "    },\n",
    "    index=m6.name_x\n",
    "    # Subset for variables of interest only and round to\n",
    "    # four decimals\n",
    ").reindex(vars_of_interest).round(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "excess-benjamin",
   "metadata": {},
   "source": [
    "The spatial lag of each type of property\n",
    "(`w_pg_XXX`) is the new addition. We observe that, except for the case\n",
    "of townhouses (same as with the binary variable, `pg_Townhouse`), they are all\n",
    "significant, suggesting our initial hypothesis on the role of the surrounding\n",
    "houses might indeed be at work here. \n",
    "\n",
    "As an illustration, let's look at some of the direct/indirect effects. \n",
    "The direct effect of the `pg_Condominium` variable means that condominiums are\n",
    "typically 11% more expensive ($\\beta_{pg\\_{Condominium}}=0.1063$) than the benchmark\n",
    "property type, apartments. More relevant to this section, any given house surrounded by \n",
    "condominiums *also* receives a price premium. But, since $pg_{Condominium}$ is a dummy variable,\n",
    "the spatial lag at site $i$ represents the *percentage* of properties near $i$ that are\n",
    "condominiums, which is between $0$ and $1$.\n",
    "So, a *unit* change in this variable means that you would increase the condominium \n",
    "percentage by 100%. Thus, a $1$ increase in `w_pg_Condominium` (a change of 100% percentage points)\n",
    "would result in a 59.2% increase in the property house price ($\\beta_{w_pg\\_Condominium} = 0.5928$). \n",
    "Similar interpretations can be derived for all other spatially lagged variables to derive the\n",
    "*indirect* effect of a change in the spatial lag. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "rolled-voluntary",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Spatial error\n",
    "\n",
    "The spatial error model includes a spatial lag in the *error* term of the equation:\n",
    "\n",
    "$$\n",
    "\\log{P_i} = \\alpha + \\sum_k \\beta_k X_{ki} + u_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_i = \\lambda u_{lag-i} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $u_{lag-i} = \\sum_j w_{i,j} u_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit spatial error model with `spreg`\n",
    "# (GMM estimation allowing for heteroskedasticity)\n",
    "m7 = spreg.GM_Error_Het(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    db[variable_names].values,\n",
    "    # Spatial weights matrix\n",
    "    w=knn,\n",
    "    # Dependent variable name\n",
    "    name_y=\"log_price\",\n",
    "    # Independent variables names\n",
    "    name_x=variable_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-andorra",
   "metadata": {},
   "source": [
    "Similarly as before, the `summary` attribute will return a full-featured table of results. For the most part, it may be interpreted in similar ways to those above. The main difference is that, in this case, we can also recover an estimate and inference for the $\\lambda$ parameter in the error term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full table of regression coefficients\n",
    "pandas.DataFrame(\n",
    "    {\n",
    "        # Pull out regression coefficients and\n",
    "        # flatten as they are returned as Nx1 array\n",
    "        \"Coeff.\": m7.betas.flatten(),\n",
    "        # Pull out and flatten standard errors\n",
    "        \"Std. Error\": m7.std_err.flatten(),\n",
    "        # Pull out P-values from t-stat object\n",
    "        \"P-Value\": [i[1] for i in m7.z_stat],\n",
    "    },\n",
    "    index=m7.name_x\n",
    "    # Subset for lambda parameter and round to\n",
    "    # four decimals\n",
    ").reindex([\"lambda\"]).round(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "other-assist",
   "metadata": {},
   "source": [
    "#### Spatial lag\n",
    "\n",
    "The spatial lag model introduces a spatial lag of the *dependent* variable. In the example we have covered, this would translate into:\n",
    "\n",
    "$$\n",
    "\\log{P_i} = \\alpha + \\rho \\log{P_{lag-i}} + \\sum_k \\beta_k X_{ki} + \\epsilon_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit spatial lag model with `spreg`\n",
    "# (GMM estimation)\n",
    "m8 = spreg.GM_Lag(\n",
    "    # Dependent variable\n",
    "    db[[\"log_price\"]].values,\n",
    "    # Independent variables\n",
    "    db[variable_names].values,\n",
    "    # Spatial weights matrix\n",
    "    w=knn,\n",
    "    # Dependent variable name\n",
    "    name_y=\"log_price\",\n",
    "    # Independent variables names\n",
    "    name_x=variable_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-alpha",
   "metadata": {},
   "source": [
    "And let's summarize the coefficients in a table as before (usual disclaimer about the `summary` object applies):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full table of regression coefficients\n",
    "pandas.DataFrame(\n",
    "    {\n",
    "        # Pull out regression coefficients and\n",
    "        # flatten as they are returned as Nx1 array\n",
    "        \"Coeff.\": m8.betas.flatten(),\n",
    "        # Pull out and flatten standard errors\n",
    "        \"Std. Error\": m8.std_err.flatten(),\n",
    "        # Pull out P-values from t-stat object\n",
    "        \"P-Value\": [i[1] for i in m8.z_stat],\n",
    "    },\n",
    "    index=m8.name_z\n",
    "    # Round to four decimals\n",
    ").round(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "discrete-broadway",
   "metadata": {},
   "source": [
    "## Q.1\n",
    "\n",
    "One common kind of spatial econometric model is the \"Spatial Durbin Model,\" which combines the SLX model with the spatial lag model. Alternatively, the \"Spatial Durbin Error Model\" combines the SLX model with the spatial error model. \n",
    "\n",
    "- Fit a Spatial Durbin variant of the spatial models we have fit in this chapter.  (5 pts)\n",
    "- Do these variants improve the model fit? (2 pts)\n",
    "- What happens to the spatial autocorrelation parameters ($\\rho$, $\\lambda$) when the SLX term is added? Why might this occur? (5 pts)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
