{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "homeless-heart",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Understand the use cases for different type of spatial weights\n",
    "\n",
    "This week's lesson is a simplied version of:  \n",
    "- The [Chapter 4 in Geographic Data Science textbook](https://geographicdata.science/book/notebooks/04_spatial_weights.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(context='paper')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "from pysal.lib import cg as geometry\n",
    "\n",
    "## warnings is a module that allows you to filter warnings\n",
    "import warnings\n",
    "## we are going to ignore all warnings (so they won't print)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936e5c7",
   "metadata": {},
   "source": [
    "# Spatial Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3cfe3",
   "metadata": {},
   "source": [
    "\"Spatial weights\" are one way to represent graphs in geographic data science and spatial statistics. They are widely used constructs that represent geographic relationships between the observational units in a spatially referenced dataset. Implicitly, spatial weights connect objects in a geographic table to one another using the spatial relationships between them. **By expressing the notion of geographical proximity or connectedness, spatial weights are the main mechanism through which the spatial relationships in geographical data is brought to bear in the subsequent analysis.**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Spatial weights often express our knowledge about spatial relationships. \n",
    "For example, proximity and adjacency are common spatial questions: *What neighborhoods are you surrounded by? How many gas stations are within 5 miles of my stalled car?*\n",
    "These are spatial questions that target specific information about the spatial configuration of a specific target (\"a neighborhood,\" \"my stalled car\") and geographically connected relevant sites (\"adjacent neighborhoods\", \"nearby gas stations\"). For us to use this information in statistical analysis, it's often necessary to compute these relationships between all pairs of observations. This means that, for many applications in geographic data science, we are building a *topology*---a mathematical structure that expresses the connectivity between observations---that we can use to examine the data. Spatial weights matrices express this topology, letting us embed all of our observations in space together, rather than asking and answering single questions about features nearby a unit. \n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.researchgate.net/publication/327776659/figure/fig1/AS:672948855791617@1537454918147/A-typical-spatial-weights-matrix-Note-that-A-and-E-do-not-share-a-side-thus-their.png\" alt=\"drawing\" width=\"600\" style=\"display: block; margin: 0 auto\"/>\n",
    "<figcaption>From Bi, Y., Xie, J., Sha, Z., Wang, M., Fu, Y., & Chen, W. (2018, August). Modeling spatiotemporal heterogeneity of customer preferences in engineering design. In International Design Engineering Technical Conferences and Computers and Information in Engineering Conference (Vol. 51753, p. V02AT03A050). American Society of Mechanical Engineers.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14622d8",
   "metadata": {},
   "source": [
    "Since they provide a way to represent these spatial relationships, spatial weights are widely used throughout spatial and geographic data science.\n",
    "In this chapter, we first consider different approaches to construct spatial weights, distinguishing between those based on **contiguity/adjacency relations** from weights obtained from **distance based relationships**. We illustrate all of these concepts through the spatial weights class in `pysal`, which provides a rich set of methods and characteristics for spatial weights and it is stored under the `weights` submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysal.lib import weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c29d73",
   "metadata": {},
   "source": [
    "Everyone check: Which version of `pysal.lib` do you have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysal\n",
    "pysal.lib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094fb011",
   "metadata": {},
   "source": [
    "Throughout the chapter, we discuss common file formats used to store spatial weights of different types, and we include visual discussion of spatial weights, making these sometimes abstract constructs more intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8dbb5",
   "metadata": {},
   "source": [
    "## Contiguity Weights\n",
    "\n",
    "A contiguous pair of spatial objects are those who share a common border. At first\n",
    "glance this seems straightforward. However, in practice this turns out to be\n",
    "more complicated. The first complication is that there are different ways that objects can \"share a common border.\" Let's start with the example of a three-by-three grid. We can create it as a geo-table from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019823b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get points in a grid\n",
    "l = np.arange(3)\n",
    "xs, ys = np.meshgrid(l, l)\n",
    "# Set up store\n",
    "polys = []\n",
    "# Generate polygons\n",
    "for x, y in zip(xs.flatten(), ys.flatten()):\n",
    "    poly = Polygon([(x, y), (x + 1, y), (x + 1, y + 1), (x, y + 1)])\n",
    "    polys.append(poly)\n",
    "# Convert to GeoSeries\n",
    "polys = gpd.GeoSeries(polys)\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"geometry\": polys,\n",
    "        \"id\": [\"P-%s\" % str(i).zfill(2) for i in range(len(polys))],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1eef1e",
   "metadata": {},
   "source": [
    "which results in the grid shown in the following figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1622ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grid geotable\n",
    "ax = gdf.plot(facecolor=\"w\", edgecolor=\"k\")\n",
    "\n",
    "# Loop over each cell and add the text\n",
    "for x, y, t in zip(\n",
    "    [p.centroid.x - 0.25 for p in polys],\n",
    "    [p.centroid.y - 0.25 for p in polys],\n",
    "    [i for i in gdf[\"id\"]],\n",
    "):\n",
    "    plt.text(\n",
    "        x,\n",
    "        y,\n",
    "        t,\n",
    "        verticalalignment=\"center\",\n",
    "        horizontalalignment=\"center\",\n",
    "    )\n",
    "\n",
    "# Remove axes\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99669af",
   "metadata": {},
   "source": [
    "A common way to express contiguity/adjacency relationships arises from an analogy to the legal moves that different chess pieces can make. *Rook* contiguity requires that the pair of polygons in\n",
    "question share an *edge*. According to this definition, polygon $0$ would be a rook neighbor of $1$ and $3$, while $1$ would be a rook neighbor with $0$, $2$, and $4$. Applying this rule to all nine polygons we can model our neighbor relations as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be251b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a rook contiguity matrix from a regular 3x3\n",
    "# lattice stored in a geo-table\n",
    "wr = weights.contiguity.Rook.from_dataframe(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3aaa6",
   "metadata": {},
   "source": [
    "Note the pattern we use to build the `W` object, which is similar across the library: we specify the criterium we want for the weights (`weights.contiguity.Rook`) and then the \"constructor\" we will use (`from_dataframe`). We can visualise the result plotted on top of the same grid of labeled polygons, using red dotted lines to represent the edges between a pair of nodes (polygon centroids in this case). We can see this in the following figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "f, ax = plt.subplots(1, 1, subplot_kw=dict(aspect=\"equal\"))\n",
    "# Plot grid\n",
    "gdf.plot(facecolor=\"w\", edgecolor=\"k\", ax=ax)\n",
    "# Loop over each cell and add the text\n",
    "for x, y, t in zip(\n",
    "    [p.centroid.x - 0.25 for p in polys],\n",
    "    [p.centroid.y - 0.25 for p in polys],\n",
    "    [i for i in gdf[\"id\"]],\n",
    "):\n",
    "    plt.text(\n",
    "        x,\n",
    "        y,\n",
    "        t,\n",
    "        verticalalignment=\"center\",\n",
    "        horizontalalignment=\"center\",\n",
    "    )\n",
    "# Plot weights connectivity\n",
    "wr.plot(gdf, edge_kws=dict(color=\"r\", linestyle=\":\"), ax=ax)\n",
    "# Remove axes\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2912e76",
   "metadata": {},
   "source": [
    "The  `neighbors` attribute of our `pysal` `W` object encodes the neighbor\n",
    "relationships by expressing the *focal* observation on the left (in the `key` of the dictionary), and expressing the *neighbors* to the *focal* in the list on the right (in the `value` of the dictionary). This representation has computational advantages, as it exploits\n",
    "the **sparse** nature of contiguity weights matrices by recording only non-zero weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23917b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0024523",
   "metadata": {},
   "source": [
    "More specifically, knowing\n",
    "that the neighbors of polygon $0$ are $3$ and $1$ implies that polygons $2, 4,\n",
    "5, 6, 7, 8$ are not Rook neighbors of 0. As such, there is no reason to store\n",
    "the \"non-neighbor\" information and this results in significant reductions in\n",
    "memory requirements. However, it is possible to create the fully **dense**, matrix\n",
    "representation if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73fc7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(*wr.full()).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a6ea55",
   "metadata": {},
   "source": [
    "As you can see from the matrix above, most entries are zero. In fact out of all of the possible $9^2=81$ linkages that there could be in this matrix, there are only twenty-four non-zero entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184ee0d",
   "metadata": {},
   "source": [
    "Thus, we can save a significant amount of memory and lose no information using these sparse representations, which only record the non-zero values. \n",
    "\n",
    "A close inspection reveals that this criterion actually places\n",
    "a restriction on the spatial relation. More specifically, polygons $0$ and $4$\n",
    "are not Rook neighbors, but they do in fact share a common border. However, in\n",
    "this instance the sharing is due to a common *vertex* rather than a shared\n",
    "*edge*. If we wanted them to be considered as neighbours, we can switch to the more inclusive notion of *Queen* contiguity, which\n",
    "requires the pair of polygons to only share one or more *vertices*. We can create the\n",
    "neighbor relations for this same configuration as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a queen contiguity matrix from a regular 3x3\n",
    "# lattice stored in a geo-table\n",
    "wq = weights.contiguity.Queen.from_dataframe(gdf)\n",
    "wq.neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7c91f",
   "metadata": {},
   "source": [
    "In addition to this neighbors representation, we can also express the graph visually, as done before. This is shown in the following figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f463a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "f, ax = plt.subplots(1, 1, subplot_kw=dict(aspect=\"equal\"))\n",
    "# Plot grid\n",
    "gdf.plot(facecolor=\"w\", edgecolor=\"k\", ax=ax)\n",
    "# Loop over each cell and add the text\n",
    "for x, y, t in zip(\n",
    "    [p.centroid.x - 0.25 for p in polys],\n",
    "    [p.centroid.y - 0.25 for p in polys],\n",
    "    [i for i in gdf[\"id\"]],\n",
    "):\n",
    "    plt.text(\n",
    "        x,\n",
    "        y,\n",
    "        t,\n",
    "        verticalalignment=\"center\",\n",
    "        horizontalalignment=\"center\",\n",
    "    )\n",
    "# Plot weights connectivity\n",
    "wq.plot(gdf, edge_kws=dict(color=\"r\", linestyle=\":\"), ax=ax)\n",
    "# Remove axes\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c93a1",
   "metadata": {},
   "source": [
    "By using `Contiguity.Queen` rather than `Contiguity.Rook`, we consider observations that share a vertex to be neighbors. The result is that the neighbors of $0$ now include $4$ along with $3$ and $1$.\n",
    "\n",
    "Akin to how the `neighbors` dictionary encodes the contiguity relations, the `weights` dictionary encodes the strength of the link connecting the focal to each neighbor. For contiguity\n",
    "weights, observations are usually either considered \"linked\" or \"not linked,\" so the resulting weights matrix is **binary**. As in any `pysal` `W` object, the actual weight values are contained in the `weights` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f96bb1a",
   "metadata": {},
   "source": [
    "Similar to the `neighbors` attribute, the `weights` object is a Python\n",
    "dictionary that only stores the non-zero weights. Although the weights for a\n",
    "given observations neighbors are all the same value for contiguity weights, it\n",
    "is important to note that the `weights` and `neighbors` **are aligned with one another**; for each observation, its first neighbor in `neighbors` has the first weight in its `weights` entry. This will be important when we examine distance based weights further\n",
    "on, when observations will have different weights. \n",
    "\n",
    "In addition to the `neighbor` and `weights` attributes, the `w` object has a\n",
    "large number of other attributes and methods that can be useful. **The\n",
    "`cardinalities` attribute reports the number of neighbors for each observation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.cardinalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f0532",
   "metadata": {},
   "source": [
    "Any ordered pair of contiguous observations constitutes a *join* represented by a non-zero weight in a $W$. In other words Polygons 0 and 1 are a join, and Polygons 1 and 0 are another join. The attribute `s0` records the number of joins (i.e. non-zero weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac54bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.s0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d4962",
   "metadata": {},
   "source": [
    "Thus, the Queen weights here have just under twice the number of joins in this case.\n",
    "The `pct_nonzero` attribute provides a measure of the density (compliment of\n",
    "sparsity) of the spatial weights matrix (if we had it stored explicitly, which\n",
    "we don't):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.pct_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16643cef",
   "metadata": {},
   "source": [
    "which is equal to $100 \\times (\\texttt{w.s0} / \\texttt{w.n}^2)$.\n",
    "\n",
    "### Spatial Weights from real-world geographic tables\n",
    "\n",
    "The regular lattice map encountered above helps us to understand the logic and\n",
    "properties of `pysal`'s spatial weights class. However, the artificial nature of\n",
    "that geography is of limited relevance to real world research problems.\n",
    "`pysal` supports the construction of spatial weights objects from a\n",
    "number of commonly used spatial data formats. Here we demonstrate this\n",
    "functionality for the case of census tracts in San Diego, California. Most spatial\n",
    "data formats, such as shapefiles, are non-topological in that they encode the\n",
    "polygons as a collection of vertices defining the edges of the geometry's\n",
    "boundary. No information about the neighbor relations is explicitly encoded, so we\n",
    "must construct it ourselves. Under the hood, `pysal` uses efficient spatial indexing\n",
    "structures to extract these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac5ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are going to read this tract data directly from a URL\n",
    "san_diego_tracts = gpd.read_file(\n",
    "    \"https://www.dropbox.com/s/g8ete3zligcozzq/sandiego_tracts.gpkg?dl=1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc485f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "san_diego_tracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cfb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_queen = weights.contiguity.Queen.from_dataframe(san_diego_tracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadeb6f5",
   "metadata": {},
   "source": [
    "Like before, we can visualize the adjacency relationships, but they are much more difficult to see without showing a closer detail. This higher level of detail is shown in the right pane of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aaab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tract geography\n",
    "f, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for i in range(2):\n",
    "    ax = san_diego_tracts.plot(\n",
    "        edgecolor=\"k\", facecolor=\"w\", ax=axs[i]\n",
    "    )\n",
    "    # Plot graph connections\n",
    "    w_queen.plot(\n",
    "        san_diego_tracts,\n",
    "        ax=axs[i],\n",
    "        edge_kws=dict(color=\"r\", linestyle=\":\", linewidth=1),\n",
    "        node_kws=dict(marker=\"\"),\n",
    "    )\n",
    "    # Remove the axis\n",
    "    axs[i].set_axis_off()\n",
    "axs[1].axis([-13040000, -13020000, 3850000, 3860000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cc1a66",
   "metadata": {},
   "source": [
    "The weights object for San Diego tracts have the same attributes and methods as\n",
    "we encountered with our artificial layout above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd84fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_queen.n)\n",
    "print(w_queen.pct_nonzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47af5a",
   "metadata": {},
   "source": [
    "First we have a larger number of spatial units. The spatial weights are\n",
    "also much sparser for the tracts than what we saw for our smaller toy\n",
    "grid. Moreover, the cardinalities have a radically different distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(w_queen.cardinalities)\n",
    "s.plot.hist(bins=s.unique().shape[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686f2a0",
   "metadata": {},
   "source": [
    "As the minimum number of neighbors is 1, while there is one polygon with 29\n",
    "Queen neighbors. The most common number of neighbors is 6. For comparison, we\n",
    "can also plot the equivalent for rook weights of the same dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30afb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rook = weights.contiguity.Rook.from_dataframe(san_diego_tracts)\n",
    "print(w_rook.pct_nonzero)\n",
    "s = pd.Series(w_rook.cardinalities)\n",
    "s.plot.hist(bins=s.unique().shape[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb709ae",
   "metadata": {},
   "source": [
    "The cardinality histogram shifts downward due to the increasing sparsity of the\n",
    "weights for the rook case relative to the Queen criterion. Conceptually, this makes sense: all Rook neighbors are also Queen neighbors, since Queen includes neighbors that share an edge; but, not all Queen neighbors are Rook neighbors, since some Queen neighbors only share a point on their boundaries in common. \n",
    "\n",
    "The example above shows how the notion of contiguity, although more\n",
    "straightforward in the case of a grid, can be naturally extended beyond the\n",
    "particular case of a regular lattice. The principle to keep in mind is that we\n",
    "consider contiguous (and hence call neighbors) observations which share part\n",
    "of their border coordinates. In the Queen case, a single point is enough to make\n",
    "the join. For Rook neighbors, we require a join to consist of one or more\n",
    "shared edges. This distinction is less relevant in the real world than\n",
    "it appears in the grid example above. In any case, there are some cases\n",
    "where this distinction can matter and it is useful to be familiar with the\n",
    "differences between the two approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3b4fa",
   "metadata": {},
   "source": [
    "## Distance Based Weights\n",
    "\n",
    "In addition to contiguity, we can also define neighbor relations as a function of\n",
    "the distance separating spatial observations. Usually, this means that a matrix expressing the distances between all pairs of observations are required. These are then provided to a **kernel** function which uses the proximity information to model proximity as a smooth function of distance. `pysal` implements a family of\n",
    "distance functions. Here we illustrate a selection beginning with the notion\n",
    "of *nearest neighbor* weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ca3da",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor weights\n",
    "\n",
    "The first type of distance based weights defines the neighbor set of a\n",
    "particular observation as containing its nearest $k$ observations, where the\n",
    "user specifies the value of $k$. To illustrate this for the San Diego\n",
    "tracts we take $k=4$. This still leaves the issue of how to measure the distance\n",
    "between these polygon objects, however. To do so we develop a representative\n",
    "point for each of the polygons using the centroid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk4 = weights.distance.KNN.from_dataframe(san_diego_tracts, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad35d6",
   "metadata": {},
   "source": [
    "The centroids are calculated from\n",
    "the spatial information stored in the `GeoDataFrame` as we have seen before. Since we are dealing with\n",
    "polygons in this case, `pysal` uses inter-centroid distances to determine the\n",
    "$k$ nearest observations to each polygon. \n",
    "\n",
    "The k-nearest neighbor weights displays no island problem, that is *everyone* has at least one neighbor, since we have created a set of weights based on everyone having 4 (and only 4) neighbors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f70c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk4.islands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk4.weights[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b275d33",
   "metadata": {},
   "source": [
    "Everyone has the same number of neighbors. In some cases, this is not an issue but a desired feature. In\n",
    "other contexts, however, this characteristic of k-nearest neighbor weights can be undesirable.\n",
    "In such situations, we can turn to other types of distance-based weights.\n",
    "\n",
    "### Kernel weights\n",
    "\n",
    "The k-nearest neighbor rule assigns binary values to the weights for neighboring observations.\n",
    "`pysal` also supports continuously valued weights to reflect Tobler's first law\n",
    "{cite}`Tobler1970computer` in a more direct way: observations that are close to a unit have larger\n",
    "weights than more distant observations.\n",
    "\n",
    "Kernel weights are one of the most commonly-used kinds of distance weights. They\n",
    "reflect the case where similarity/spatial proximity is assumed or expected to\n",
    "decay with distance. The essence of kernel weights is that the weight between\n",
    "observations $i$ and $j$ is based on their distance, but it is further modulated by\n",
    "a kernel function with certain properties. `pysal` implements several kernels.\n",
    "All of them share the properties of distance decay (thus encoding Tobler's First \n",
    "Law), but may decay at different rates with respect to distance.\n",
    "\n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.researchgate.net/publication/323460636/figure/fig4/AS:1086420894654636@1636034334990/Gaussian-kernel-function-used-in-the-GWR-model-Here-oi-is-weight-of-the-ith-observation.jpg\" alt=\"drawing\" width=\"600\" style=\"display: block; margin: 0 auto\"/>\n",
    "<figcaption>From Zhan, C., Han, J., Hu, S., Liu, L., & Dong, Y. (2018). Spatial downscaling of GPM annual and monthly precipitation using regression-based algorithms in a mountainous area. Advances in Meteorology, 2018, 1-13.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Below are some examples of what might be ways to relate observations $i$ and $j$ is based on their distance:\n",
    "</figure>\n",
    "<img src=\"https://www.researchgate.net/publication/234704634/figure/fig1/AS:299724839112705@1448471378437/Conceptualizing-distance-decay-in-patient-physician-interactions-1a-gravity-function.png\" alt=\"drawing\" width=\"600\" style=\"display: block; margin: 0 auto\"/>\n",
    "<figcaption>From Wang, F. (2012). Measurement, optimization, and impact of health care accessibility: a methodological review. Annals of the Association of American Geographers, 102(5), 1104-1112.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "As a computational note, it is worth mentioning that many of these distance-based decay functions require more resources than the contiguity weights or K-nearest neighbor weights discussed above. This is because the contiguity & k-nearest neighbor structures embed simple assumptions about how shapes relate in space, while kernel functions relax several of those assumptions. Thus, they provide more flexibility at the expense of computation.\n",
    "\n",
    "The simplest way to compute Kernel weights in `pysal` involves a single function\n",
    "call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c88cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_kernel = weights.distance.Kernel.from_dataframe(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5ec21",
   "metadata": {},
   "source": [
    "Like k-nearest neighbor weights, the Kernel weights are based on distances between observations. By default, if the input data is an areal unit, we use a central representative point (like the centroid) for that polygon.\n",
    "The value of the weights will be a function of two main options for\n",
    "kernel weights: choice of kernel function; and the bandwidth. The\n",
    "former controls how distance between $i$ and $j$ is \"modulated\" to produce a\n",
    "the weight that goes in $w_{ij}$. In this respect, `pysal` offers [a large number\n",
    "of functions](https://pysal.org/libpysal/generated/libpysal.weights.Kernel.html) that determine the shape of the distance\n",
    "decay function. The bandwidth specifies the distance from each focal unit over which\n",
    "the kernel function is applied. **For observations separated by distances larger\n",
    "than the bandwidth, the weights are set to zero.**\n",
    "\n",
    "The default values for kernels are to use a triangular kernel with a bandwidth distance\n",
    "equal to the maximum knn=2 distance\n",
    "for all observations. The latter implies a so-called fixed bandwidth where all\n",
    "observations use the same distance for the cut-off. We can inspect this from\n",
    "the generated `W` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_kernel.function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6c610",
   "metadata": {},
   "source": [
    "for the kernel function, and:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc462ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first five values of bandwidths\n",
    "# and see they are all the same\n",
    "w_kernel.bandwidth[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7574d",
   "metadata": {},
   "source": [
    "For the bandwidth applied to each observation.\n",
    "\n",
    "Although simple, a fixed bandwidth is not always the best choice. For example,\n",
    "in cases where the density of the observations varies over the study region,\n",
    "using the same threshold anywhere will result in regions with a high density\n",
    "of neighbors while others with observations very sparsely connected. In these\n",
    "situations, an *adaptive* bandwidth -one which varies by observation and its\n",
    "characteristics- can be preferred. \n",
    "\n",
    "Adaptive bandwidths are picked again using a K-nearest neighbor rule. A bandwidth for each observation is chosen such that, once the $k$-nearest observation is considered, all the remaining observations have zero weight. To illustrate it, we will use a subset of tracts in our San Diego dataset. First, visualising the centroids, we can see that they are not exactly regularly-spaced, although others do nearly fall into a regular spacing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e2991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset of tracts\n",
    "sub_30 = san_diego_tracts[san_diego_tracts['sub_30']==True]\n",
    "# Plot polygons\n",
    "ax = sub_30.plot(facecolor=\"w\", edgecolor=\"k\")\n",
    "# Create and plot centroids\n",
    "sub_30.head(30).centroid.plot(color=\"r\", ax=ax)\n",
    "# Remove axis\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc5606",
   "metadata": {},
   "source": [
    "First let's take a look at what fixed bandwith weights look like: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fixed = weights.distance.Kernel.from_dataframe(sub_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fc846",
   "metadata": {},
   "source": [
    "Bandwidths are all the same: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b777a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fixed.bandwidth[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8700086",
   "metadata": {},
   "source": [
    "And, we can visualize what these kernels look like on the map, too, by focusing on an individual unit and showing how the distance decay attenuates the weight by grabbing the corresponding row of the full kernel matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full matrix version of weights\n",
    "full_matrix, ids = w_fixed.full()\n",
    "# Set up figure with two subplots in a row\n",
    "f, ax = plt.subplots(\n",
    "    1, 3, figsize=(12, 6), subplot_kw=dict(aspect=\"equal\")\n",
    ")\n",
    "# Append weights for first polygon and plot on first subplot\n",
    "sub_30.assign(weight_0=full_matrix[0]).plot(\n",
    "    \"weight_0\", cmap=\"plasma\", ax=ax[0]\n",
    ")\n",
    "# Append weights for 18th polygon and plot on first subplot\n",
    "sub_30.assign(weight_18=full_matrix[17]).plot(\n",
    "    \"weight_18\", cmap=\"plasma\", ax=ax[1]\n",
    ")\n",
    "\n",
    "# Append weights for 24th polygon and plot on first subplot\n",
    "sub_30.assign(weight_24=full_matrix[23]).plot(\n",
    "    \"weight_24\", cmap=\"plasma\", ax=ax[2]\n",
    ")\n",
    "\n",
    "# Add centroid of focal tracts\n",
    "sub_30.iloc[[0], :].centroid.plot(\n",
    "    ax=ax[0], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "sub_30.iloc[[17], :].centroid.plot(\n",
    "    ax=ax[1], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "sub_30.iloc[[23], :].centroid.plot(\n",
    "    ax=ax[2], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "# Add titles\n",
    "ax[0].set_title(\"Fixed bandwidth weights for first tract\")\n",
    "ax[1].set_title(\"Fixed bandwidth weights for 18th tract\")\n",
    "ax[2].set_title(\"Fixed bandwidth weights for 24th tract\")\n",
    "\n",
    "# Remove axis\n",
    "[ax_.set_axis_off() for ax_ in ax]\n",
    "# Add legend\n",
    "[ax_.legend(loc=\"upper left\") for ax_ in ax];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a4ee7",
   "metadata": {},
   "source": [
    "If we now build a weights object with adaptive bandwidth (`fixed=False`), the values for bandwith differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d1330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build weights with adaptive bandwidth and using the 15 nearest neighbors \n",
    "# to develop the bandwidth\n",
    "w_adaptive = weights.distance.Kernel.from_dataframe(\n",
    "    sub_30, fixed=False, k=15\n",
    ")\n",
    "# Print first five bandwidth values\n",
    "w_adaptive.bandwidth[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123ff04",
   "metadata": {},
   "source": [
    "And, we can visualize what these kernels look like on the map, too, by focusing on an individual unit and showing how the distance decay attenuates the weight by grabbing the corresponding row of the full kernel matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce397f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full matrix version of weights\n",
    "full_matrix, ids = w_adaptive.full()\n",
    "# Set up figure with two subplots in a row\n",
    "f, ax = plt.subplots(\n",
    "    1, 3, figsize=(12, 6), subplot_kw=dict(aspect=\"equal\")\n",
    ")\n",
    "# Append weights for first polygon and plot on first subplot\n",
    "sub_30.assign(weight_0=full_matrix[0]).plot(\n",
    "    \"weight_0\", cmap=\"plasma\", ax=ax[0]\n",
    ")\n",
    "# Append weights for 18th polygon and plot on first subplot\n",
    "sub_30.assign(weight_18=full_matrix[17]).plot(\n",
    "    \"weight_18\", cmap=\"plasma\", ax=ax[1]\n",
    ")\n",
    "\n",
    "# Append weights for 24th polygon and plot on first subplot\n",
    "sub_30.assign(weight_24=full_matrix[23]).plot(\n",
    "    \"weight_24\", cmap=\"plasma\", ax=ax[2]\n",
    ")\n",
    "\n",
    "# Add centroid of focal tracts\n",
    "sub_30.iloc[[0], :].centroid.plot(\n",
    "    ax=ax[0], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "sub_30.iloc[[17], :].centroid.plot(\n",
    "    ax=ax[1], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "sub_30.iloc[[23], :].centroid.plot(\n",
    "    ax=ax[2], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "# Add titles\n",
    "ax[0].set_title(\"Adaptive bandwidth weights for first tract\")\n",
    "ax[1].set_title(\"Adaptive bandwidth weights for 18th tract\")\n",
    "ax[2].set_title(\"Adaptive bandwidth weights for 24th tract\")\n",
    "\n",
    "# Remove axis\n",
    "[ax_.set_axis_off() for ax_ in ax]\n",
    "# Add legend\n",
    "[ax_.legend(loc=\"upper left\") for ax_ in ax];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c9cc6",
   "metadata": {},
   "source": [
    "What the kernel looks like can be strongly affected by the structure of spatial proximity, so any part of the map can look quite different from any other part of the map. By imposing a clear distance decay over several of the neighbors of each observation,\n",
    "kernel weights incorporate Tobler's law explicitly. Often, this comes at the cost of\n",
    "increased memory requirements, as every single pair of observations within the\n",
    "bandwidth distance is considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f570b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_kernel.pct_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98303145",
   "metadata": {},
   "source": [
    "In many instances, this may be at odds with the nature of the spatial\n",
    "interactions at hand, which operate over a more limited range of distance. In\n",
    "these cases, expanding the neighborhood set beyond might lead us to consider\n",
    "interactions which either do not take place, or are inconsequential. **Thus, for\n",
    "both substantive and computational reasons, it might make sense to further\n",
    "limit the range, keeping impacts to be hyper-local.**\n",
    "\n",
    "### Distance bands and hybrid Weights\n",
    "\n",
    "In some contexts, it makes sense to draw a circle around each observation and\n",
    "consider as neighbors every other observation that falls within the circle.\n",
    "In the GIS terminology, this is akin to drawing a buffer around each point and\n",
    "performing a point-in-polygon operation that determines whether each of the\n",
    "other observations are within the buffer. If they are, they are assigned a\n",
    "weight of one in the spatial weights matrix, if not they receive a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6355d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bdb = weights.distance.DistanceBand.from_dataframe(\n",
    "    sub_30, 3000, binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d62967",
   "metadata": {},
   "source": [
    "This creates a binary distance weights where every other observation within\n",
    "a distance of 3000 meters is considered neighbor.\n",
    "\n",
    "Distance band weights can also be continuously weighted. These could be seen as a kind of \"censored\" kernel, where the kernel function is applied only within a pre-specified distance. For example, let us calculate the DistanceBand weights that use inverse distance\n",
    "weights up to a certain threshold and then truncate the weights to zero for\n",
    "everyone else. For this example we will return to the small lattice example\n",
    "covered in the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce08b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hy = weights.distance.DistanceBand.from_dataframe(\n",
    "    sub_30, 3000, binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35e580",
   "metadata": {},
   "source": [
    "We apply a threshold of 1.5 for this illustration. `pysal` truncates continuous\n",
    "weights at this distance. It is important to keep in mind that the threshold\n",
    "distance must use the same units of distance as the units used to define the\n",
    "matrix.\n",
    "\n",
    "To see the difference, consider a tract in our San Diego dataset.\n",
    "\n",
    "The queen set of weights includes eight neighbors with a uniform weight of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq_sd = weights.Queen.from_dataframe(sub_30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fcbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full matrix version of weights\n",
    "full_matrix1, ids1 = w_bdb.full()\n",
    "full_matrix2, ids2 = w_hy.full()\n",
    "full_matrix3, ids3 = wq_sd.full()\n",
    "\n",
    "# Set up figure with two subplots in a row\n",
    "f, ax = plt.subplots(\n",
    "    1, 3, figsize=(12, 6), subplot_kw=dict(aspect=\"equal\")\n",
    ")\n",
    "# Append weights for 24th polygon and plot on first subplot\n",
    "sub_30.assign(weight_24=full_matrix1[23]).plot(\n",
    "    \"weight_24\", cmap=\"plasma\", ax=ax[0]\n",
    ")\n",
    "\n",
    "# Append weights for 24th polygon and plot on second subplot\n",
    "sub_30.assign(weight_24=full_matrix2[23]).plot(\n",
    "    \"weight_24\", cmap=\"plasma\", ax=ax[1]\n",
    ")\n",
    "\n",
    "# Append weights for 24th polygon and plot on third subplot\n",
    "sub_30.assign(weight_24=full_matrix3[23]).plot(\n",
    "    \"weight_24\", cmap=\"plasma\", ax=ax[2]\n",
    ")\n",
    "\n",
    "# Add centroid of focal tracts\n",
    "sub_30.iloc[[23], :].centroid.plot(\n",
    "    ax=ax[0], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "sub_30.iloc[[23], :].centroid.plot(\n",
    "    ax=ax[1], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "sub_30.iloc[[23], :].centroid.plot(\n",
    "    ax=ax[2], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "# Add titles\n",
    "\n",
    "ax[0].set_title(\"Binary dist bw weights for 24th tract\")\n",
    "ax[1].set_title(\"Continuously weighted dist bw weights for 24th tract\")\n",
    "ax[2].set_title(\"Queen contiguity weights for 24th tract\")\n",
    "\n",
    "# Remove axis\n",
    "[ax_.set_axis_off() for ax_ in ax]\n",
    "# Add legend\n",
    "[ax_.legend(loc=\"upper left\") for ax_ in ax];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d93a53",
   "metadata": {},
   "source": [
    "## Block Weights \n",
    "\n",
    "A final type of spatial weight we examine here are block weights. In this case, \n",
    "it is membership in geographic\n",
    "a group that defines the neighbor relationships. Block weights connect every\n",
    "observation in a data set that belong to the same category in a provided list.\n",
    "In essence, a block weight structure groups\n",
    "individual observations and considers all members of the group as \"near\" one another. This means that they then have a value of one for every pair of observations in the same group. Contrariwise, all members *not* in that group are considered disconnected from any observation within the group, and given a value of zero. \n",
    "This is done for every group, so the resulting matrix looks like \"blocks\" of 1s stacked on the diagonal (assuming that observations in the same group are near one another in the input data table), hence the \"block\" weights. \n",
    "\n",
    "To demonstrate this class of spatial weights, we will use zones from the Mandatory Inclusionary Housing program in NYC,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2bb4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih = pd.read_json('https://data.cityofnewyork.us/resource/m79g-k9r4.json')\n",
    "from shapely.geometry import shape\n",
    "\n",
    "## the apply method applies the function to each row of the dataframe\n",
    "mih['the_geom'] = mih['the_geom'].apply(shape)\n",
    "\n",
    "## I'm going to use the GeoDataFrame method to create a GeoDataFrame\n",
    "mih_geo = gpd.GeoDataFrame(mih,geometry='the_geom')\n",
    "\n",
    "## Artificially create a unique ID for each row\n",
    "mih_geo['id'] = np.arange(len(mih_geo)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ebd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih_geo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dafc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec743e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "boros = pd.read_json('https://data.cityofnewyork.us/resource/7t3b-ywvw.json')\n",
    "\n",
    "## the apply method applies the function to each row of the dataframe\n",
    "boros['the_geom'] = boros['the_geom'].apply(shape)\n",
    "\n",
    "## I'm going to use the GeoDataFrame method to create a GeoDataFrame\n",
    "boros_geo = gpd.GeoDataFrame(boros,geometry='the_geom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mih_geo[[\"id\", \"boro\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d228687",
   "metadata": {},
   "source": [
    "To build a block weights object, we do not even need spatial data beyond the\n",
    "list of memberships. In this case, we will use the county membership:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c50a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_bl = weights.util.block_weights(\n",
    "    mih_geo[\"boro\"].values,\n",
    "    ids=mih_geo[\"id\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full matrix version of weights\n",
    "full_matrix, ids = w_bl.full()\n",
    "\n",
    "# Set up figure with two subplots in a row\n",
    "f, ax = plt.subplots(\n",
    "    1, 2, figsize=(15, 6),\n",
    ")\n",
    "\n",
    "boros_geo.plot( ax=ax[0], color=\"white\", edgecolor=\"black\", linewidth=0.5)\n",
    "boros_geo.plot( ax=ax[1], color=\"white\", edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "# Append weights for 24th polygon and plot on first subplot\n",
    "mih_geo.assign(weight_28=full_matrix[28]).plot(\n",
    "    \"weight_28\", cmap=\"plasma\", ax=ax[0]\n",
    ")\n",
    "\n",
    "# Append weights for 24th polygon and plot on second subplot\n",
    "mih_geo.assign(weight_100=full_matrix[100]).plot(\n",
    "    \"weight_100\", cmap=\"plasma\", ax=ax[1]\n",
    ")\n",
    "\n",
    "# Add centroid of focal tracts\n",
    "mih_geo.iloc[[28], :].centroid.plot(\n",
    "    ax=ax[0], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "mih_geo.iloc[[100], :].centroid.plot(\n",
    "    ax=ax[1], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    ")\n",
    "# sub_30.iloc[[23], :].centroid.plot(\n",
    "#     ax=ax[2], marker=\"*\", color=\"k\", label=\"Focal Tract\"\n",
    "# )\n",
    "# Add titles\n",
    "\n",
    "ax[0].set_title(\"Block weights for 27th zone\")\n",
    "ax[1].set_title(\"Block weights for 99th zone\")\n",
    "# ax[2].set_title(\"Queen contiguity weights for 24th tract\")\n",
    "\n",
    "# Remove axis\n",
    "[ax_.set_axis_off() for ax_ in ax]\n",
    "# Add legend\n",
    "[ax_.legend(loc=\"upper left\") for ax_ in ax];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e4af8",
   "metadata": {},
   "source": [
    "## Use case: Boundary detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf9a89a",
   "metadata": {},
   "source": [
    "We close the chapter with an illustration of how weights can be useful by themselves in geographic data science. Note that the application displayed below involves some concepts and code that are a bit more advanced than in the rest of the chapter. If you are up for the challenge, we think the insights it enables are worth the effort!\n",
    "\n",
    "Spatial weights are ubiquitous in the analysis of spatial patterns in data, since they provide a direct method to represent spatial structure. \n",
    "However, spatial weights are also useful in their own right, such as when examining latent structures directly in the graphs themselves or when using them to conduct descriptive analysis. \n",
    "One clear use case that arises in the analysis of social data is to characterize latent *data discontinuities*. By *data discontinuity*, we mean a single border (or collection of borders) where data for a variable (or many variables) of interest change abruptly. \n",
    "These can be used in models of inequality {cite}`Lu2005bayesian,Fitzpatrick2010ecological,Dean2019frontiers` or used to adapt classic empirical outlier detection methods. \n",
    "Below, we'll show one model-free way to identify empirical boundaries in your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddcad42",
   "metadata": {},
   "source": [
    "First, let's consider the median household income for our census tracts in San Diego, shown in the following figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f35edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "san_diego_tracts.plot(\"median_hh_income\", ax=ax[0])\n",
    "ax[0].set_axis_off()\n",
    "san_diego_tracts[\"median_hh_income\"].plot.hist(ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8068ffa",
   "metadata": {},
   "source": [
    "Now, we see some cases where there are very stark differences between neighboring areas, and some cases where there appear to be no difference between adjacent areas. Digging into this, we can examine the *distribution of differences* in neighboring areas using the adjacency list, a different representation of a spatial graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e900d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist = w_rook.to_adjlist()\n",
    "adjlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf3673",
   "metadata": {},
   "source": [
    "This provides us with a table featuring three columns. `Focal` is the column containing the \"origin\" of the link; `neighbor` is the column containing the \"destination\" of the link, or neighbor of the focal polygon; and `weight` contains how strong the link from `focal` to `neighbor` is. Since our weights are *symmetrical*, this table contains two entries per pair of neighbors, one for `(focal,neighbor)` and the other for `(neighbor,focal)`. \n",
    "\n",
    "Now we want to connect this table representing spatial structure with information on median household income. Using `pandas`, we can merge up the focal units' and neighboring units' median household incomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e46d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist_income = adjlist.merge(\n",
    "    san_diego_tracts[[\"median_hh_income\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"focal\",\n",
    "    right_index=True,\n",
    ").merge(\n",
    "    san_diego_tracts[[\"median_hh_income\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"neighbor\",\n",
    "    right_index=True,\n",
    "    suffixes=(\"_focal\", \"_neighbor\"),\n",
    ")\n",
    "adjlist_income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90be65",
   "metadata": {},
   "source": [
    "This operation brings together the income at both the focal observation and the neighbor observation. The difference between these two yields income differences between *adjacent* tracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39007bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist_income[\"diff\"] = (\n",
    "    adjlist_income[\"median_hh_income_focal\"]\n",
    "    - adjlist_income[\"median_hh_income_neighbor\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d20e0",
   "metadata": {},
   "source": [
    "With this information on difference we can now do a few things. First, we can compare whether or not this *distribution* is distinct from the distribution of non-neighboring tracts' differences in wealth. This will give us a hint at the extent to which income follows a spatial pattern. This is also discussed more in depth in the spatial inequality chapter, specifically in reference to the Spatial Gini. \n",
    "\n",
    "To do this, we can first compute the all-pairs differences in income using the `numpy.subtract` function. Some functions in `numpy` have special functionality; these `ufuncs` (short for \"universal functions\") often support special applications to your data. Here, we will use `numpy.subtract.outer` to take the difference over the \"outer cartesian product\" of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = np.subtract.outer(\n",
    "    san_diego_tracts[\"median_hh_income\"].values,\n",
    "    san_diego_tracts[\"median_hh_income\"].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc6b7e",
   "metadata": {},
   "source": [
    "In practice, this results in an $N\\times N$ array that stores the subtraction of all of the combinations of the input vectors.\n",
    "\n",
    "Then, we need to filter out those cells of `all_pairs` that are neighbors. Fortunately, our weights matrix is *binary*. So, subtracting it from an $N \\times N$ matrix of $1$'s will result in the *complement* of our original weights matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27814c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rook.sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110bff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complement_wr = 1 - w_rook.sparse.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee12e59",
   "metadata": {},
   "source": [
    "Note `complement_wr` inserts a 0 where `w_rook` includes a 1, and vice versa. Using this complement, we can filter the `all_pairs` matrix to only consider the differences in median household income for tracts that are not neighboring: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0745c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_neighboring_diffs = (complement_wr * all_pairs).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df3a98",
   "metadata": {},
   "source": [
    "Now, we can compare the two distributions of the difference in wealth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 3))\n",
    "plt.hist(\n",
    "    non_neighboring_diffs,\n",
    "    color=\"lightgrey\",\n",
    "    edgecolor=\"k\",\n",
    "    density=True,\n",
    "    bins=50,\n",
    "    label=\"Nonneighbors\",\n",
    ")\n",
    "plt.hist(\n",
    "    adjlist_income[\"diff\"],\n",
    "    color=\"salmon\",\n",
    "    edgecolor=\"orangered\",\n",
    "    linewidth=3,\n",
    "    density=True,\n",
    "    histtype=\"step\",\n",
    "    bins=50,\n",
    "    label=\"Neighbors\",\n",
    ")\n",
    "sns.despine()\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Dollar Differences ($)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cc49c",
   "metadata": {},
   "source": [
    "From this, we can see that the two distributions are distinct, with the distribution of difference in *non-neighboring* tracts being slightly more dispersed than that for *neighboring* tracts. Thus, on the whole, this means that neighboring tracts have more *smaller differences in wealth* than non-neighboring tracts. This is consistent with the behavior we will talk about in later chapters concerning *spatial autocorrelation*, the tendency for observations to be statistically more similar to nearby observations than they are to distant observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef440ac7",
   "metadata": {},
   "source": [
    "The adjacency table we have build can also help us find our *most extreme* observed differences in income, hinting at possible hard boundaries between the areas. Since our links are symmetric, we can then focus only on focal observations with *the most extreme* difference in wealth from their immediate neighbors, considering only on those where the *focal* is higher, since they each have an equivalent *negative* back-link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d8ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extremes = adjlist_income.sort_values(\"diff\", ascending=False).head()\n",
    "extremes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bde9cd",
   "metadata": {},
   "source": [
    "Thus, we see that observation $473$ appears often on the the `focal` side, suggesting it's quite distinct from its nearby polygons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4c925",
   "metadata": {},
   "source": [
    "To verify whether these differences are truly significant, we can use a map randomization strategy. In this case, we shuffle values across the map and compute *new* `diff` columns. This time, `diff` represents the difference between random incomes, rather than the neighboring incomes we actually observed using our Rook contiguity matrix. Using many `diff` vectors, we can find the observed differences which tend to be much smaller than those encountered in randomly-drawn maps of household income.\n",
    "\n",
    "To start, we can construct many random `diff` vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60dd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: this cell runs a simulation and may take a bit longer\n",
    "## If you want it to run faster, decrease the number of shuffles\n",
    "## by setting a lower value in `n_simulations`\n",
    "\n",
    "# Set number or random shuffles\n",
    "n_simulations = 1000\n",
    "# Create an empty array to store results\n",
    "simulated_diffs = np.empty((len(adjlist), n_simulations))\n",
    "# Loop over each random draw\n",
    "for i in range(n_simulations):\n",
    "    # Extract income values\n",
    "    median_hh_focal = adjlist_income[\"median_hh_income_focal\"].values\n",
    "    # Shuffle income values across locations\n",
    "    random_income = (\n",
    "        san_diego_tracts[[\"median_hh_income\"]]\n",
    "        .sample(frac=1, replace=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    # Join income to adjacency\n",
    "    adjlist_random_income = adjlist.merge(\n",
    "        random_income, left_on=\"focal\", right_index=True\n",
    "    ).merge(\n",
    "        random_income,\n",
    "        left_on=\"neighbor\",\n",
    "        right_index=True,\n",
    "        suffixes=(\"_focal\", \"_neighbor\"),\n",
    "    )\n",
    "    # Store reslults from random draw\n",
    "    simulated_diffs[:, i] = (\n",
    "        adjlist_random_income[\"median_hh_income_focal\"]\n",
    "        - adjlist_random_income[\"median_hh_income_neighbor\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe745ff",
   "metadata": {},
   "source": [
    "After running our simulation, we get many distributions of pairwise differences in household income. Below, we plot the shroud of all of the simulated differences, shown in black, and our observed differences, shown in red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ad60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "f = plt.figure(figsize=(12, 3))\n",
    "# Build background histogram for observed differences\n",
    "plt.hist(\n",
    "    adjlist_income[\"diff\"],\n",
    "    color=\"salmon\",\n",
    "    bins=50,\n",
    "    density=True,\n",
    "    alpha=1,\n",
    "    linewidth=4,\n",
    ")\n",
    "# Plot simulated, random differences\n",
    "[\n",
    "    plt.hist(\n",
    "        simulation,\n",
    "        histtype=\"step\",\n",
    "        color=\"k\",\n",
    "        alpha=0.01,\n",
    "        linewidth=1,\n",
    "        bins=50,\n",
    "        density=True,\n",
    "    )\n",
    "    for simulation in simulated_diffs.T\n",
    "]\n",
    "# Build histogram borderline for observed differences\n",
    "plt.hist(\n",
    "    adjlist_income[\"diff\"],\n",
    "    histtype=\"step\",\n",
    "    edgecolor=\"orangered\",\n",
    "    bins=50,\n",
    "    density=True,\n",
    "    linewidth=2,\n",
    ")\n",
    "# Style figure\n",
    "sns.despine()\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Dollar Differences ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2edbc2-566b-431a-8626-53fc3a24f806",
   "metadata": {},
   "source": [
    "Again, our random distribution is much more dispersed than our observed distribution of the differences between nearby tracts. Empirically, we can pool our simulations and construct and use their quantiles to summarize how unlikely any of our *observed* differences are if neighbors' household incomes were randomly assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6cac6-3ea1-423f-9824-d224d438ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_diffs.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49c95f-814f-4985-a871-85e9abbfe37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all simulated differences into a single vector\n",
    "pooled_diffs = simulated_diffs.flatten()\n",
    "# Calculate the 0.5th, 50th and 99.5th percentiles\n",
    "lower, median, upper = np.percentile(\n",
    "    pooled_diffs, q=(0.5, 50, 99.5)\n",
    ")\n",
    "# Create a filter that is True if the value is \"extreme\"\n",
    "# (in the 0.5th percentile or/`|` in the 00.5th), False otherwise\n",
    "outside = (adjlist_income[\"diff\"] < lower) | (\n",
    "    adjlist_income[\"diff\"] > upper\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec5286-4fc8-4476-b4f6-7133aed060d6",
   "metadata": {},
   "source": [
    "Despite the fact that that our observed differences are less dispersed on average, we can identify two boundaries in the data that are in the top 1% most extreme differences in neighboring household incomes across the map. These boundaries are shown in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc8d48-ee37-4658-a958-37fb318be8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist_income[outside]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61992d5c-2e94-422e-ba13-665ad340a0c0",
   "metadata": {},
   "source": [
    "Note that one of these, observation $473$, appears in both boundaries. This means that the observation is likely to be *outlying*, extremely unlike *all* of its neighbors. These kinds of generalized neighborhood comparisons are discussed in the subsequent chapter on local spatial autocorrelation. For now we can visualize this on a map, focusing on the two boundaries around observation $473$, shown also in the larger context of San Diego incomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc664b1-cd2a-420a-ba14-c05e23465a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot tracts\n",
    "for i in range(2):\n",
    "    san_diego_tracts.plot(\"median_hh_income\", ax=ax[i])\n",
    "\n",
    "# Zoom 1\n",
    "first_focus = san_diego_tracts.iloc[[473, 157]]\n",
    "ax[0].plot(\n",
    "    first_focus.centroid.x, first_focus.centroid.y, color=\"red\"\n",
    ")\n",
    "west, east, south, north = first_focus.buffer(1000).total_bounds\n",
    "ax[0].axis([west, south, east, north])\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "# Zoom 2\n",
    "second_focus = san_diego_tracts.iloc[[473, 163]]\n",
    "ax[1].plot(\n",
    "    second_focus.centroid.x, second_focus.centroid.y, color=\"red\"\n",
    ")\n",
    "ax[1].axis([west, south, east, north])\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "# Context\n",
    "san_diego_tracts.plot(\n",
    "    facecolor=\"k\", edgecolor=\"w\", linewidth=0.5, alpha=0.5, ax=ax[2]\n",
    ")\n",
    "cx.add_basemap(ax[2], crs=san_diego_tracts.crs)\n",
    "area_of_focus = (\n",
    "    pd.concat((first_focus, second_focus))\n",
    "    .buffer(12000)\n",
    "    .total_bounds\n",
    ")\n",
    "ax[2].plot(\n",
    "    first_focus.centroid.x, first_focus.centroid.y, color=\"red\"\n",
    ")\n",
    "ax[2].plot(\n",
    "    second_focus.centroid.x, second_focus.centroid.y, color=\"red\"\n",
    ")\n",
    "west, east, south, north = area_of_focus\n",
    "ax[2].axis([west, south, east, north])\n",
    "ax[2].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a22a9a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Spatial weights are central to how we *represent* spatial relationships in mathematical and computational environments. At their core, they are a \"geo-graph,\" or a network defined by the geographical relationships between observations. They form kind of a \"spatial index,\" in that they record which observations have a specific geographical relationship. Since spatial weights are fundamental to how spatial relationships are represented in geographic data science, we will use them again and again throughout the book. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c2a0eb6-40ea-484f-b964-333ad5e46bf8",
   "metadata": {},
   "source": [
    "## Q.1 \n",
    "While every node in a $k$-nearest neighbor graph has 5 neighbors, there is a conceptual difference between *in-degree* and *out-degree* of nodes in a graph. \n",
    "\n",
    "The *out-degree* of a node is the number of outgoing links from a node; for a K-Nearest Neighbor graph, this is $k$ for every variable. The *in-degree* of a node in a graph is the number of *incoming* links to that node; for a K-Nearest Neighbor graph, this is the number of other observations that pick the target as their nearest neighbor. The *in-degree* of a node in the K-Nearest Neighbor graph can provide a measure of *hubbiness*, or how central a node is to other nodes. \n",
    "\n",
    "1. Using the San Diego Tracts data, build a $k=6$ nearest neighbor weight and call it `knn_6`. \n",
    "2. Verify that the $k=6$ by taking the row sum over the weights matrix in `knn_6.sparse`.\n",
    "3. Compute the in-degree of each observation by taking the *column sum* over the weights matrix in `knn_6.sparse`, and divide by 6, the out-degree for all observations. \n",
    "4. Make a histogram of the in-degrees for the $k=6$ weights. How evenly-distributed is the distribution of in-degrees? (You can just eyeball this.)\n",
    "5. Make a new histogram of the in-degree standardized by the out-degree when $k=26$. Does hubbiness reduce when increasing the number of $k$-nearest neighbors?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d2a60-276c-4b67-a39c-bee8e9632c66",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949eded2-b1fd-4525-9361-74e3af1e38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab67a3-b144-43a4-a64a-30cead7afd27",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5232ea36-b6c3-4507-a655-76a8d605b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will not show the matrix itself, but know that the output is \n",
    "## a numpy matrix.\n",
    "knn_6.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e580c-4b66-4727-b5be-f056e2e271ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can sum along different dimensions of our matrix. \n",
    "## Axis = 1 allows us to sum each row\n",
    "## Axis = 0 allos us to sum each column\n",
    "knn_6.sparse.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e1dab",
   "metadata": {},
   "source": [
    "#### 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90aeeac-0871-40c2-824b-27456503e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a690ac-727a-4f80-bd22-083f68532e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_6_indeg_flat = (knn_6_indeg).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3d86c-8c3a-44fc-b6e8-d47fab244c22",
   "metadata": {},
   "source": [
    "#### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34f74b-1ad8-4c62-924b-4a2307d50ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd06fd1-db96-48b2-8009-213ca8a20d15",
   "metadata": {},
   "source": [
    "#### 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93c642-a02a-4e9a-8368-2d3f4da2372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3780a-c7f7-4ba4-94c7-591e030a9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290c66d-bb86-4c32-9771-cf4cdb4644ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2d8ca-6b87-4c7b-90a1-b4f6f8b7a2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5b5e944a9b71ce46e873efe99abd044b4b2cd2a5153fea1f2fc73581012ba31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
