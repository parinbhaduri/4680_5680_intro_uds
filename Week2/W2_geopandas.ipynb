{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "clean-sessions",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After this week's lesson you should be able to:\n",
    "- Read and write spatial data formats using GeoPandas\n",
    "- Explore spatial data in a map\n",
    "- Set and change map projections for GeoDataframes. \n",
    "- Create GeoDataFrames with CSVs.\n",
    "- Perform a spatial and attribute join.\n",
    "\n",
    "This week's lessons are adapted from:\n",
    "- [Automating GIS Processes Lesson 2](https://autogis-site.readthedocs.io/en/latest/lessons/lesson-2/geopandas-an-introduction.html)\n",
    "- Wenzheng Li's materials from CRP 5680 Spring 2022. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "iraqi-funds",
   "metadata": {},
   "source": [
    "# 0. What is geopandas? \n",
    "\n",
    "**Geopandas** is a python library that allows us to ingest, analyze, and map geospatial vector data. It combines what we have learned in the previous two classes: The tabular data analysis tools in **Pandas** with the geometry handling of shapely. Under the hood, it is using a python library called **fiona**, which handles all different kinds of spatial file formats, and **pyproj**, which manages our coordinate reference systems.\n",
    "\n",
    "The main data structures in Geopandas are GeoDataFrames and GeoSeries, which are intended to mirror the Pandas DataFrame and Series structures. \n",
    "\n",
    "The key distinction in Geopandas is that we will always a column called `geometry` like so that contains the geometries related to each row: \n",
    "<figure class=\"image\">\n",
    "<img src=\"https://autogis-site.readthedocs.io/en/2019/_images/geodataframe.png\" alt=\"drawing\" width=\"500\" style=\"display: block; margin: 0 auto\"/>\n",
    " <figcaption><center>(From Automating GIS Processes)</figcaption>\n",
    "</figure>\n",
    "\n",
    "## 0.1 The three components of a GeoPandas GeoDataFrame\n",
    "To create a GeoDataFrame, we need three things:\n",
    "\n",
    "1. a pandas *DataFrame (df)*\n",
    "2. a *CRS* (coordinate reference system presented by EPSG code, e.g., \"epsg: 4326\");\n",
    "3. a shapely *geometry list* which defines the geometric object types of each observation, e.g., points, lines, or polygons.\n",
    "\n",
    "We are familiar with 1. and 3. from the previous two classes and the Coordinate Reference System from your GIS knowledge. There is a different EPSG for each CRS. THe two most common that we will use are `EPSG:4326` (WGS84) and `EPSG:3857` (WGS84 / pseudo-Mercator, which is common projection used by Google and OSM). \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f529455",
   "metadata": {},
   "source": [
    "# 1. Reading to different spatial data file formats\n",
    "Because of fiona's great under the hood functionality, Geopandas supports almost every vector spatial data format. For us, the to most common formats will be ESRI shapefiles and GeoJSONs.\n",
    "\n",
    "Let's take a look at NYC subyway data. \n",
    "\n",
    "Before we get started, let's orient where we are in our directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a80afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your jupyter notebook path\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dd5412b",
   "metadata": {},
   "source": [
    "Now let's read in the NYC subway stations dataset by doing the following: \n",
    "- Go to the NYC OpenData portal's page on the data for [Subway Stations](https://data.cityofnewyork.us/Transportation/Subway-Stations/arq3-7z49)\n",
    "- Select **Export** in the upper-right hand corner of the page and and select **Shapefile**. \n",
    "- Save this file in the same folder as this notebook. \n",
    "- Keep it as a zipped file! (Geopandas can read both the zipped and unzipped version of the shapefile, but I like to keep it in zipped format because it's cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need this to use geopandas\n",
    "import geopandas as gpd\n",
    "# and remember that we're assigning a nickname to the package\n",
    "\n",
    "# Now let's assign the variable name `stations` to the geodataframe\n",
    "# These file names have spaces in them, which, as a reminder, \n",
    "# you should not do in your own work!\n",
    "stations = gpd.read_file('Subway Stations.zip')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1110850e",
   "metadata": {},
   "source": [
    "# 2. Initial data exploration with Geopandads\n",
    "Once we've read in the file, let's take a look at the data. I'm sure somewhere online is the data dictionary (which I couldn't find :/) but this table contains what we'd expect: \n",
    "- `objectid` which is the ID number for each geometry\n",
    "- `name` the name of the station \n",
    "- `line` which are all the lines that stop at that station\n",
    "- `geometry` which is the shapely geometry\n",
    "- and other columns that we'll probably not need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as you would do in Pandas\n",
    "stations.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "512e372b",
   "metadata": {},
   "source": [
    "But differently than a pandas dataframe, we can also do this with our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9768557",
   "metadata": {},
   "source": [
    "We can also find the CRS for this dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15347071",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.crs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1001a088",
   "metadata": {},
   "source": [
    "# 3. Analyzing and manipulating data in geopandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b69ed941",
   "metadata": {},
   "source": [
    "## 3.1 Changing projections \n",
    "\n",
    "Currently my data is in `EPSG:4326` but say I wanted to change my CRS to `EPSG:3857` as I know other datasets I'm working in are in 3857. \n",
    "\n",
    "\n",
    "I can use the `.to_crs()` function to do this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61367f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note here that this function requires as an input\n",
    "# the name of the new coordinate system\n",
    "stations.to_crs(epsg=3857)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01af1a75",
   "metadata": {},
   "source": [
    "Now let's check our dataset to make sure the CRS was changed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.crs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecee620e",
   "metadata": {},
   "source": [
    "Hm, it wasn't changed! Why not? \n",
    "\n",
    "`stations.to_crs(epsg=3857)` only returns re-projected geometry, but if we don't re-assign our variable name `stations` to this reprojected version, the old version will not be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96364a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = stations.to_crs(epsg=3857)\n",
    "stations.crs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54ce0523",
   "metadata": {},
   "source": [
    "That worked! A good thing about using 3857 is that our units are in meters. This means that when we perform calculations on these geometries, the values we get are also going be in meters. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbc6e15f",
   "metadata": {},
   "source": [
    "## 3.2 Combining Pandas and Shapely functionalities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f3c906c",
   "metadata": {},
   "source": [
    "Note that you can use  the shapely functionalities we covered in class on Monday by selecting the geomeries from this GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the stations are points, they have no area\n",
    "stations['geometry'].area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc6809",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations['geometry'].buffer(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fde4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns the x coordinate of each row's point\n",
    "stations['geometry'].x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef55bf35",
   "metadata": {},
   "source": [
    "You can also use the Pandas functionalities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f877cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.loc[1:10,'geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we can take the head of any column \n",
    "# or subset of columns \n",
    "\n",
    "stations['name'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1bb94d9",
   "metadata": {},
   "source": [
    "## 3.3 Mapping\n",
    "Let's talk about mapping in a bit more detail here.\n",
    "\n",
    "You can also read the [GeoPandas user guide](https://geopandas.org/en/stable/docs/user_guide/mapping.html) for more on mapping. \n",
    "\n",
    "Beyond the basic map we just made above, we use the following to enhance our map a bit: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eee855b5",
   "metadata": {},
   "source": [
    "We can specify which column will determine the color schema of the plot. If the column is a numeric data type (`int` or `float`) `plot()` will try to create a **choropleth** map, if the column is a `str` or `obj`, it will try to create a **categorical** map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column 'line' is an object type here\n",
    "stations['line'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b532250",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.plot(column='line')\n",
    "\n",
    "# This plot is not very informative\n",
    "# since there are many different combinations of lines that pass through \n",
    "# each station\n",
    "# The map does not exactly line up with subway lines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b9c97a4",
   "metadata": {},
   "source": [
    "We can also specifcy the size of our plot using the `figsize=` optional input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we're using the `figsize` argument to make the plot bigger\n",
    "# figsize takes a tuple of (width, height) in inches\n",
    "stations.plot(column='line',\n",
    "            figsize=(10,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "856f50f8",
   "metadata": {},
   "source": [
    "Because the dimensions of the plot are constrained by the CRS we use, `figsize` is going to find the largest plot it can create given these dimension constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004fc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces the same size plot as above\n",
    "stations.plot(column='line',\n",
    "            figsize=(20,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25aa1b39",
   "metadata": {},
   "source": [
    "You can also add a legend to this plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces the same size plot as above\n",
    "stations.plot(column='line',\n",
    "            figsize=(20,10),\n",
    "            legend=True)\n",
    "\n",
    "# This legend is not very informative \n",
    "# given the issue mentioned above \n",
    "# that there are many different combinations of lines that pass through\n",
    "# each station"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "200be2a6",
   "metadata": {},
   "source": [
    "# 4. Working with multiple geospatial datasets. \n",
    "\n",
    "Most often, we are working with multiple datasets in order to analyze their relationship to each other.\n",
    "\n",
    "\n",
    "For this example, let's take a look at public housing accessibility from a transit perspective. \n",
    "\n",
    "First, download the NYCHA public housing data from [here](https://data.cityofnewyork.us/Housing-Development/Map-of-NYCHA-Developments/i9rv-hdr5) and save it down in the same folder that contains this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bad file name again! Also, why do they call it a \"Map\"?\n",
    "public_housing = gpd.read_file('Map of NYCHA Developments.zip')\n",
    "\n",
    "# We are going to filter the data to exclude MultiPolygon objects for this exercise\n",
    "public_housing = public_housing[public_housing['geometry'].type!='MultiPolygon']\n",
    "\n",
    "# oops, there's a typo in the column name where \"development\" is spelled\n",
    "# \"developmen\". \n",
    "# Let's fix that.\n",
    "public_housing = public_housing.rename(columns={'developmen':'development'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f8c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_housing.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This look very faint, because we're working with building footprints \n",
    "# at the scale of the entire city\n",
    "public_housing.plot(figsize=(10,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7fac131",
   "metadata": {},
   "source": [
    "Let's first change our CRS to 3857. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c56f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_housing = public_housing.to_crs(epsg=3857)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d884a873",
   "metadata": {},
   "source": [
    "Now, let's say we want calculate **how many subway stations are within a 10 minute walk of each housing unit**. \n",
    "\n",
    "We are going to do this by: \n",
    "- Providing an estimate of the distance a typical person can walk in 10 minutes\n",
    "- Creating a new geometry that is buffered around each building by that distance.\n",
    "\n",
    "\n",
    "A quick google search tells me 10 minutes is about 800 meters based on average walking speeds. \n",
    "\n",
    "## 4.1 Making a new GeoDataFrame\n",
    "Let's make a new dataset that buffers each building with a 800 meter distance but still has the original tabular data of our public housing dataset. Recall that a GeoDataFrame takes a dataframe, a CRS, and a set of geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c44d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's make our geometries\n",
    "buffer_geom = public_housing['geometry'].buffer(800)\n",
    "\n",
    "# Second, we already know the CRS\n",
    "# This the same as the CRS of public housing data \n",
    "buffer_crs = public_housing.crs \n",
    "\n",
    "# Third, let's grab the data we want\n",
    "buffer_data = public_housing[['borough', 'development', 'tds_num']]  \n",
    "\n",
    "# Now, let's put it all together using the GeoDataFrame constructor\n",
    "public_housing_buffer = gpd.GeoDataFrame(buffer_data, crs=buffer_crs, geometry=buffer_geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that I've started new lines for each argument \n",
    "# to make it easier to read\n",
    "public_housing_buffer.plot(figsize=(10,10),\n",
    "                            facecolor=\"none\", \n",
    "                            edgecolor=\"green\", \n",
    "                            lw=.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88c8fae3",
   "metadata": {},
   "source": [
    "Whoa, what happened here?? Beyond setting the figure size, I'm including other optional inputs that allows me to style these more clearly. \n",
    "- `facecolor` is the fill color, which I want to set to \"none\" to make the polygons transparent\n",
    "- `edgecolor` is the edge color, and `plot()` [recognizes certain named colors](https://matplotlib.org/stable/gallery/color/named_colors.html). \n",
    "- `lw` allows me to set the line weight. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1603ba1b",
   "metadata": {},
   "source": [
    "## 4.1.1 (Detour) Making a new GeoDataFrame from a CSV  \n",
    "Let's say we have a CSV with a latitude and longitude column. We can easily turn this into a GeoDataFrame by transforming these lat/lng shapely `Points`. \n",
    "\n",
    "As an example: \n",
    "- download the CSV of [NYC Firehouses](https://data.cityofnewyork.us/Public-Safety/FDNY-Firehouse-Listing/hc8x-tcnd). (It's under **Export**.)\n",
    "- Make sure this CSV is in the same folder as this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8538e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "firehouses_csv = pd.read_csv('FDNY_Firehouse_Listing.csv')\n",
    "firehouses_csv.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a45f92d1",
   "metadata": {},
   "source": [
    "In order to create a GeoDataFrame, we need to Geopandas all three components (data/DF, CRS, and geometry) of the GeoDataFrame. We will use the `points_from_xy()` function in GeoPandas. This is basically using shapely `Points` under the hood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I know this is 4326 but it also says on the Firehouses listing page:\n",
    "# \"Latitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326)\"\n",
    "firehouses = gpd.GeoDataFrame(firehouses_csv, \n",
    "                            geometry=gpd.points_from_xy(firehouses_csv['Longitude'], firehouses_csv['Latitude']),\n",
    "                            crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eddb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voila\n",
    "firehouses.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dd77e9f",
   "metadata": {},
   "source": [
    "Going back to our public housing buffer data, let's see what it looks like."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99de9bb4",
   "metadata": {},
   "source": [
    "## 4.2 Creating a spatial join \n",
    "\n",
    "Now, let's count how many subway stations are in each buffer to get a sense of transit accessibility by using a spatial join between our new `public_housing_buffer` dataset and our `subway_stops` dataset. \n",
    "\n",
    "We are going to use  `gpd.sjoin(left_geoDF,right_geoDF)`. This function optionally takes as an input `how` to specify what type of spatial join. \n",
    "\n",
    "There are couple of different types of spatial joins: \n",
    "- `Left outer join`: In a LEFT OUTER JOIN (how='left'), we keep all rows from the left and duplicate them if necessary to represent multiple hits between the two dataframes. We retain attributes of the right if they intersect and lose right rows that don’t intersect. A left outer join implies that we are interested in retaining the geometries of the left.\n",
    "- `Right outer join`: In a RIGHT OUTER JOIN (how='right'), we keep all rows from the right and duplicate them if necessary to represent multiple hits between the two dataframes. We retain attributes of the left if they intersect and lose left rows that don’t intersect. A right outer join implies that we are interested in retaining the geometries of the right.\n",
    "- `Inner join` (this is the default setting): In an INNER JOIN (how='inner'), we keep rows from the right and left only where their binary predicate is True. We duplicate them if necessary to represent multiple hits between the two dataframes. We retain attributes of the right and left only if they intersect and lose all rows that do not. An inner join implies that we are interested in retaining the geometries of the left.\n",
    "\n",
    "In this case, we want to join `public_housing_buffer` and `stations` and a **left outer join** because we want to keep the hits between our public housing buffer and all the subway stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we assign this to a new variable, \n",
    "# let's check to see what the join looks like\n",
    "gpd.sjoin(public_housing_buffer,stations,how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "298ead72",
   "metadata": {},
   "source": [
    "As we can, see there are duplicate rows of the buffers where each buffered geometry has intersected with multiple stations.\n",
    "\n",
    "Great, since that looked like it worked, let's assign it to a new variable name: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffers_w_stations = gpd.sjoin(public_housing_buffer,stations,how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "028466de",
   "metadata": {},
   "source": [
    "Lastly, we are going use a common pandas operation I'm going to call **groupby-and-summarize**. Here, we aggregate our tabular data by a column containing a category (here, we are aggregating by `development` and perform some kind of summary function, typically something like a count, mean, min, or max. \n",
    "\n",
    "It's constructed like so: \n",
    "- `df.groupby('col_name').sum()` or\n",
    "- `df.groupby('col_name').count()` or\n",
    "- `df.groupby('col_name').min()`\n",
    "\n",
    "Note, however, that the sum/count/min will be applied to all numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are counting all the instances of each column value per development. \n",
    "buffers_w_stations.groupby('development').count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a093b2d8",
   "metadata": {},
   "source": [
    "## Q.1 In-Class Execise #1 (2pts)\n",
    "Q: In our the code `buffers_w_stations.groupby('development').count()` why do some rows have the same count all the way across and why do some rows have different counts in the row? ? \n",
    "\n",
    "A: [answer in the markdown cell here]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "158fc02e",
   "metadata": {},
   "source": [
    "Here, the column we are interested in is `objectid` which is now displaying a count of the different station IDs within each development's buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_counts = buffers_w_stations.groupby('development').count()['objectid']\n",
    "station_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "721c74a0",
   "metadata": {},
   "source": [
    "## 4.3 Attribute joins \n",
    "Now, let's join this back to our `public_housing_buffers` GeoDataFrame so we can map it. \n",
    "\n",
    "In an attribute join, a `GeoSeries` or `GeoDataFrame` is combined with a regular `pandas.Series` or `pandas.DataFrame` based on a common variable. This is analogous to normal merging or joining in pandas.\n",
    "\n",
    "This is what a merge looks like visually\n",
    "\n",
    "</figure>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*ZCpo3gXuXI4KFhKivEt2ZA.png \" alt=\"drawing\" width=\"700\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffce1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .merge() takes as an argument the dataframe you want to merge with\n",
    "# and the left and right columns you want to merge on\n",
    "# Here, we're merging on the index of the station_counts pandas.Series\n",
    "# and the development column of the public_housing_buffer geodataframe\n",
    "public_housing_buffer.merge(station_counts, \n",
    "                            left_on='development', \n",
    "                            right_index=True)\n",
    "\n",
    "# You will typically be merging on a column, not an index\n",
    "# Here station_counts has the development column as its index   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46bdbd8d",
   "metadata": {},
   "source": [
    "That worked! Let's update our `public_housing_buffer` variable name to point to our updated geodataframe with this new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_housing_buffer = public_housing_buffer.merge(station_counts, \n",
    "                            left_on='development', \n",
    "                            right_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c7e6a7f",
   "metadata": {},
   "source": [
    "The last thing I want to change is the column name from `objectid`, which it is not, to somethign more descriptive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcfad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_housing_buffer = public_housing_buffer.rename(columns={'objectid':'station_count'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3313c944",
   "metadata": {},
   "source": [
    "## 4.4 Writing to a file\n",
    "Now let's write this buffer data we've created to a file. The default is writing to a shapefile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85283b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will write to a folder containing a .shp, .shx, .dbf, and .prj file\n",
    "public_housing_buffer.to_file('public_housing_buffer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e404a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will write to a single .geojson file\n",
    "# You need to specify the driver\n",
    "public_housing_buffer.to_file('public_housing_buffer_geojson',driver='GeoJSON')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22d2951d",
   "metadata": {},
   "source": [
    "## 4.5 Making a choropleth map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f3827b4",
   "metadata": {},
   "source": [
    "Now let's make a choropleth map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_housing_buffer.plot(column='station_count',\n",
    "                            figsize=(15,15),\n",
    "                            legend=True,\n",
    "                            alpha=.6)\n",
    "# Alpha is a value between 0 and 1 that controls the transparency of the fill color"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f45e7f0f",
   "metadata": {},
   "source": [
    "  The default color map (`cmap`) in GeoPandas is `viridis`. You can find other ones [here](https://matplotlib.org/stable/tutorials/colors/colormaps.html). Let's change our color map to `Reds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88901ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_housing_buffer.plot(column='station_count',\n",
    "                            figsize=(15,15),\n",
    "                            legend=True,\n",
    "                            alpha=.6,\n",
    "                            cmap='Reds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8775921",
   "metadata": {},
   "source": [
    "Ok, this is not a great map (and probably won't be today) since it's, without more context, a poetic suggestion of transit accessibility. \n",
    "\n",
    "## 4.6 Mapping multiple layers on the same map. \n",
    "\n",
    "To give some context to our map, let's plot our subway stations, public housing buildings, and building buffer data together. \n",
    "\n",
    "We are going to use a library called `matplotlib` (it's actually being used by `.plot()`). We'll cover this library more extensively in the coming week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad707fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot module from matplotlib and assign it the nickname plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.subplots() returns a tuple of the figure and the axis, \n",
    "# which we are assigning to fig and ax, respectively\n",
    "\n",
    "# Note that using this method of creating a plot, \n",
    "# we set the figure size using the figsize argument in plt.subplots()\n",
    "fig1, ax1 = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "# The drawing order is determined by the order in which we call the plot methods\n",
    "\n",
    "# We can use the ax argument to specify the axis we want to plot on\n",
    "# Here, we're plotting the public housing buffer on the axis we just created\n",
    "public_housing_buffer.plot(column='station_count',\n",
    "                            ax=ax1,\n",
    "                            alpha=.6,\n",
    "                            cmap='Reds')\n",
    "\n",
    "# markersize is the marker size\n",
    "stations.plot(markersize=5,\n",
    "            color='black',\n",
    "            ax=ax1)\n",
    "# markersize is the marker size\n",
    "public_housing.plot(\n",
    "            color='red',\n",
    "            ax=ax1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd8870a4",
   "metadata": {},
   "source": [
    "Still not a great map, but at least we have a bit more context."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7396aa61",
   "metadata": {},
   "source": [
    "## Q.2 In-Class Exercise 2 (2pts)\n",
    "Which building or buildings have the highest| number of stations within a 10 min walk? Show the code you used to get this answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ecfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0421549",
   "metadata": {},
   "source": [
    "## Q.3 In-Class Exercise 3 (5pts)\n",
    "Create a new column called `area` in `public_housing_buffer` that the area of the original building footprints in meters. (Hint: you'll need to do a `merge`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a033da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "473517d3",
   "metadata": {},
   "source": [
    "## Q.4 In-Class Exercise 4 (2 pts)\n",
    "- From the NYC open data portal, download a [shapefile of neighborhoods](https://data.cityofnewyork.us/City-Government/2010-Neighborhood-Tabulation-Areas-NTAs-/cpf4-rkhq)\n",
    "- Make sure to change the CRS so it matches the other layers. \n",
    "- Add it to the map in section 4.5 *first* (i.e. below the other layers)\n",
    "- Make the fill color of the neighorhoods `lightgray`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c11ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3988b25",
   "metadata": {},
   "source": [
    "## Q.5 In-Class Exercise 5 - OPTIONAL (5pts)\n",
    "Separately, create a choropleth map that shows the number of subway stations in each neighborhood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('gds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:12:38) \n[Clang 11.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc571d7ca67236538d190807671ab3198970b7d67f23d825ad141ff90f68066a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
