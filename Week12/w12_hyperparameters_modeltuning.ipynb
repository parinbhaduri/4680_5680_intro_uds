{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-gambling",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99a7a244",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Use cross-validation\n",
    "- Find the best model for classification and regression problems based on tuning hyperparameters and calculating performance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-introduction",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "from pysal.lib import weights\n",
    "\n",
    "import contextily\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17628626",
   "metadata": {},
   "source": [
    "## 0.1 Import the data\n",
    "\n",
    "For this exercise, let's use the San Diego AirBnB data set again. As a reminder: This dataset contains house intrinsic characteristics, both continuous (number of beds as in `beds`) and categorical (type of renting or, in Airbnb jargon, property group as in the series of `pg_X` binary variables), but also variables that explicitly refer to the location and spatial configuration of the dataset (e.g., distance to Balboa Park, `d2balboa` or neighborhood id, `neighborhood_cleansed`).\n",
    "\n",
    "\n",
    "Our aim here is to make two kinds of predictions: \n",
    "- **`log_price`** (Regression): We want to use other features to predict the log(Price) of each airbnb\n",
    "- **`coastal`** (Classification): We also want to use our features to see if we can predict whether an Airbnb is coastal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b832b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = gpd.read_file(\"https://www.dropbox.com/s/zkucu7jf1xug869/regression_db.geojson?dl=1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ed271",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5182eb",
   "metadata": {},
   "source": [
    "Again, notice here that we have: \n",
    "- **Discrete variables** (number of bedrooms, beds, baths)\n",
    "- **Dummy variables** (whether there is a pool, whether near the coast, room type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a770dfc6",
   "metadata": {},
   "source": [
    "# 1. `log_price`\n",
    "Let's start off with predicting the price of the Airbnb. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b565af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = db['log_price']\n",
    "\n",
    "## We'll make our X, independent variables, the \"kitchen sink\" of all of our other variables for now. \n",
    "## I'm using all the variables we have available with the exception of `neighborhood`, which we have to turn into dummy variables in a second. \n",
    "\n",
    "X = db[['accommodates', 'bathrooms', 'bedrooms', 'beds', 'pool',\n",
    "       'd2balboa', 'coastal', 'pg_Apartment',\n",
    "       'pg_Condominium', 'pg_House', 'pg_Other', 'pg_Townhouse',\n",
    "       'rt_Entire_home/apt', 'rt_Private_room', 'rt_Shared_room']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2cba824",
   "metadata": {},
   "source": [
    "## 1.1 Feature engineering\n",
    "Feature engineering is the process of creating new variables from the ones you already have. Common feature engineering tasks include:\n",
    "- Creating dummy variables from categorical variables\n",
    "- Creating interaction terms between variables\n",
    "- Creating polynomial terms from variables\n",
    "- Creating log or square root terms from variables\n",
    "- Creating lagged variables from time series data or lagged spatial variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_dummies = pd.get_dummies(db['neighborhood'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f31ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce981b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here, I want to concatenate my X and neighborhood_dummies into one dataframe.\n",
    "## I need to tell pd.concat() to either add new columns (axis=1) or add new rows (axis=0).\n",
    "## The default is axis=0, ie new rows, so I need to specify axis=1.\n",
    "X = pd.concat([X, neighborhood_dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d711091",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8da70da6",
   "metadata": {},
   "source": [
    "Let's also create a new column that is the KNN spatial lag for the 'neighborhood context'. Here, I'm going to use the columns: \n",
    "- `pool`, which is a binary (0,1) variable for whether the airbnb has a pool\n",
    "- `pg_House` which is a binary (0,1) variable for whether the airbnb is a house\n",
    "\n",
    "I will choose K=20, to give me the 20 closest neighboring Airbnbs. My spatial lag should be a number between 0 and 20 to estimate, of the 20 closest Airbnb, how many have pools and how many are other housees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = weights.KNN.from_dataframe(db, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_lag = weights.lag_spatial(knn, db['pool'])\n",
    "house_lag = weights.lag_spatial(knn, db['pg_House'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6da1a81c",
   "metadata": {},
   "source": [
    "The chart below shows that neighboring airbnbs mostly don't have a pool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b550b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pool_lag)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e29d695",
   "metadata": {},
   "source": [
    "But there are a lot more neighborinng Airbnbs that are houses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08209f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(house_lag)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6857883b",
   "metadata": {},
   "source": [
    "Add these to new features to our original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['pool_lag'] = pool_lag\n",
    "X['house_lag'] = house_lag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f668afa",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Create our Train-Test Split\n",
    "We almost always start off with splitting our data into our **train** and **test** sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Let's use the default split for now, which is 75-25 train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e90bb038",
   "metadata": {},
   "source": [
    "## 1.2 Predict the data\n",
    "Here, let's use a decision tree regressor to predict the price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7273cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ee508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c24418e0",
   "metadata": {},
   "source": [
    "## 1.3 Cross-validation\n",
    "Now we use the k-fold cross-validation method is run our model several times on different parts of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c798a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The default scoring metric for Random Forest is R^2, so we can use cross_val_score() to get the R^2 for each fold.\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1bae9e7",
   "metadata": {},
   "source": [
    "As you can see, there is some variation here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caaa1275",
   "metadata": {},
   "source": [
    "Our average R^2 is about 45.5%. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17522117",
   "metadata": {},
   "source": [
    "## 1.4 Test score \n",
    "Let's see how well our model does on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the second set of data\n",
    "ypred_rf = model.predict(X_test)\n",
    "print(\"R^2 is:\", r2_score(y_test, ypred_rf))\n",
    "print(\"Mean absolute error is:\", mean_absolute_error(y_test, ypred_rf))\n",
    "print(\"Mean squared error is:\", mean_squared_error(y_test, ypred_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ffb9976",
   "metadata": {},
   "source": [
    "So the model performed slightly worse - 49% - on the test set. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fca9cb56",
   "metadata": {},
   "source": [
    "## 1.5 Tuning our hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5490da16",
   "metadata": {},
   "source": [
    "### 1.5.1 Tweaking our trees\n",
    "Let's say we think to get a better score, we need to maximum tree depth. Let's test this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6465fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = np.linspace(1,20,20).astype(int)\n",
    "trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c5defce",
   "metadata": {},
   "source": [
    "The below might take a bit of time to run. (uff it took 3 minutes and 14 seconds for me!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e43e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create an empty list to hold our scores\n",
    "mean_scores_train = []\n",
    "mean_scores_test = []\n",
    "\n",
    "## Now, let's loop through our trees and get the mean score for each\n",
    "for t in trees: \n",
    "    model = DecisionTreeRegressor(max_depth=int(t))\n",
    "    model.fit(X_train, y_train)\n",
    "    scores_train = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    ypred_rf = model.predict(X_test)\n",
    "    score_test = r2_score(y_test, ypred_rf)\n",
    "\n",
    "    mean_scores_train.append(scores_train.mean())\n",
    "    mean_scores_test.append(score_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trees,mean_scores_train,color='blue',label='Train')\n",
    "plt.plot(trees,mean_scores_test,color='red',label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50db2cfa",
   "metadata": {},
   "source": [
    "Two insights emerge: \n",
    "1. Setting a maximum depth generally produced better scores than the default of not setting any depth (just letting the tree split until each leaf is \"pure\", i.e. contains values of the same category). \n",
    "2. We can see from this that after about a 5 maximum depth, our performance starts to decrease. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50c111fa",
   "metadata": {},
   "source": [
    "## 1.6 Grid Search\n",
    "But what about other parameters? \n",
    "\n",
    "`sklearn` has a way of optimizing for all the hyperparameters you'd like to tune. Let's say, here, we want to test the following hyperparatmers. You can see the full list for this algorithm in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html): \n",
    "- The loss criterion: `criterion`, which can be {“squared_error”, “friedman_mse”, “absolute_error”, “poisson”}\n",
    "- The maximum tree depth: `max_depth`\n",
    "- The minimum number of samples required for the next split: `min_samples_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "              'max_depth': np.linspace(1,20,20).astype(int),\n",
    "              'min_samples_split': np.linspace(1,100,20).astype(int)}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2da3288b",
   "metadata": {},
   "source": [
    "~~This might take a while since we're doing 8000 model fits!~~ Don't run this now as it takes 80 minutes. You can try to decrease the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d233b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ccf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(grid.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7496a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f48dd61e",
   "metadata": {},
   "source": [
    "`grid.best_params` got me `{'criterion': 'poisson', 'max_depth': 6, 'min_samples_split': 89}`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65b13aad",
   "metadata": {},
   "source": [
    "## Q.1 Classification\n",
    "Instead of predicting the log price, now instead predict whether an AirBnB is coastal. Using the same steps (feature engineering, train-test split, cross-validation, hyperparameter tuning) as above, **select one hyperparameter** to optimize for a classification model. It doesn't have to be a Decision Tree! (10 pts)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
