{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a2ed0e",
   "metadata": {},
   "source": [
    "# Homework 4 (84 pts)\n",
    "\n",
    "## Overview\n",
    "This homework assignment is divided into two parts: regionalization and exploring how spatial regression can incorporate space into regression analysis. Although numerous variations of spatial regression are available in lecture examples and online resources, this assignment will only focus on the most commonly used models: slx, spatial error, and spatial lag models.\n",
    "\n",
    "The primary objective of this assignment is to gain a deeper understanding of regionalization, spatial regression, and their practical application. By working with census data in NYC, you will gain insight into how spatially related areas can be clustered. Additionally, by analyzing Airbnb data in NYC, you will learn how spatial variables can impact regression outcomes and how to incorporate them into your analysis. \n",
    "\n",
    "## Grading\n",
    "Each exercise will be graded based on the following rubrics:\n",
    "- 2 points. Completed the task, presented the expected results, and codes were well documented and explained.\n",
    "- 1 point. Completed the task with some disparity from the expected results.\n",
    "- 0 point. Did not complete the excercise.\n",
    "\n",
    "## Late submission policy\n",
    "For late submission, every extra day after the due date will lead to 20% off of your points. Less than a day will be counted as one day. All submissions after the TA team posts the answers will not be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f32d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import numpy as np\n",
    "from shapely.geometry import shape\n",
    "import cenpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc1a293",
   "metadata": {},
   "source": [
    "## Regionalization\n",
    "### 1. Retrieve and Clean Data (6 pts)\n",
    "For the first section of this assignment, census data at the census tract level will be utilized. Similar to homework 2, you will retrieve census data using the 2015-2019 5-year estimate from the American Community Survey. Only NYC will be downloaded with the following variables: 'B19013_001E', 'B25077_001E', 'B15003_001E','B15003_022E', 'B02001_001E', 'B02001_002E', 'B25008_001E', 'B25008_003E', 'B11001_001E', 'B11001_006E', 'B25018_001E', 'B19083_001E',  'B01002_001E',  and 'B08012_001E'. The retrieved data will be stored in a dataframe named **acs_data**. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a77ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9995db",
   "metadata": {},
   "source": [
    "To generate new columns in acs_data, refer to [ACS Detailed Table Shells](https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.2019.html#list-tab-LO1F1MU1CQP3YOHD2T). These new columns will include median household value, percentage of white, percentage of renters, percentage of female-led households, percentage of bachelor's degree holders, median number of rooms, Gini index, median age, and travel time to work. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc72263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f35a8f",
   "metadata": {},
   "source": [
    "In order to meet the requirements for later analysis, all nan values in acs_data should be replaced with 0. Following that, ONLY the above 9 variables should be rescaled using the `robust_scale()` function. The resulting scaled dataframe should be saved as **acs_scaled**. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ae1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98a442",
   "metadata": {},
   "source": [
    "### 2. Analyze Data (2 pts)\n",
    "Despite collecting all variables for each census tract, the underlying spatial characteristics are still uncertain. To confirm the applicability of regionalization to this dataset, it is necessary to determine whether spatial autocorrelation exists in these variables.\n",
    "\n",
    "To do this, create a spatial weight matrix using the Queen method and calculate the global Moran's I for all 9 variables. Then, present your results in a dataframe that displays Moran's I and P-value for each variable. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db202d17",
   "metadata": {},
   "source": [
    "### 3. Regionalization (6 pts)\n",
    "Create the regionalization model based on **acs_scaled** by specifying the linkage as \"ward\" and the number of clusters as 5. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c413cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc9e7a",
   "metadata": {},
   "source": [
    "Using the results from your model, create a choropleth map of the 5 clusters. Remember to add a legend. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a4827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046c405",
   "metadata": {},
   "source": [
    "Regionalization results heavily rely on the spatial constraint, which is determined by the spatial weight matrix. To explore the impact of the spatial weight matrix on the results, try a new spatial matrix using 4 nearest neighbors.\n",
    "\n",
    "Using ward linkage and 5 clusters specified in your function, generate a new regionalization model. Then, plot the new clusters with a legend. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817925cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccbb160",
   "metadata": {},
   "source": [
    "### 4. Evaluate Regionalization Models (6 pts)\n",
    "To determine which regionalization model is \"better,\" you can compare your models based on measures of geographical coherence and goodness of fit.\n",
    "### 4.1 Geographical Coherence (2 pts)\n",
    "Compute the isoperimetric quotient for each cluster in both models. Then, concatenate the isoperimetric quotients into a new dataframe with columns labeled as *ward5wq* and *ward5wknn* and rows representing the 5 clusters. Show your dataframe. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca39ad2",
   "metadata": {},
   "source": [
    "### 4.2 Feature Coherence (goodness of fit) (4 pts)\n",
    "Another measurement that can be used to evaluate the quality of the regionalization models is the `metrics.calinski_harabasz_score()` (CH) function, which calculates the ratio of within-cluster variance to between-cluster variance. Compute the CH score for both the original model with Queen spatial weight matrix and the model with 4 nearest neighbors. Then, report the CH scores of both models. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca2e99c",
   "metadata": {},
   "source": [
    "Based on Geographical Coherence and Feature Coherence, which regionalization model performs better? And why? (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76968d2c",
   "metadata": {},
   "source": [
    "## Spatial Regression\n",
    "The second section uses Airbnb listing in Manhattan. Since you have been familiar with the datasets, codes are provided for retrieving data. \n",
    "### 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_qr = \"https://data.cityofnewyork.us/resource/63ge-mke6.json?$where=BoroName='Manhattan'\"\n",
    "ct = pd.read_json(url_qr)\n",
    "ct['the_geom'] = ct['the_geom'].apply(shape)\n",
    "ct = gpd.GeoDataFrame(ct, geometry='the_geom').set_crs(epsg = 4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da850a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listing_raw = pd.read_csv('http://data.insideairbnb.com/united-states/ny/new-york-city/2022-12-04/data/listings.csv.gz')\n",
    "listing_raw.price = listing_raw.price.replace('[\\$,]', '', regex=True).astype(float)\n",
    "listing_g = gpd.GeoDataFrame(listing_raw, \n",
    "                             geometry = gpd.points_from_xy(listing_raw.longitude, \n",
    "                                                           listing_raw.latitude)).set_crs(epsg = 4326)\n",
    "listing = gpd.tools.sjoin(listing_g, ct[['geoid', 'boroname', 'the_geom']], predicate=\"within\", how='inner')\n",
    "listing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0fe721",
   "metadata": {},
   "source": [
    "### 2. Clean and Visualize Data (24 pts)\n",
    "### 2.1 Clean Data (8 pts)\n",
    "Before proceeding with any regression analysis, it is always important to familiarize yourself with the data. One of the assumptions of Ordinary Least Squared (OLS) linear regression is that the underlying distribution of each variable should follow a normal distribution. One way to check for normality is by plotting the frequency of values in each variable using a histogram.\n",
    "\n",
    "Make a histogram of *price* and answer whether it follows normal distribution in a separate markdown cell. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b59e4",
   "metadata": {},
   "source": [
    "Notice that 0 may exist in *price* column, which makes no sense in real world because no Airbnb is offered for free. To confirm whether 0 exists, use `describe` function to check the numeric distribution of *price*. If yes, remove all rows with 0 in *price*. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8258a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2039ac",
   "metadata": {},
   "source": [
    "For analytical purposes, it is common to transform highly skewed variables into data that appears to have a normal distribution. One commonly used transformation is to take the logarithm of the original variable.\n",
    "\n",
    "Take the log value of *price* (named as *log_price*) and make a histogram plot of *log_price*. (2 pts). \n",
    "\n",
    "Does it look like a normal distribution? Answer in a separate markdown cell. (2 pts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ce41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0854de5",
   "metadata": {},
   "source": [
    "### 2.2 Create Dummy Variables (12 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0a607",
   "metadata": {},
   "source": [
    "Read [lecture example of San Diego Airbnb](https://geographicdata.science/book/notebooks/11_regression.html#exogenous-effects-the-slx-model) and you'll notice that both room type and property type can be important in determining listing prices. However, in order to include this information in a regression analysis, you need to transform these text variables into dummy variables.  \n",
    "\n",
    "Following the same step in the example, you can create four dummy variables (rt_Entire_home/apt, rt_Hotel_room, rt_Private_room, and rt_Shared_room) for room type. Show your **rt** dataframe. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5a653",
   "metadata": {},
   "source": [
    "Another variable that may be important in determining Airbnb listing prices is whether the host has been verified with their identity. Similarly, you can create a dummy variable to record this information. Since only t (standing for ture) and f (standing for false) are stored in *host_id_verified*, you can only rename t as the new *host_id_verified* in dataframe **host**. Show your dataframe **host**. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81aa29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d94b2",
   "metadata": {},
   "source": [
    "Though [the San Diego example](https://geographicdata.science/book/notebooks/11_regression.html#exogenous-effects-the-slx-model) provides a method to create dummy variables for property type, the code cannot be directly used in the New York City Airbnb dataset because the property type texts are recorded differently. With the modified *simplify* function, create a new column named *property_group* in **listing** dataframe. Next you can create dummy variables in dataframe **pg**. Show your dataframe **pg**. (2 pts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f529062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(p):\n",
    "    bigs = ['apartment', 'condo', 'townhouse', 'resort']\n",
    "    p_lst = p.split()\n",
    "    for p in p_lst:\n",
    "        if p in bigs:\n",
    "            return p\n",
    "\n",
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232209e",
   "metadata": {},
   "source": [
    "Another important variable that affects listing prices is the number of bathrooms in each listing, which is stored as text in the **listing** dataframe with null values. To prepare the data for analysis, replace all null values in the *bathrooms_text* column with 0. \n",
    "\n",
    "Next, you can create a function called *find_bath* like *simplify*. *find_bath* function will split the text in *bathrooms_text* and return the number of bathrooms in float. (2 pts) \n",
    "\n",
    "Apply this function to *bathrooms_text* to create a new column *bathrooms* in **listing**. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6280875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387b288",
   "metadata": {},
   "source": [
    "Merge dataframe **rt**, **host**, and **pg** to **listing**. Show the infomation of your new **listing**. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c76ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac304b1d",
   "metadata": {},
   "source": [
    "### 2.3 Create Spatial Variable (Distance to City Center) (4 pts)\n",
    "Given the spatial nature of NYC listing, adding spatial variable can help improve the regression performance. One tpical spatial variable is the distance to city center. Though there is no concensus on where exactly the city center is in NYC, you can use the median center as a proxy. Recall what you've done in Homework 2 to create the median center and name it as *city_center*. (2 pts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d088048",
   "metadata": {},
   "source": [
    "To calculate distance in meters from coordinates, you must first transform the crs into the USA Contiguous Albers Equal Area (provided below) because this datum is measured by meters. Refer to [data cleaning process for San Diego Airbnb](https://geographicdata.science/book/data/airbnb/regression_cleaning.html) and generate *d2center* column in **listing**. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e333ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_crs = \"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 \"\\\n",
    "          \"+lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import Point\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "center = gpd.GeoSeries(Point(city_center[0], city_center[1]), crs=listing.crs).to_crs(tgt_crs)[0]\n",
    "center = (center.x, center.y)\n",
    "\n",
    "d2b = lambda pt: cdist([(pt.x, pt.y)], [center])[0][0]/1000\n",
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9349bd",
   "metadata": {},
   "source": [
    "## 3. Regressions (40 pts)\n",
    "### 3.1 Linear Regression (6 pts)\n",
    "The dependent variables used here include \"accommodates\", \"bedrooms\", \"bathrooms\", \"host_id_verified\", \"rt_Shared_room\", \"pg_apartment\", \"pg_condo\", \"pg_townhouse\", \"pg_resort\", \"d2center\" and the independent variable is \"log_price\". Create a new dataframe **X** which contains all dependent variables and *geometry* in **listing**. Remove all null values in **X** and show the shape of **X**. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c99c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985f0ec",
   "metadata": {},
   "source": [
    "Create another series **y** storing *log_price*. Remember to keep **X** and **y** in the same number of rows. Show the number of rows of **y**. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1907e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203c534",
   "metadata": {},
   "source": [
    "Run OLS based on your **X** and **y**. Interpret your results: Do these variables explain the listing price well? Why or why not? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29145d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a804be",
   "metadata": {},
   "source": [
    "### 3.2 Verification (8 pts)\n",
    "The above OLS model provides a base model for further analysis. From all perspectives, this model should be revised to better represent the reality. To verify whether space plays an important role in explaining listing price, the residuals of OLS can help.\n",
    "\n",
    "Create a new column *residual* in dataframe **X**. Plot the residuals with 5 quantile using your color. Add legend and basemap to your map. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38481b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae7c9b",
   "metadata": {},
   "source": [
    "How do you interpret the spatial distribution of your OLS residuals? Is it clustered or scattered across NYC? (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e321e3",
   "metadata": {},
   "source": [
    "In addition to visual check, Moran's I provides more proof on spatial distribution. Create a row-standardized spatial weight matrix (named as wd) from distance-based knn method. Use 4 as the number of neighbors. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ba8eb",
   "metadata": {},
   "source": [
    "Next, you can check the global Moran's I with the residual. What is the value? And how can you conclude the spatial characteristics of the residual? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e453dc",
   "metadata": {},
   "source": [
    "### 3.3 Exogenous Effect -- SLX Model (8 pts)\n",
    "With spatial clustering exists in the OLS residuals, it implies that adding spatial components to your regression can improve the model performance. Assuming that there is no interaction between the dependent variables and their spatial lags, SLX model can be built using `spreg.OLS` function as the spatial lags of the dependent variables are treated as exogenous variables.\n",
    "\n",
    "Create only spatial lags for dummy variables of property types condo, townhouse, and resort. Rename the spatial lags by adding \"w_\" before their original names. Store these new variables in dataframe **wx**. (2 pts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b445d",
   "metadata": {},
   "source": [
    "Create a new dataframe **slx_exog** by merging **X** and **wx**. Now you can apply `spreg.OLS` function to the new dataframe and fit the model. Present your model results below. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f558d1",
   "metadata": {},
   "source": [
    "Compare the result with the OLS result. Is there any difference with the coefficients of each variable? Are there any new variables? Are these coefficients all statistically significant? (2 pts)\n",
    "\n",
    "Does slx model improve the performance of OLS? (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126627ad",
   "metadata": {},
   "source": [
    "### 3.4 Spatial Error Regression (6 pts)\n",
    "Another assumption is that spatial clustering exists only in the error terms of OLS model. Use the same dependent and independ variables and spatial weight matrix *wd* for your spatial error model. Report the model results below. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0eff2b",
   "metadata": {},
   "source": [
    "Compare the result with the OLS result. Is there any difference with the coefficients of each variable? Are these coefficients all statistically significant? (2 pts) \n",
    "\n",
    "What is the meaning of lambda? Does spatial error model improve the performance of OLS? (2 pts) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24996464",
   "metadata": {},
   "source": [
    "### 3.5 Spatial Lag Regression (6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b9f68",
   "metadata": {},
   "source": [
    "You can also assume that spatial clustering exists in the dependent variables of the OLS model. Use the same dependent and independ variables and spatial weight matrix *wd* for your spatial lag model. Report the model results below. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99bb4d3",
   "metadata": {},
   "source": [
    "Compare the result with the OLS result. Is there any difference with the coefficients of each variable? Are these coefficients all statistically significant? (2 pts) \n",
    "\n",
    "What is the meaning of W_log_price? Does spatial lag model improve the performance of OLS? (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd9967",
   "metadata": {},
   "source": [
    "### 3.6 Other model (6 pts)\n",
    "To better improve the model performance, what else will you do? For example, you can create a new spatial variable and pick up one of the spatial models to build a new model. Show how you create the new spatial variable (2 pts) and new model results (2 pts). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd81ba7",
   "metadata": {},
   "source": [
    "How does your variable improve the models? Answer in a separate markdown cell. (2 pts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
