{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "homeless-heart",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Get Census data from the IPUMS/NHGIS data portal\n",
    "- Get Census data with cenpy\n",
    "- Use the Socrata API\n",
    "\n",
    "\n",
    "Some of today's lessons borrow from: \n",
    "- [The cenpy documentation](https://github.com/cenpy-devs/cenpy)\n",
    "- [The Socrata SODA API documentation](https://dev.socrata.com/consumers/getting-started.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337fb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to start importing the libraries we need\n",
    "# all in one cell. \n",
    "# It is a good practice to keep all the imports in one cell so that\n",
    "# we can easily see what libraries we are using in the notebook.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## The set_context() function is really useful!\n",
    "## It allows us to set the size of the fonts in our plots based on whether \n",
    "## we are making a poster, a talk, a notebook, etc.\n",
    "\n",
    "## If you are only presenting these figures in your jupyter notebook, \n",
    "## there is no need to set the context to be \"talk\" or \"poster\"\n",
    "## But, I sometimes set my context to be \"talk\" or \"poster\" even for articles\n",
    "## because I like the fonts to be bigger.\n",
    "sns.set_context(context='paper')\n",
    "\n",
    "# we use the inline backend to generate the plots within the browser\n",
    "%matplotlib inline\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd8b51",
   "metadata": {},
   "source": [
    "\n",
    "# 0. Census Data: Census survey and statistical boundaries\n",
    "\n",
    "## 0.1 Census Surveys\n",
    "The United States Census Bureau has been collecting information on its residents in the country since 1780 through surveys sent by mail (since 2020, you can submit your survey by phone, mail, or online). Census data is used for a variety of governmental purposes including: provision of housing, infrastructure, and public amenities; making districting decisions for schools, precints, and elections; and more generally, to understand the population, socio-economic, and demographic characteristics of residents in the country. [Did you know that the punch card machine (a prototype for the computer) was created for the 1890 Census?](https://en.wikipedia.org/wiki/Tabulating_machine)\n",
    "\n",
    "The US Census has historically been taken every 10 years. Every household in the U.S. is sent a Census survey (and you are legally required to respond.) In 2005, the Census Bureau created the American Community Survey (ACS), which is collected every month on a sample of households.\n",
    "\n",
    "Since 2020, the Census only contains 10 questions (historically called the \"short form census\") such as age, sex, race, Hispanic origin, and owner/renter status. The ACS contains a larger set of questions such as employment, education, transportation.\n",
    "\n",
    "Because the ACS is more frequent, it is often used for more current census needs; however, because it is also a sample, we generally need a longer time span to get a robust sample. This is why we will often use the **5-yr ACS** (for ex: 2012 - 2016 ACS) to represent the year (here, 2014).\n",
    "\n",
    "Census data is often the baseline survey dataset in the area of urban planning because it provides racial, socio-economic, housing, etc. information that is often the highlight or backdrop of a study.\n",
    "\n",
    "## 0.2 Census Geographies\n",
    "There are different, often nested Census geographic regions used for  different administrative scales. The most commonly used regions are statistical areas, typically nested within each other, whose boundaries are defined by certain physical, administrative, and population constraints. For instance, a **Census block** is bounded by physical features such as streets and administrative boundaries such as city limits and school districts. **Block groups**, the smallest unit of analysis that is still mostly statistically robust, are collections of Census blocks (hence the name) that generally have between 800 to 5000 people. **Census tracts** generally have between 1000 and 8000 people. [Here's more information](https://pitt.libguides.com/uscensus/understandinggeography) about Census geographies if you're curious.\n",
    "\n",
    "See the image below for how these regions nest within one another.\n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/8w69pibhwffgoc0/qgis_censusgeography.png?dl=1\" alt=\"drawing\" width=\"500\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "## 0.3 [Social Explorer](https://www-socialexplorer-com.proxy.library.cornell.edu/ezproxy)\n",
    "This is a great tool for looking at Census and ACS data visually. They also have datasets beyond just Census Bureau data. You can also output images and shareable links to the map. I encourage you to sign up (through Cornell it's free) and explore this tool on your own time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097bc518",
   "metadata": {},
   "source": [
    "\n",
    "# 1. IPUMS\n",
    "\n",
    "I have found that the easiest way to query and download a large Census dataset is to use a service provided by [IPUMS](https://www.ipums.org/)\n",
    "\n",
    "You will see that IPUMS provides data from various sources, including the Census Bureau, the Bureau of Labor Statistics, the National Science Foundation, the National Center for Health Statistics, the Centers for Disease Control, and the National Aeronautics and Space Administration. According to IPUMS, there is also census or survey data available for over 100 countries. For US data, there is Census data going back to 1790.\n",
    "\n",
    "Here, we are using Census data from IPUM's [National Historical Geographic Information System](https://www.nhgis.org/) (NHGIS).\n",
    "\n",
    "## 1.1 Getting started with IPUMS\n",
    "\n",
    "First you'll need to register an account here:\n",
    "https://uma.pop.umn.edu/nhgis/user/new\n",
    "\n",
    "IPUMS will also send you an email verification.\n",
    "\n",
    "From the [NHGIS website](https://www.nhgis.org/), click on **Get Data**. This should take you to a page like this:\n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/jv9aciqkfemgkzt/Screen%20Shot%202023-02-18%20at%2011.51.26%20AM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "## 1.2 Getting data with NHGIS\n",
    "We are going to use the **2017 - 2021** 5-year ACS at the **tract level** in the U.S. to understand the **educational attainment's relationship to income** (Remember: we use the 5-year to represent the median year.) \n",
    "\n",
    "**The aim: get a geospatial dataset of tracts, education, and income**\n",
    "\n",
    "There are two main sections on the page:\n",
    "- **APPLY FILTERS** allows you to choose which datasets and what levels of granularity. (The default year of 2019, but we can change this.)\n",
    "- **SELECT DATA** lets you choose specific tables and columns in your dataset once you've chosen your dataset.\n",
    "\n",
    "Now, let's make the following selections:\n",
    "1. In geographic levels, select `TRACT` amongst the different levels and hit **SUBMIT**\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/xoz9tfsff7ahezv/Screen%20Shot%202023-02-18%20at%2011.48.16%20AM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "2. In years, select `2017-2021` and hit **SUBMIT**\n",
    "3. In topics, select **POPULATION** on the left hand panel, and select `Educational Attainment` and hit **SUBMIT**\n",
    "\n",
    "\n",
    "Now, we can see that our **SELECT DATA** table has been populated by our filter with the relevant scale, topics, and years. A good first step is to sort by Popularity. You should see something like this.\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/f7yobykeef23n8v/Screen%20Shot%202023-02-18%20at%2011.51.54%20AM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "Next to the table name `Educational Attainment for the Population 25 Years and Over`, select the **plus**.\n",
    "\n",
    "4. In topics again, now **de-select** `Educational Attainment` and select `Per Capita Income` and hit **SUBMIT**. \n",
    "And select the `Per Capita Income in the Past 12 Months (in 2021 Inflation-Adjusted Dollars)`. \n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/a76xdvajo488gf2/Screen%20Shot%202023-02-18%20at%2012.06.18%20PM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "After selecting our data, we need to select our Census tract geometries that go along with it.\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/fr5i9rqeay139tp/Screen%20Shot%202023-02-18%20at%2012.07.19%20PM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "Under **SELECT DATA**, go to the third tab and select the 2021 Census Tract.\n",
    "\n",
    "Your data cart in the upper right should show:\n",
    "- `2 SOURCE TABLE`\n",
    "- `0 TIMES SERIES TABLES`\n",
    "- `1 GIS FILE`\n",
    "\n",
    "Confirm this is what you have, then click **Continue**. You'll be taken to a Data Option page, click the **Continue** button again.\n",
    "\n",
    "In the description box, you can write anything. I recommend including a text that has the tables you selected, ACS vintage, and scale. (For ex: `2017-2021 ACS educational attainment, per capita income, tract level`) Then hit **Submit**. (If you haven't logged in already, you might have to do that first.)\n",
    "\n",
    "You'll be taken to your Extracts History. It might take a minute, but soon the **Download Data** column should be populated with two buttons that allow you download the tables and GIS files.\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/5bf3uv5d1tmgbgs/Screen%20Shot%202023-02-18%20at%2012.11.05%20PM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "You should have two zipped files called: \n",
    "- `nhgisXXXX_csv.zip`, where `XXXX` is the job number. \n",
    "- `nhgisXXXX_shape.zip`, where `XXXX` is the job number. \n",
    "\n",
    "Save your `TABLES` and `GIS FILES` to your folder for this class and unzip them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d13b73",
   "metadata": {},
   "source": [
    "## 1.3 Read in the data\n",
    "Finally, let's take a look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracts = gpd.read_file('nhgis0124_shape/nhgis0124_shapefile_tl2021_us_tract_2021.zip')\n",
    "tracts = gpd.read_file('nhgis0001_shape/nhgis0001_shapefile_tl2021_us_tract_2021.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I got an UnicodeDecodeError, when I tried to read the csv without specifying \n",
    "## that the encoding should be latin-1.\n",
    "## Latin-1 encoding is different from UTF-8, which is the default encoding in Python.\n",
    "## Encoding is typically when characters are converted into a binary format (bytes)\n",
    "## to be stored or transmitted.\n",
    "\n",
    "## Specifying the encoding allows the file to be read correctly when we decode it back to characters.\n",
    "# acs_data = pd.read_csv('nhgis0124_csv/nhgis0124_ds254_20215_tract.csv',encoding='latin-1')\n",
    "\n",
    "acs_data = pd.read_csv('nhgis0001_csv/nhgis0001_ds254_20215_tract.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracts[tracts['STATEFP'] == '36'].to_file('ny_tracts.geojson', driver='GeoJSON')\n",
    "# acs_data[acs_data['STATEA'] == 36].to_csv('ny_acs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068dc82e",
   "metadata": {},
   "source": [
    "If you had trouble opening the shapefile for the entire country, I have created two files for just NY state, which are smaller and will be easier too handle: \n",
    "- `ny_tracts.geojson` (download [here]('https://www.dropbox.com/s/e7u3fphsjp7fqd9/ny_tracts.geojson?dl=0'))\n",
    "- `ny_acs.csv` (download [here]('https://www.dropbox.com/s/vvht4wpr954nvj9/ny_acs.csv?dl=0'))\n",
    "\n",
    "You can still use your `xxxx_codebook.txt` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f73988",
   "metadata": {},
   "source": [
    "#### Tracts\n",
    "Let's take a look at the tracts dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts[tracts['STATEFP']=='01'].plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad05829",
   "metadata": {},
   "source": [
    "Notice that we only have geographical characteristics in our shapefile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52147332",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a17041",
   "metadata": {},
   "source": [
    "#### Census data\n",
    "Now, let's take a look at our Census data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e704586",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c050d481",
   "metadata": {},
   "source": [
    "Notice that \n",
    "- Most of the column names are coded in a way that we'll need the `xxxx_codebook.txt` file to parse. \n",
    "- The `GISJOIN` column is the geometry ID that allows us to join the shapefiles to the data. \n",
    "\n",
    "Let's first resolve the column name issue. Open up your codebook, which should be in the same folder as your CSV file. \n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/0tseofmb7c2sl79/Screen%20Shot%202023-02-18%20at%2012.49.23%20PM.png?dl=1\" alt=\"drawing\" width=900\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "Notice that you have many different \"context\" columns that allows your to select by different geographies. If you scroll down we can see what each column name translates into: \n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/gqos37pt7goayt7/Screen%20Shot%202023-02-18%20at%2012.55.24%20PM.png?dl=1\" alt=\"drawing\" width=900\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0de42",
   "metadata": {},
   "source": [
    "#### Column selection \n",
    "Notice that our original data selections from the NHGIS portal has been broken down into \"tables\". \n",
    "- Table 1 contains columns related to educational attainment, where each column represents the **number of people in a tract who's highest educational attainment is X**. \n",
    "- Table 2 is our per capita income for each tract. \n",
    "\n",
    "Let's use **Bachelors degree and above** as our higher ed proxy. Since these are numbers and we probably also want percentages, we'll need the **Total** population 25 years and over. And we'll need the **Per capita income**: \n",
    "- AOP8E001:    Total\n",
    "- AOP8E022:    Bachelor's degree\n",
    "- AOP8E023:    Master's degree\n",
    "- AOP8E024:    Professional school degree\n",
    "- AOP8E025:    Doctorate degree\n",
    "- AORME001:    Per capita income in the past 12 months (in 2021 inflation-adjusted dollars)\n",
    " \n",
    "\n",
    " As we saw above, there are 95 columns in our `acs_data` df! I don't really need all of those to proceed as I have a clear, targeted question. Therefore, I will filter my columns in `acs_data` to only contain those I need for my analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I also need the GISJOIN so I can join this to my shapefiles\n",
    "cols_need = ['GISJOIN','AOP8E001','AOP8E022','AOP8E023','AOP8E024','AOP8E025','AORME001']\n",
    "acs_data_new = acs_data[cols_need]\n",
    "acs_data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6121e5",
   "metadata": {},
   "source": [
    "Let's rename our column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33cdad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_data_new = acs_data_new.rename(columns={'AOP8E001':'population_over_25',\n",
    "                                            'AOP8E022':'ba',\n",
    "                                            'AOP8E023':'ma',\n",
    "                                            'AOP8E024':'prof',\n",
    "                                            'AOP8E025':'doctorate',\n",
    "                                            'AORME001':'per_capita_income'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f2e3a",
   "metadata": {},
   "source": [
    "And create a new column that is the % of people who's highest level of education is or above the bachelor's degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_data_new['perc_highered'] = (acs_data_new['ba']+acs_data_new['ma']+acs_data_new['prof']+acs_data_new['doctorate'])/acs_data_new['population_over_25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751d60e",
   "metadata": {},
   "source": [
    "Finally, let's join this DF back to the tracts data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb07b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I'm going to use the default join method, which is an inner join\n",
    "## So no need specify how='inner'\n",
    "tracts_acs = tracts.merge(acs_data_new, on='GISJOIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_acs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b4247",
   "metadata": {},
   "source": [
    "## 1.4 Descriptive statistics\n",
    "Let's take a look some descriptive statistics. I'm only going to select a subset of columns I'm interested in for my descriptive analysis. For instance, the `ALAND` (area of land in the tract) is not really relevant in this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5655a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_acs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02491d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_using = ['population_over_25', 'ba', 'ma', 'prof', 'doctorate',\n",
    "       'per_capita_income', 'perc_highered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840473b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_acs[cols_using].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_acs[cols_using].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Even plotting 7 columns took about 11 seconds for me!\n",
    "## I've set the transparency to be really low at 0.2, so that we can see the density of the points\n",
    "sns.pairplot(tracts_acs[cols_using],\n",
    "            markers='+',diag_kind='kde',\n",
    "            plot_kws={'alpha':0.2,'s':30,'color':'red'},\n",
    "            diag_kws={'color':'red'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee50bba",
   "metadata": {},
   "source": [
    "Oh, pretty!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328af4e7",
   "metadata": {},
   "source": [
    "## Q.1 (2 pts)\n",
    "List at least three reasons why the above is not a clear figure in a markdown. Can you only focus on New York State & plotting percent of higher education to improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467b39c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just plotting NY state so that the map doesn't take so long to render\n",
    "tracts_acs[tracts_acs['STATEFP']=='36'].plot(column='perc_highered',figsize=(10,10),legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db8235",
   "metadata": {},
   "source": [
    "# 2. `cenpy`\n",
    "\n",
    "`cenpy` is python library that allows us to easily use the [US Census Bureau's API](http://www.census.gov/data/developers/data-sets.html) to programmatically read the publicly available data sets into a dataframe or geodataframe. This does not include the Decennial Census and American Community Survey, but other data products from the Bureau such as the Longitudinal Employer-Household Dynamics dataset, the Commodity Flow Survey, Survey of Business Owners, etc. \n",
    "\n",
    "\n",
    "\n",
    "#### APIs\n",
    "What is an API? API stands for Application Programming Interface and essentially is a tool that allow our computers to communicate and use an API host's (in this case, the Census) \"servers\" (computers) in a regulated manner. There are different kinds of APIs, though most APIs that you will use are called REST APIs. This is when we (the \"client\") send data to the OSM (the \"server\") so that its API reads the data and returns the outputs we asked for - the Census data in this case. `cenpy` is acting as facilitator in this case, since the Census's API is a little clunky. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cenpy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e75862",
   "metadata": {},
   "source": [
    "## 2.1 Getting ACS data from Cenpy\n",
    "`cenpy.products` is the tool that we will mostly be interacting with. Here let's explore `cenpy.products.ACS()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This creates the \"connection\" to the latest 5-year ACS data in the API, which is 2019\n",
    "## cenpy.products.ACS(2018) would give us the 2014-2018 ACS data\n",
    "acs_cp= cenpy.products.ACS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491997c9",
   "metadata": {},
   "source": [
    "We then need to tell cenpy how to extract the data based on geography. We can\n",
    "- `from_place()`\n",
    "- `from_county()`\n",
    "- `from_state()`\n",
    "- `from_csa()`\n",
    "- `from_msa()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624694a",
   "metadata": {},
   "source": [
    "We will also need to tell cenpy which [ACS Table Lists and Shells](https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.2019.html#list-tab-LO1F1MU1CQP3YOHD2T) we want. Luckily, we already have this from the NHGIS codebook. \n",
    "\n",
    "Notice in your code that **Source code: B15003**. The \"source\" here is the original Census table. \n",
    "\n",
    "Lastly, we need to indicate the geography level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note, because this a free API, it is limited to\n",
    "acs_cp_data = acs_cp.from_place('New York, NY',\n",
    "                                level='tract',\n",
    "                                variables=['B15003'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df31b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_cp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7837d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## They are all in the same order as the NHGIS data, so I can just use the same columns\n",
    "acs_cp_data['perc_highered'] = (acs_cp_data['B15003_022E']+acs_cp_data['B15003_023E']+acs_cp_data['B15003_024E']+acs_cp_data['B15003_025E'])/acs_cp_data['B15003_001E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce605f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_cp_data.plot(column='perc_highered',figsize=(10,10),legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b819d",
   "metadata": {},
   "source": [
    "Pretty neat! There are certain limitations here (we can't get the whole of NY state or the country, given the API's limitations, for instance), but cenpy is a pretty easy to use out of the box tool for some fast case studies uses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a18968",
   "metadata": {},
   "source": [
    "## Q.2 (5 pts)\n",
    "Using the [ACS table](https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.2019.html#list-tab-LO1F1MU1CQP3YOHD2T) lookup page to download \"2019 ACS Detailed Table Shells\"\n",
    "- Find the table ID (in the format of \"B00000\") for the **HISPANIC OR LATINO ORIGIN BY RACE** table \n",
    "- For the state of Massachusetts, use the `.from_state()` function with your `acs_cp` engine. \n",
    "\n",
    "Call this geodataframe `ma`.\n",
    "\n",
    "\n",
    "(This query took about 1 minute for me.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = ### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f427f",
   "metadata": {},
   "source": [
    "Now calculate the percentage of \"Non Hispanic or Latino, White Alone\" in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma['perc_white'] = ### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35606ffa",
   "metadata": {},
   "source": [
    "And create a plot of the non-hispanic White percentage across the state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ec21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "ma.plot(### INSERT YOUR CODE HERE)\n",
    "ax.set_axis_off()\n",
    "\n",
    "## Use tight_layout to remove the white space around the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "## I forgot to show you all how to save down your plots!\n",
    "fig.savefig('MA_perc_white.png')   # save the figure to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a6c51",
   "metadata": {},
   "source": [
    "## 3. Socrata and Socrata APIs\n",
    "Many government open data portals were built by the same company, Socrata (acquired a few years back by Tyler Technologies), which created the infrastructure and front-end interface to access open government data. \n",
    "\n",
    "Here, we are going to re-visit our NYCHA developments dataset [here](https://data.cityofnewyork.us/Housing-Development/Map-of-NYCHA-Developments/i9rv-hdr5).\n",
    "\n",
    "\n",
    "You may have noticed that, when we go to export data, that there is a **SODA API** section: \n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/7pvi2f0jnbrlwdt/Screen%20Shot%202023-02-18%20at%205.57.09%20PM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "SODA is Socrata's API for allowing users from researchers to (more often) people building tools and applications to access open-portal data. This is most useful when you have to programmatically connect your data export to something else. For instance, if you're running a website that needs to update data in real-time or if you don't want to download an updated dataset each time, you can connect your notebook or app to this API. Click to expand the **SODA API** section: \n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/ure1ep5y7ussvxs/Screen%20Shot%202023-02-18%20at%205.57.44%20PM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "**Copy the API endpoint URL**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01f24f",
   "metadata": {},
   "source": [
    "## 3.1 API endpoint to GeoDataFrame\n",
    "\n",
    "We can pretty easily this JSON file into a geodataframe. FYI, a JSON stands for \"JavaScript Object Notation\" and is a file format that was desisgned for the JavaScript language, but is easily translated to other formats that we know well. \n",
    "\n",
    "The good thing is that pandas has a `pd.read_json()` function that will allow us read this JSON as a DF and eventually turn it into a geodataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha = pd.read_json('https://data.cityofnewyork.us/resource/5j2e-zhmb.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99913fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ab6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63284f5",
   "metadata": {},
   "source": [
    "Notice that there is a **the_geom** column that looks like it might have geometry information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ignore the warnings \n",
    "nycha['the_geom'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75987309",
   "metadata": {},
   "source": [
    "We are going to turn these strings, into Shapely geometries, which is the only piece of our data that is missing so we can turn this into a geometry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1de13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape\n",
    "\n",
    "## the apply method applies the function to each row of the dataframe\n",
    "nycha['the_geom'] = nycha['the_geom'].apply(shape)\n",
    "\n",
    "## I'm going to use the GeoDataFrame method to create a GeoDataFrame\n",
    "nycha_geo = gpd.GeoDataFrame(nycha,geometry='the_geom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Faint, but these are our buildings\n",
    "\n",
    "nycha_geo.plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaeb266",
   "metadata": {},
   "source": [
    "## 3.2 Filtering\n",
    "The SODA API allows us to filter data from the endpoint url. Why might we want to do this? For one, there are very large datasets such as the [311 Service Requests dataset](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9) (with 32 million rows) or the [Open Parking and Camera Violations](https://data.cityofnewyork.us/City-Government/Open-Parking-and-Camera-Violations/nc67-uf89) (with 93 million rows!) that are difficult to work with due to their size. \n",
    "\n",
    "There are two ways to filter data using the SODA API: \n",
    "- [Simple Filters](https://dev.socrata.com/docs/filtering.html)\n",
    "- [SoQL Queries](https://dev.socrata.com/docs/queries/)\n",
    "\n",
    "\n",
    "**Both of these filters are text we append to the original endpoint URL.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996410ef",
   "metadata": {},
   "source": [
    "### 3.2.1 Simple Filters\n",
    "Any column in the dataset can be used as a filter for specific values within that column and is in the format :\n",
    "\n",
    "`http://yourendpointurl.json?col_name=element_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_url_orig = \"https://data.cityofnewyork.us/resource/5j2e-zhmb.json\"\n",
    "\n",
    "## Note, this query is CASE-SENSITIVE! \n",
    "## If the column name is in all caps, it must be in all caps here\n",
    "## If the value of interest is in all caps, it must be in all caps here\n",
    "nycha_url_mh = \"https://data.cityofnewyork.us/resource/5j2e-zhmb.json?borough=MANHATTAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f394ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_mh = pd.read_json(nycha_url_mh)\n",
    "nycha_mh['the_geom'] = nycha_mh['the_geom'].apply(shape)\n",
    "nycha_mh_geo = gpd.GeoDataFrame(nycha_mh,geometry='the_geom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78991888",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_mh_geo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa383cb9",
   "metadata": {},
   "source": [
    "You can join multiple queries with an `&`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_url_mh_jeff = \"https://data.cityofnewyork.us/resource/5j2e-zhmb.json?borough=MANHATTAN&developmen=JEFFERSON\"\n",
    "nycha_mh_jeff = pd.read_json(nycha_url_mh_jeff)\n",
    "nycha_mh_jeff['the_geom'] = nycha_mh_jeff['the_geom'].apply(shape)\n",
    "nycha_mh_jeff_geo = gpd.GeoDataFrame(nycha_mh_jeff,geometry='the_geom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_mh_jeff_geo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006aade",
   "metadata": {},
   "source": [
    "### 3.2.2 SoQL Queries\n",
    "The “Socrata Query Language” (SoQL) is a simple, SQL-like query language specifically designed for making it easy to work with data on the web. If you're familiar with SQL, the following may be familiar. And even if you're not, this will seem pretty intuitive. \n",
    "\n",
    "Here are all the different parameters that you can use in this query: \n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/r4edgdtyzm2vrxn/Screen%20Shot%202023-02-19%20at%2010.09.27%20AM.png?dl=1\" alt=\"drawing\" width=\"800\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "**One key formatting difference here is the use of white space, which allowed in the query but must be translated into `%20` for URL purposes, since no white spaces are allowed in the URL.** I am using the `.replace(\"to_be_replace_str\",\"new_str\")` function to replace empty spaces with `%20`.\n",
    "\n",
    "The same filtering for Manhattan and the Jefferson Development we did above would look like this: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ea5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note the use of single vs double quotes here, since I need to include a single quote in the query\n",
    "nycha_url_mh_soql = \"https://data.cityofnewyork.us/resource/5j2e-zhmb.json?$where=borough='MANHATTAN' and developmen='JEFFERSON'\".replace(\" \", \"%20\")\n",
    "\n",
    "nycha_mh_jeff2 = pd.read_json(nycha_url_mh_soql)\n",
    "nycha_mh_jeff2['the_geom'] = nycha_mh_jeff2['the_geom'].apply(shape)\n",
    "nycha_mh_jeff2_geo = gpd.GeoDataFrame(nycha_mh_jeff2,geometry='the_geom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycha_mh_jeff2_geo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075bb44",
   "metadata": {},
   "source": [
    "### 3.2.3 A more complex SoQL query\n",
    "\n",
    "Let's say we wanted to look at the [311 Service Requests data](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9). Here are the ways I want to filter the dataset based on the columns available: \n",
    "- **Created Date** is since Feb 2023\n",
    "- **Complaint Type**  is `Noise - Residential`\n",
    "- **Descriptor** is `Loud Music/Party` \n",
    "\n",
    "Looking at the [311 API docs](https://dev.socrata.com/foundry/data.cityofnewyork.us/erm2-nwe9) will give you some example queries and will also show you the correct column names for the API. You can also find the column names when you click on each column in the \"Columns in the Dataset\" section of the data homepage. \n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/wlrh8jzes9dcsvv/Screen%20Shot%202023-02-19%20at%2011.55.08%20AM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_url = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json?$where=created_date between '2023-02-01T0:00:00.000' and '2023-02-19T0:00:00.000' and complaint_type='Noise - Residential' and descriptor='Loud Music/Party'\".replace(\" \", \"%20\")\n",
    "servicereq = pd.read_json(servicereq_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe6ea0",
   "metadata": {},
   "source": [
    "Let's turn this into a GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_geo = gpd.GeoDataFrame(servicereq, \n",
    "                                  geometry=gpd.points_from_xy(servicereq['longitude'], \n",
    "                                                              servicereq['latitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee374ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_geo.plot(markersize=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07289578",
   "metadata": {},
   "source": [
    "## 3.3 `offset` and `limit`\n",
    "The issue with using this endpoint is that we are limited to 1000 rows per query. You will see the documentation refer to this as \"pages\" sometimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3afcf16",
   "metadata": {},
   "source": [
    "What to do? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda8430",
   "metadata": {},
   "source": [
    "One way to get around this is to use the `limit` and `offset` parameters. From the SODA documentation: \n",
    "\n",
    ">The $offset parameter is most often used in conjunction with $limit to page through a dataset. The $offset is the number of records into a dataset that you want to start, indexed at 0. For example, to retrieve the “4th page” of records (records 151 - 200) where you are using $limit to page 50 records at a time, you’d ask for an $offset of 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_url_offset = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json?$limit=50&$offset=150&$where=created_date between '2023-02-01T0:00:00.000' and '2023-02-19T0:00:00.000' and complaint_type='Noise - Residential' and descriptor='Loud Music/Party'\".replace(\" \", \"%20\")\n",
    "servicereq_offset = pd.read_json(servicereq_url_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4b196",
   "metadata": {},
   "source": [
    "This is now 50 entries of the \"4th page\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4af51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a606b",
   "metadata": {},
   "source": [
    "So, to get all the data, what we can do is run a loop to change that offset amount iteratively. \n",
    "\n",
    "OR\n",
    "\n",
    "If we are getting the data just once, we can use the filter function, accessible through the  \"View Data\" button on the dataset's home page. \n",
    "\n",
    "</figure>\n",
    "<img src=\"https://www.dropbox.com/s/oz26ti7y164pm8r/Screen%20Shot%202023-02-19%20at%2012.35.21%20PM.png?dl=1\" alt=\"drawing\" width=\"1000\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e73a535",
   "metadata": {},
   "source": [
    "### 3.3.1 A short review of loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_counter = np.arange(0,1000,50)\n",
    "print(my_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14336041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The for loop will iterate through each value in the list\n",
    "# The {} is a placeholder for the value in the list within a string\n",
    "\n",
    "for i in my_counter: \n",
    "    print(\"Current Counter is now at {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset i to 0\n",
    "i = 0\n",
    "## The while loop will continue to run until the condition is no longer true\n",
    "while i < 1000:\n",
    "    print(\"Current Counter is now at {}\".format(i))\n",
    "    \n",
    "    ## This is an example of an incrementer\n",
    "    ## An incrementer is a variable that is used to increment a value\n",
    "    ## After each iteration, the value of i will increase by 50\n",
    "    i = i + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be77f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(0,100000,50):\n",
    "    print(\"Current Counter is now at {}\".format(i))\n",
    "    i = i + 50\n",
    "\n",
    "    if i >1000 :\n",
    "        print(\"We are done\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5496d6a",
   "metadata": {},
   "source": [
    "To programmatically run different queries, I just going to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d7f52",
   "metadata": {},
   "source": [
    "This might take a while to run and might not work at all given our 1000 an hour limit. :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc65168",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I actually don't know what the upper range is for my dataset, but I will just use 100,000\n",
    "# offset_list = np.arange(0,100000,50)\n",
    "\n",
    "# I'm actually going to use a smaller list for demo and not overloading the API\n",
    "offset_list_smaller = np.arange(0,200,50)\n",
    "\n",
    "list_of_dfs = []\n",
    "\n",
    "for offset in offset_list_smaller:\n",
    "    servicereq_url_offset = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json?$limit=50&$offset={}&$where=created_date between '2023-02-01T0:00:00.000' and '2023-02-19T0:00:00.000' and complaint_type='Noise - Residential' and descriptor='Loud Music/Party'\".replace(\" \", \"%20\").format(offset)\n",
    "    servicereq_offset = pd.read_json(servicereq_url_offset)\n",
    "\n",
    "    ## Here I am creating a list of dataframes by appending each dataframe to the list\n",
    "    list_of_dfs.append(servicereq_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34713251",
   "metadata": {},
   "source": [
    "I now have a list of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0776095",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pd.concat will concatenate the dataframes in the list\n",
    "## to create a single dataframe\n",
    "servicereq_final = pd.concat(list_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7dbb4",
   "metadata": {},
   "source": [
    "If I were to really try and get all this data, I'd put a `sleep()` call from the library `time` to pause my code from running the next line for a certain amount of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "list_of_dfs = []\n",
    "\n",
    "for offset in offset_list_smaller:\n",
    "    servicereq_url_offset = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json?$limit=50&$offset={}&$where=created_date between '2023-02-01T0:00:00.000' and '2023-02-19T0:00:00.000' and complaint_type='Noise - Residential' and descriptor='Loud Music/Party'\".replace(\" \", \"%20\").format(offset)\n",
    "    servicereq_offset = pd.read_json(servicereq_url_offset)\n",
    "\n",
    "    ## Here I am creating a list of dataframes by appending each dataframe to the list\n",
    "    list_of_dfs.append(servicereq_offset)\n",
    "    \n",
    "    ## I am adding a sleep timer to avoid overloading the API\n",
    "    ## The sleep timer will pause the code for 3 minutes\n",
    "    ## This gives me 3 min/run for each 50 records = 20 queries per hour = 1000 records per hour\n",
    "    time.sleep(180)\n",
    "    if servicereq_offset.shape[0] == 50:\n",
    "        print(\"We are done\")\n",
    "        break\n",
    "\n",
    "servicereq_final = pd.concat(list_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304b44d",
   "metadata": {},
   "source": [
    "Lastly! Don't think this means we can just get all the data at once. Each query we make \"costs\" the API provider resources. To ensure that everyone is able to use the API, the provider will limit your capacity to query. \n",
    "\n",
    ">## Throttling and Application Tokens\n",
    ">Hold on a second! Before you go storming off to make the next great open data app, you should understand how SODA handles throttling. You can make a certain number of requests without an application token, but they come from a shared pool and you’re eventually going to get cut off.\n",
    ">\n",
    ">If you want more requests, sign up for a Socrata account, then register for an application token and your application will be granted up to 1000 requests per rolling hour period. If you need even more than that, special exceptions are made by request. You can contact our support team here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_url_offset = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json?$limit=50&$offset=5000000&$where=created_date between '2023-02-01T0:00:00.000' and '2023-02-19T0:00:00.000' and complaint_type='Noise - Residential' and descriptor='Loud Music/Party'\".replace(\" \", \"%20\").format(offset)\n",
    "servicereq_offset = pd.read_json(servicereq_url_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "servicereq_offset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d846f09",
   "metadata": {},
   "source": [
    "## Q.3 Querying and Concatenating (5 pts)\n",
    "- Using the [Film Permits](https://data.cityofnewyork.us/City-Government/Film-Permits/tg4x-b46p) dataset to retrieve two dataframes: \n",
    "    1. The **StartDateTime** should be after July 1, 2022\n",
    "    2. The **StartDateTime** should be after July 1, 2022 & The **Category** should be `Television`. \n",
    "- Create a list of two dataframes with 50 rows per \"page\"\n",
    "- Concatenate these two dataframes together into one dataframe\n",
    "- Show the first 5 rows of the new dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81311c",
   "metadata": {},
   "source": [
    "Using the [Film Permits](https://data.cityofnewyork.us/City-Government/Film-Permits/tg4x-b46p) dataset to retrieve two dataframes: \n",
    "1. The **StartDateTime** should be after July 1, 2022\n",
    "2. The **StartDateTime** should be after July 1, 2022 & The **Category** should be `Television`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "film_url1 = ## INSERT YOUR CODE HERE\n",
    "film1 = pd.read_json(film_url1)\n",
    "\n",
    "film_url2 = ## INSERT YOUR CODE HERE\n",
    "film2 = pd.read_json(film_url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fa907",
   "metadata": {},
   "source": [
    "Concatenate these two dataframes together into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064c12b",
   "metadata": {},
   "source": [
    "Show the first 5 rows of the new dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5b5e944a9b71ce46e873efe99abd044b4b2cd2a5153fea1f2fc73581012ba31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
